{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DLAV-2023'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/vita-epfl/DLAV-2023.git\n",
    "path = os.getcwd() + '/DLAV-2023/homeworks/hw1/data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    error = y.reshape(-1,1) - np.dot(x,theta.reshape(-1,1))\n",
    "    return np.dot(error.T,error)/error.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.14546775]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # compute gradient and loss\n",
    "    error = y.reshape(-1,1) - np.dot(tx,w.reshape(-1,1))\n",
    "    gradient = -np.dot(tx.T, error) / tx.shape[0] \n",
    "    loss = computeCost(tx, y, w)\n",
    "    return gradient, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        [gradient, loss] = compute_gradient(y, X, theta)\n",
    "        \n",
    "        # update theta by gradient\n",
    "        theta = theta - alpha * gradient.reshape(-1,)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=[[64.14546775]], w0=0.058391350515463895, w1=0.6532884974555669\n",
      "Gradient Descent(1/999): loss=[[13.47438093]], w0=0.06289175271039382, w1=0.7700097825599364\n",
      "Gradient Descent(2/999): loss=[[11.86318714]], w0=0.057822927461428114, w1=0.7913481156584673\n",
      "Gradient Descent(3/999): loss=[[11.80230941]], w0=0.051063625160778135, w1=0.795729810284954\n",
      "Gradient Descent(4/999): loss=[[11.79045717]], w0=0.04401437836500259, w1=0.7970961782721866\n",
      "Gradient Descent(5/999): loss=[[11.78018989]], w0=0.03692413114216261, w1=0.7979254732843951\n",
      "Gradient Descent(6/999): loss=[[11.77000832]], w0=0.029837117577144825, w1=0.7986582394519285\n",
      "Gradient Descent(7/999): loss=[[11.75986496]], w0=0.022761181894038834, w1=0.7993727912003019\n",
      "Gradient Descent(8/999): loss=[[11.74975819]], w0=0.01569769957420013, w1=0.8000830518518655\n",
      "Gradient Descent(9/999): loss=[[11.73968782]], w0=0.008646896228913532, w1=0.8007914983590768\n",
      "Gradient Descent(10/999): loss=[[11.72965373]], w0=0.0016087930989843596, w1=0.8014985729280016\n",
      "Gradient Descent(11/999): loss=[[11.71965578]], w0=-0.005416624870320648, w1=0.8022043560583255\n",
      "Gradient Descent(12/999): loss=[[11.70969384]], w0=-0.012429379151800774, w1=0.8029088639482521\n",
      "Gradient Descent(13/999): loss=[[11.69976778]], w0=-0.019429492325268326, w1=0.8036121013621461\n",
      "Gradient Descent(14/999): loss=[[11.68987748]], w0=-0.026416987133500124, w1=0.8043140710284592\n",
      "Gradient Descent(15/999): loss=[[11.6800228]], w0=-0.033391886314481416, w1=0.8050147753103407\n",
      "Gradient Descent(16/999): loss=[[11.67020362]], w0=-0.04035421257164587, w1=0.8057142165026173\n",
      "Gradient Descent(17/999): loss=[[11.6604198]], w0=-0.04730398856864606, w1=0.8064123968845912\n",
      "Gradient Descent(18/999): loss=[[11.65067123]], w0=-0.05424123692848455, w1=0.8071093187294315\n",
      "Gradient Descent(19/999): loss=[[11.64095777]], w0=-0.06116598023341994, w1=0.8078049843058496\n",
      "Gradient Descent(20/999): loss=[[11.6312793]], w0=-0.06807824102501053, w1=0.8084993958784036\n",
      "Gradient Descent(21/999): loss=[[11.62163569]], w0=-0.07497804180418248, w1=0.8091925557075582\n",
      "Gradient Descent(22/999): loss=[[11.61202681]], w0=-0.08186540503130207, w1=0.8098844660497012\n",
      "Gradient Descent(23/999): loss=[[11.60245254]], w0=-0.08874035312624866, w1=0.8105751291571527\n",
      "Gradient Descent(24/999): loss=[[11.59291276]], w0=-0.0956029084684876, w1=0.8112645472781728\n",
      "Gradient Descent(25/999): loss=[[11.58340735]], w0=-0.10245309339714315, w1=0.8119527226569687\n",
      "Gradient Descent(26/999): loss=[[11.57393617]], w0=-0.10929093021107113, w1=0.8126396575337025\n",
      "Gradient Descent(27/999): loss=[[11.5644991]], w0=-0.11611644116893155, w1=0.813325354144498\n",
      "Gradient Descent(28/999): loss=[[11.55509603]], w0=-0.12292964848926106, w1=0.8140098147214483\n",
      "Gradient Descent(29/999): loss=[[11.54572682]], w0=-0.12973057435054527, w1=0.8146930414926227\n",
      "Gradient Descent(30/999): loss=[[11.53639137]], w0=-0.13651924089129092, w1=0.8153750366820742\n",
      "Gradient Descent(31/999): loss=[[11.52708954]], w0=-0.14329567021009798, w1=0.8160558025098472\n",
      "Gradient Descent(32/999): loss=[[11.51782121]], w0=-0.1500598843657316, w1=0.8167353411919838\n",
      "Gradient Descent(33/999): loss=[[11.50858627]], w0=-0.15681190537719383, w1=0.8174136549405314\n",
      "Gradient Descent(34/999): loss=[[11.49938459]], w0=-0.16355175522379548, w1=0.8180907459635505\n",
      "Gradient Descent(35/999): loss=[[11.49021606]], w0=-0.17027945584522738, w1=0.8187666164651208\n",
      "Gradient Descent(36/999): loss=[[11.48108055]], w0=-0.17699502914163212, w1=0.8194412686453493\n",
      "Gradient Descent(37/999): loss=[[11.47197794]], w0=-0.1836984969736751, w1=0.8201147047003767\n",
      "Gradient Descent(38/999): loss=[[11.46290812]], w0=-0.19038988116261576, w1=0.8207869268223856\n",
      "Gradient Descent(39/999): loss=[[11.45387098]], w0=-0.1970692034903787, w1=0.8214579371996062\n",
      "Gradient Descent(40/999): loss=[[11.44486638]], w0=-0.20373648569962446, w1=0.8221277380163249\n",
      "Gradient Descent(41/999): loss=[[11.43589421]], w0=-0.2103917494938204, w1=0.8227963314528903\n",
      "Gradient Descent(42/999): loss=[[11.42695437]], w0=-0.21703501653731122, w1=0.8234637196857207\n",
      "Gradient Descent(43/999): loss=[[11.41804672]], w0=-0.22366630845538962, w1=0.8241299048873114\n",
      "Gradient Descent(44/999): loss=[[11.40917116]], w0=-0.23028564683436664, w1=0.8247948892262416\n",
      "Gradient Descent(45/999): loss=[[11.40032757]], w0=-0.23689305322164192, w1=0.8254586748671812\n",
      "Gradient Descent(46/999): loss=[[11.39151584]], w0=-0.24348854912577383, w1=0.826121263970898\n",
      "Gradient Descent(47/999): loss=[[11.38273584]], w0=-0.25007215601654953, w1=0.8267826586942654\n",
      "Gradient Descent(48/999): loss=[[11.37398747]], w0=-0.25664389532505477, w1=0.8274428611902682\n",
      "Gradient Descent(49/999): loss=[[11.3652706]], w0=-0.2632037884437438, w1=0.8281018736080105\n",
      "Gradient Descent(50/999): loss=[[11.35658514]], w0=-0.2697518567265089, w1=0.8287596980927223\n",
      "Gradient Descent(51/999): loss=[[11.34793096]], w0=-0.27628812148874987, w1=0.8294163367857669\n",
      "Gradient Descent(52/999): loss=[[11.33930795]], w0=-0.28281260400744346, w1=0.8300717918246473\n",
      "Gradient Descent(53/999): loss=[[11.33071601]], w0=-0.2893253255212127, w1=0.8307260653430134\n",
      "Gradient Descent(54/999): loss=[[11.32215501]], w0=-0.2958263072303959, w1=0.8313791594706694\n",
      "Gradient Descent(55/999): loss=[[11.31362484]], w0=-0.3023155702971157, w1=0.8320310763335801\n",
      "Gradient Descent(56/999): loss=[[11.3051254]], w0=-0.30879313584534807, w1=0.8326818180538778\n",
      "Gradient Descent(57/999): loss=[[11.29665658]], w0=-0.315259024960991, w1=0.8333313867498697\n",
      "Gradient Descent(58/999): loss=[[11.28821826]], w0=-0.32171325869193307, w1=0.8339797845360446\n",
      "Gradient Descent(59/999): loss=[[11.27981033]], w0=-0.328155858048122, w1=0.8346270135230796\n",
      "Gradient Descent(60/999): loss=[[11.27143269]], w0=-0.3345868440016331, w1=0.835273075817847\n",
      "Gradient Descent(61/999): loss=[[11.26308523]], w0=-0.34100623748673753, w1=0.8359179735234217\n",
      "Gradient Descent(62/999): loss=[[11.25476783]], w0=-0.3474140593999704, w1=0.8365617087390872\n",
      "Gradient Descent(63/999): loss=[[11.24648039]], w0=-0.3538103306001988, w1=0.8372042835603428\n",
      "Gradient Descent(64/999): loss=[[11.2382228]], w0=-0.3601950719086897, w1=0.8378457000789108\n",
      "Gradient Descent(65/999): loss=[[11.22999496]], w0=-0.36656830410917784, w1=0.8384859603827427\n",
      "Gradient Descent(66/999): loss=[[11.22179675]], w0=-0.37293004794793316, w1=0.8391250665560264\n",
      "Gradient Descent(67/999): loss=[[11.21362807]], w0=-0.37928032413382856, w1=0.8397630206791926\n",
      "Gradient Descent(68/999): loss=[[11.20548881]], w0=-0.38561915333840713, w1=0.8403998248289224\n",
      "Gradient Descent(69/999): loss=[[11.19737887]], w0=-0.39194655619594954, w1=0.8410354810781528\n",
      "Gradient Descent(70/999): loss=[[11.18929815]], w0=-0.3982625533035412, w1=0.8416699914960846\n",
      "Gradient Descent(71/999): loss=[[11.18124653]], w0=-0.4045671652211394, w1=0.8423033581481884\n",
      "Gradient Descent(72/999): loss=[[11.17322391]], w0=-0.41086041247163996, w1=0.8429355830962117\n",
      "Gradient Descent(73/999): loss=[[11.16523018]], w0=-0.41714231554094433, w1=0.8435666683981857\n",
      "Gradient Descent(74/999): loss=[[11.15726525]], w0=-0.42341289487802614, w1=0.8441966161084314\n",
      "Gradient Descent(75/999): loss=[[11.14932901]], w0=-0.42967217089499776, w1=0.844825428277567\n",
      "Gradient Descent(76/999): loss=[[11.14142136]], w0=-0.43592016396717675, w1=0.8454531069525142\n",
      "Gradient Descent(77/999): loss=[[11.13354219]], w0=-0.4421568944331523, w1=0.8460796541765048\n",
      "Gradient Descent(78/999): loss=[[11.1256914]], w0=-0.4483823825948513, w1=0.8467050719890875\n",
      "Gradient Descent(79/999): loss=[[11.11786888]], w0=-0.4545966487176044, w1=0.8473293624261348\n",
      "Gradient Descent(80/999): loss=[[11.11007454]], w0=-0.4607997130302122, w1=0.8479525275198488\n",
      "Gradient Descent(81/999): loss=[[11.10230828]], w0=-0.4669915957250108, w1=0.8485745692987691\n",
      "Gradient Descent(82/999): loss=[[11.09456999]], w0=-0.47317231695793777, w1=0.8491954897877779\n",
      "Gradient Descent(83/999): loss=[[11.08685958]], w0=-0.47934189684859757, w1=0.8498152910081079\n",
      "Gradient Descent(84/999): loss=[[11.07917693]], w0=-0.4855003554803273, w1=0.8504339749773481\n",
      "Gradient Descent(85/999): loss=[[11.07152196]], w0=-0.4916477129002617, w1=0.8510515437094506\n",
      "Gradient Descent(86/999): loss=[[11.06389457]], w0=-0.4977839891193989, w1=0.8516679992147372\n",
      "Gradient Descent(87/999): loss=[[11.05629464]], w0=-0.5039092041126652, w1=0.8522833434999061\n",
      "Gradient Descent(88/999): loss=[[11.04872209]], w0=-0.5100233778189799, w1=0.8528975785680377\n",
      "Gradient Descent(89/999): loss=[[11.04117682]], w0=-0.5161265301413209, w1=0.8535107064186023\n",
      "Gradient Descent(90/999): loss=[[11.03365872]], w0=-0.5222186809467889, w1=0.8541227290474656\n",
      "Gradient Descent(91/999): loss=[[11.0261677]], w0=-0.5282998500666722, w1=0.8547336484468955\n",
      "Gradient Descent(92/999): loss=[[11.01870367]], w0=-0.5343700572965113, w1=0.8553434666055688\n",
      "Gradient Descent(93/999): loss=[[11.01126652]], w0=-0.5404293223961635, w1=0.8559521855085775\n",
      "Gradient Descent(94/999): loss=[[11.00385616]], w0=-0.5464776650898668, w1=0.8565598071374354\n",
      "Gradient Descent(95/999): loss=[[10.99647249]], w0=-0.5525151050663047, w1=0.8571663334700842\n",
      "Gradient Descent(96/999): loss=[[10.98911541]], w0=-0.5585416619786697, w1=0.8577717664809001\n",
      "Gradient Descent(97/999): loss=[[10.98178484]], w0=-0.5645573554447276, w1=0.8583761081407008\n",
      "Gradient Descent(98/999): loss=[[10.97448067]], w0=-0.5705622050468814, w1=0.8589793604167507\n",
      "Gradient Descent(99/999): loss=[[10.96720281]], w0=-0.5765562303322347, w1=0.8595815252727688\n",
      "Gradient Descent(100/999): loss=[[10.95995116]], w0=-0.5825394508126558, w1=0.8601826046689336\n",
      "Gradient Descent(101/999): loss=[[10.95272563]], w0=-0.5885118859648409, w1=0.8607826005618906\n",
      "Gradient Descent(102/999): loss=[[10.94552613]], w0=-0.5944735552303778, w1=0.8613815149047582\n",
      "Gradient Descent(103/999): loss=[[10.93835257]], w0=-0.6004244780158086, w1=0.861979349647134\n",
      "Gradient Descent(104/999): loss=[[10.93120484]], w0=-0.6063646736926934, w1=0.8625761067351013\n",
      "Gradient Descent(105/999): loss=[[10.92408285]], w0=-0.6122941615976734, w1=0.8631717881112356\n",
      "Gradient Descent(106/999): loss=[[10.91698652]], w0=-0.6182129610325333, w1=0.8637663957146104\n",
      "Gradient Descent(107/999): loss=[[10.90991575]], w0=-0.6241210912642648, w1=0.864359931480804\n",
      "Gradient Descent(108/999): loss=[[10.90287045]], w0=-0.630018571525129, w1=0.8649523973419058\n",
      "Gradient Descent(109/999): loss=[[10.89585052]], w0=-0.6359054210127186, w1=0.865543795226522\n",
      "Gradient Descent(110/999): loss=[[10.88885588]], w0=-0.6417816588900213, w1=0.8661341270597828\n",
      "Gradient Descent(111/999): loss=[[10.88188644]], w0=-0.6476473042854812, w1=0.8667233947633476\n",
      "Gradient Descent(112/999): loss=[[10.8749421]], w0=-0.6535023762930622, w1=0.8673116002554123\n",
      "Gradient Descent(113/999): loss=[[10.86802277]], w0=-0.6593468939723088, w1=0.8678987454507151\n",
      "Gradient Descent(114/999): loss=[[10.86112836]], w0=-0.6651808763484093, w1=0.8684848322605423\n",
      "Gradient Descent(115/999): loss=[[10.85425879]], w0=-0.671004342412257, w1=0.8690698625927351\n",
      "Gradient Descent(116/999): loss=[[10.84741396]], w0=-0.6768173111205126, w1=0.8696538383516959\n",
      "Gradient Descent(117/999): loss=[[10.84059379]], w0=-0.6826198013956652, w1=0.8702367614383937\n",
      "Gradient Descent(118/999): loss=[[10.83379818]], w0=-0.6884118321260947, w1=0.8708186337503714\n",
      "Gradient Descent(119/999): loss=[[10.82702705]], w0=-0.6941934221661327, w1=0.8713994571817509\n",
      "Gradient Descent(120/999): loss=[[10.82028031]], w0=-0.6999645903361239, w1=0.8719792336232401\n",
      "Gradient Descent(121/999): loss=[[10.81355787]], w0=-0.7057253554224879, w1=0.8725579649621386\n",
      "Gradient Descent(122/999): loss=[[10.80685964]], w0=-0.7114757361777797, w1=0.873135653082344\n",
      "Gradient Descent(123/999): loss=[[10.80018554]], w0=-0.7172157513207511, w1=0.8737122998643578\n",
      "Gradient Descent(124/999): loss=[[10.79353548]], w0=-0.7229454195364116, w1=0.874287907185292\n",
      "Gradient Descent(125/999): loss=[[10.78690937]], w0=-0.728664759476089, w1=0.8748624769188749\n",
      "Gradient Descent(126/999): loss=[[10.78030713]], w0=-0.7343737897574906, w1=0.8754360109354568\n",
      "Gradient Descent(127/999): loss=[[10.77372867]], w0=-0.7400725289647632, w1=0.876008511102017\n",
      "Gradient Descent(128/999): loss=[[10.76717391]], w0=-0.745760995648554, w1=0.8765799792821695\n",
      "Gradient Descent(129/999): loss=[[10.76064276]], w0=-0.751439208326071, w1=0.8771504173361683\n",
      "Gradient Descent(130/999): loss=[[10.75413513]], w0=-0.7571071854811431, w1=0.8777198271209147\n",
      "Gradient Descent(131/999): loss=[[10.74765094]], w0=-0.7627649455642801, w1=0.8782882104899624\n",
      "Gradient Descent(132/999): loss=[[10.7411901]], w0=-0.7684125069927333, w1=0.8788555692935243\n",
      "Gradient Descent(133/999): loss=[[10.73475254]], w0=-0.7740498881505551, w1=0.8794219053784776\n",
      "Gradient Descent(134/999): loss=[[10.72833817]], w0=-0.7796771073886586, w1=0.8799872205883708\n",
      "Gradient Descent(135/999): loss=[[10.7219469]], w0=-0.785294183024878, w1=0.8805515167634289\n",
      "Gradient Descent(136/999): loss=[[10.71557865]], w0=-0.7909011333440276, w1=0.8811147957405598\n",
      "Gradient Descent(137/999): loss=[[10.70923334]], w0=-0.7964979765979616, w1=0.8816770593533604\n",
      "Gradient Descent(138/999): loss=[[10.70291088]], w0=-0.8020847310056336, w1=0.882238309432122\n",
      "Gradient Descent(139/999): loss=[[10.6966112]], w0=-0.8076614147531557, w1=0.8827985478038369\n",
      "Gradient Descent(140/999): loss=[[10.69033421]], w0=-0.8132280459938577, w1=0.8833577762922041\n",
      "Gradient Descent(141/999): loss=[[10.68407983]], w0=-0.8187846428483464, w1=0.883915996717635\n",
      "Gradient Descent(142/999): loss=[[10.67784797]], w0=-0.8243312234045646, w1=0.8844732108972596\n",
      "Gradient Descent(143/999): loss=[[10.67163857]], w0=-0.8298678057178496, w1=0.8850294206449325\n",
      "Gradient Descent(144/999): loss=[[10.66545153]], w0=-0.8353944078109924, w1=0.8855846277712385\n",
      "Gradient Descent(145/999): loss=[[10.65928677]], w0=-0.840911047674296, w1=0.8861388340834986\n",
      "Gradient Descent(146/999): loss=[[10.65314422]], w0=-0.8464177432656345, w1=0.8866920413857761\n",
      "Gradient Descent(147/999): loss=[[10.64702379]], w0=-0.8519145125105108, w1=0.8872442514788821\n",
      "Gradient Descent(148/999): loss=[[10.64092541]], w0=-0.8574013733021155, w1=0.887795466160382\n",
      "Gradient Descent(149/999): loss=[[10.634849]], w0=-0.8628783435013853, w1=0.8883456872246002\n",
      "Gradient Descent(150/999): loss=[[10.62879447]], w0=-0.8683454409370605, w1=0.8888949164626272\n",
      "Gradient Descent(151/999): loss=[[10.62276175]], w0=-0.8738026834057434, w1=0.8894431556623249\n",
      "Gradient Descent(152/999): loss=[[10.61675076]], w0=-0.8792500886719564, w1=0.8899904066083322\n",
      "Gradient Descent(153/999): loss=[[10.61076142]], w0=-0.8846876744681996, w1=0.8905366710820709\n",
      "Gradient Descent(154/999): loss=[[10.60479366]], w0=-0.8901154584950085, w1=0.891081950861752\n",
      "Gradient Descent(155/999): loss=[[10.59884739]], w0=-0.8955334584210117, w1=0.8916262477223809\n",
      "Gradient Descent(156/999): loss=[[10.59292254]], w0=-0.9009416918829886, w1=0.892169563435763\n",
      "Gradient Descent(157/999): loss=[[10.58701903]], w0=-0.9063401764859261, w1=0.8927118997705107\n",
      "Gradient Descent(158/999): loss=[[10.58113678]], w0=-0.9117289298030771, w1=0.8932532584920476\n",
      "Gradient Descent(159/999): loss=[[10.57527572]], w0=-0.9171079693760165, w1=0.8937936413626153\n",
      "Gradient Descent(160/999): loss=[[10.56943577]], w0=-0.922477312714699, w1=0.8943330501412785\n",
      "Gradient Descent(161/999): loss=[[10.56361686]], w0=-0.9278369772975161, w1=0.8948714865839315\n",
      "Gradient Descent(162/999): loss=[[10.55781891]], w0=-0.9331869805713527, w1=0.8954089524433027\n",
      "Gradient Descent(163/999): loss=[[10.55204184]], w0=-0.938527339951644, w1=0.8959454494689618\n",
      "Gradient Descent(164/999): loss=[[10.54628558]], w0=-0.943858072822432, w1=0.8964809794073243\n",
      "Gradient Descent(165/999): loss=[[10.54055005]], w0=-0.9491791965364226, w1=0.8970155440016577\n",
      "Gradient Descent(166/999): loss=[[10.53483518]], w0=-0.9544907284150417, w1=0.8975491449920872\n",
      "Gradient Descent(167/999): loss=[[10.5291409]], w0=-0.9597926857484916, w1=0.898081784115601\n",
      "Gradient Descent(168/999): loss=[[10.52346713]], w0=-0.9650850857958077, w1=0.8986134631060566\n",
      "Gradient Descent(169/999): loss=[[10.51781379]], w0=-0.9703679457849137, w1=0.8991441836941857\n",
      "Gradient Descent(170/999): loss=[[10.51218082]], w0=-0.9756412829126788, w1=0.8996739476076004\n",
      "Gradient Descent(171/999): loss=[[10.50656813]], w0=-0.9809051143449731, w1=0.9002027565707984\n",
      "Gradient Descent(172/999): loss=[[10.50097567]], w0=-0.9861594572167234, w1=0.9007306123051692\n",
      "Gradient Descent(173/999): loss=[[10.49540334]], w0=-0.9914043286319694, w1=0.9012575165289988\n",
      "Gradient Descent(174/999): loss=[[10.48985109]], w0=-0.996639745663919, w1=0.9017834709574764\n",
      "Gradient Descent(175/999): loss=[[10.48431884]], w0=-1.001865725355004, w1=0.902308477302699\n",
      "Gradient Descent(176/999): loss=[[10.47880651]], w0=-1.0070822847169358, w1=0.9028325372736775\n",
      "Gradient Descent(177/999): loss=[[10.47331404]], w0=-1.01228944073076, w1=0.9033556525763423\n",
      "Gradient Descent(178/999): loss=[[10.46784135]], w0=-1.017487210346913, w1=0.9038778249135484\n",
      "Gradient Descent(179/999): loss=[[10.46238838]], w0=-1.0226756104852757, w1=0.9043990559850815\n",
      "Gradient Descent(180/999): loss=[[10.45695504]], w0=-1.0278546580352297, w1=0.9049193474876631\n",
      "Gradient Descent(181/999): loss=[[10.45154128]], w0=-1.0330243698557118, w1=0.9054387011149563\n",
      "Gradient Descent(182/999): loss=[[10.44614702]], w0=-1.038184762775269, w1=0.9059571185575711\n",
      "Gradient Descent(183/999): loss=[[10.44077218]], w0=-1.0433358535921131, w1=0.9064746015030702\n",
      "Gradient Descent(184/999): loss=[[10.43541671]], w0=-1.0484776590741756, w1=0.9069911516359741\n",
      "Gradient Descent(185/999): loss=[[10.43008053]], w0=-1.0536101959591622, w1=0.907506770637767\n",
      "Gradient Descent(186/999): loss=[[10.42476356]], w0=-1.0587334809546072, w1=0.9080214601869019\n",
      "Gradient Descent(187/999): loss=[[10.41946575]], w0=-1.063847530737928, w1=0.9085352219588062\n",
      "Gradient Descent(188/999): loss=[[10.41418702]], w0=-1.0689523619564796, w1=0.9090480576258873\n",
      "Gradient Descent(189/999): loss=[[10.40892731]], w0=-1.074047991227608, w1=0.9095599688575379\n",
      "Gradient Descent(190/999): loss=[[10.40368654]], w0=-1.0791344351387053, w1=0.9100709573201414\n",
      "Gradient Descent(191/999): loss=[[10.39846464]], w0=-1.0842117102472633, w1=0.9105810246770776\n",
      "Gradient Descent(192/999): loss=[[10.39326156]], w0=-1.0892798330809268, w1=0.9110901725887276\n",
      "Gradient Descent(193/999): loss=[[10.38807721]], w0=-1.0943388201375486, w1=0.9115984027124796\n",
      "Gradient Descent(194/999): loss=[[10.38291155]], w0=-1.0993886878852421, w1=0.9121057167027342\n",
      "Gradient Descent(195/999): loss=[[10.37776448]], w0=-1.1044294527624354, w1=0.91261211621091\n",
      "Gradient Descent(196/999): loss=[[10.37263596]], w0=-1.109461131177925, w1=0.9131176028854483\n",
      "Gradient Descent(197/999): loss=[[10.36752591]], w0=-1.1144837395109286, w1=0.9136221783718195\n",
      "Gradient Descent(198/999): loss=[[10.36243426]], w0=-1.1194972941111392, w1=0.9141258443125273\n",
      "Gradient Descent(199/999): loss=[[10.35736096]], w0=-1.1245018112987775, w1=0.9146286023471152\n",
      "Gradient Descent(200/999): loss=[[10.35230593]], w0=-1.1294973073646457, w1=0.9151304541121706\n",
      "Gradient Descent(201/999): loss=[[10.3472691]], w0=-1.1344837985701803, w1=0.9156314012413316\n",
      "Gradient Descent(202/999): loss=[[10.34225042]], w0=-1.1394613011475048, w1=0.9161314453652908\n",
      "Gradient Descent(203/999): loss=[[10.33724982]], w0=-1.1444298312994827, w1=0.9166305881118018\n",
      "Gradient Descent(204/999): loss=[[10.33226722]], w0=-1.1493894051997708, w1=0.9171288311056836\n",
      "Gradient Descent(205/999): loss=[[10.32730258]], w0=-1.1543400389928709, w1=0.9176261759688266\n",
      "Gradient Descent(206/999): loss=[[10.32235582]], w0=-1.1592817487941824, w1=0.9181226243201975\n",
      "Gradient Descent(207/999): loss=[[10.31742687]], w0=-1.164214550690056, w1=0.9186181777758449\n",
      "Gradient Descent(208/999): loss=[[10.31251568]], w0=-1.169138460737845, w1=0.9191128379489037\n",
      "Gradient Descent(209/999): loss=[[10.30762218]], w0=-1.1740534949659573, w1=0.9196066064496018\n",
      "Gradient Descent(210/999): loss=[[10.3027463]], w0=-1.1789596693739084, w1=0.9200994848852637\n",
      "Gradient Descent(211/999): loss=[[10.29788799]], w0=-1.1838569999323731, w1=0.9205914748603171\n",
      "Gradient Descent(212/999): loss=[[10.29304718]], w0=-1.1887455025832376, w1=0.9210825779762974\n",
      "Gradient Descent(213/999): loss=[[10.2882238]], w0=-1.1936251932396513, w1=0.921572795831853\n",
      "Gradient Descent(214/999): loss=[[10.2834178]], w0=-1.1984960877860784, w1=0.9220621300227506\n",
      "Gradient Descent(215/999): loss=[[10.27862911]], w0=-1.2033582020783502, w1=0.9225505821418805\n",
      "Gradient Descent(216/999): loss=[[10.27385766]], w0=-1.208211551943716, w1=0.9230381537792615\n",
      "Gradient Descent(217/999): loss=[[10.2691034]], w0=-1.213056153180895, w1=0.9235248465220464\n",
      "Gradient Descent(218/999): loss=[[10.26436627]], w0=-1.217892021560128, w1=0.9240106619545269\n",
      "Gradient Descent(219/999): loss=[[10.2596462]], w0=-1.2227191728232283, w1=0.9244956016581389\n",
      "Gradient Descent(220/999): loss=[[10.25494313]], w0=-1.227537622683633, w1=0.9249796672114676\n",
      "Gradient Descent(221/999): loss=[[10.25025701]], w0=-1.232347386826454, w1=0.9254628601902526\n",
      "Gradient Descent(222/999): loss=[[10.24558776]], w0=-1.2371484809085298, w1=0.9259451821673932\n",
      "Gradient Descent(223/999): loss=[[10.24093533]], w0=-1.2419409205584755, w1=0.9264266347129532\n",
      "Gradient Descent(224/999): loss=[[10.23629965]], w0=-1.2467247213767343, w1=0.9269072193941664\n",
      "Gradient Descent(225/999): loss=[[10.23168068]], w0=-1.2514998989356283, w1=0.9273869377754415\n",
      "Gradient Descent(226/999): loss=[[10.22707834]], w0=-1.2562664687794085, w1=0.9278657914183671\n",
      "Gradient Descent(227/999): loss=[[10.22249258]], w0=-1.2610244464243063, w1=0.928343781881717\n",
      "Gradient Descent(228/999): loss=[[10.21792333]], w0=-1.2657738473585838, w1=0.928820910721455\n",
      "Gradient Descent(229/999): loss=[[10.21337055]], w0=-1.2705146870425834, w1=0.9292971794907403\n",
      "Gradient Descent(230/999): loss=[[10.20883416]], w0=-1.2752469809087792, w1=0.9297725897399323\n",
      "Gradient Descent(231/999): loss=[[10.20431412]], w0=-1.2799707443618265, w1=0.9302471430165956\n",
      "Gradient Descent(232/999): loss=[[10.19981035]], w0=-1.2846859927786125, w1=0.9307208408655054\n",
      "Gradient Descent(233/999): loss=[[10.19532281]], w0=-1.289392741508306, w1=0.9311936848286524\n",
      "Gradient Descent(234/999): loss=[[10.19085143]], w0=-1.2940910058724073, w1=0.9316656764452471\n",
      "Gradient Descent(235/999): loss=[[10.18639616]], w0=-1.2987808011647985, w1=0.932136817251726\n",
      "Gradient Descent(236/999): loss=[[10.18195693]], w0=-1.303462142651793, w1=0.932607108781756\n",
      "Gradient Descent(237/999): loss=[[10.17753369]], w0=-1.3081350455721847, w1=0.9330765525662391\n",
      "Gradient Descent(238/999): loss=[[10.17312639]], w0=-1.312799525137299, w1=0.9335451501333181\n",
      "Gradient Descent(239/999): loss=[[10.16873496]], w0=-1.3174555965310406, w1=0.9340129030083806\n",
      "Gradient Descent(240/999): loss=[[10.16435935]], w0=-1.3221032749099442, w1=0.9344798127140652\n",
      "Gradient Descent(241/999): loss=[[10.1599995]], w0=-1.3267425754032232, w1=0.9349458807702655\n",
      "Gradient Descent(242/999): loss=[[10.15565535]], w0=-1.331373513112819, w1=0.9354111086941351\n",
      "Gradient Descent(243/999): loss=[[10.15132685]], w0=-1.335996103113451, w1=0.9358754980000933\n",
      "Gradient Descent(244/999): loss=[[10.14701394]], w0=-1.3406103604526642, w1=0.9363390501998291\n",
      "Gradient Descent(245/999): loss=[[10.14271657]], w0=-1.3452163001508792, w1=0.9368017668023066\n",
      "Gradient Descent(246/999): loss=[[10.13843467]], w0=-1.349813937201441, w1=0.9372636493137702\n",
      "Gradient Descent(247/999): loss=[[10.1341682]], w0=-1.3544032865706677, w1=0.9377246992377486\n",
      "Gradient Descent(248/999): loss=[[10.1299171]], w0=-1.3589843631978988, w1=0.9381849180750605\n",
      "Gradient Descent(249/999): loss=[[10.12568131]], w0=-1.3635571819955448, w1=0.9386443073238195\n",
      "Gradient Descent(250/999): loss=[[10.12146077]], w0=-1.3681217578491345, w1=0.9391028684794385\n",
      "Gradient Descent(251/999): loss=[[10.11725544]], w0=-1.3726781056173645, w1=0.9395606030346343\n",
      "Gradient Descent(252/999): loss=[[10.11306526]], w0=-1.377226240132147, w1=0.9400175124794338\n",
      "Gradient Descent(253/999): loss=[[10.10889016]], w0=-1.3817661761986584, w1=0.9404735983011773\n",
      "Gradient Descent(254/999): loss=[[10.10473011]], w0=-1.3862979285953874, w1=0.9409288619845244\n",
      "Gradient Descent(255/999): loss=[[10.10058504]], w0=-1.3908215120741827, w1=0.9413833050114583\n",
      "Gradient Descent(256/999): loss=[[10.0964549]], w0=-1.3953369413603018, w1=0.9418369288612908\n",
      "Gradient Descent(257/999): loss=[[10.09233964]], w0=-1.3998442311524586, w1=0.9422897350106672\n",
      "Gradient Descent(258/999): loss=[[10.0882392]], w0=-1.4043433961228704, w1=0.9427417249335708\n",
      "Gradient Descent(259/999): loss=[[10.08415353]], w0=-1.4088344509173072, w1=0.9431929001013282\n",
      "Gradient Descent(260/999): loss=[[10.08008257]], w0=-1.4133174101551385, w1=0.9436432619826134\n",
      "Gradient Descent(261/999): loss=[[10.07602628]], w0=-1.4177922884293805, w1=0.9440928120434534\n",
      "Gradient Descent(262/999): loss=[[10.0719846]], w0=-1.4222591003067446, w1=0.9445415517472323\n",
      "Gradient Descent(263/999): loss=[[10.06795748]], w0=-1.426717860327684, w1=0.9449894825546963\n",
      "Gradient Descent(264/999): loss=[[10.06394486]], w0=-1.4311685830064413, w1=0.9454366059239585\n",
      "Gradient Descent(265/999): loss=[[10.0599467]], w0=-1.435611282831096, w1=0.9458829233105037\n",
      "Gradient Descent(266/999): loss=[[10.05596294]], w0=-1.4400459742636116, w1=0.9463284361671929\n",
      "Gradient Descent(267/999): loss=[[10.05199352]], w0=-1.4444726717398821, w1=0.9467731459442682\n",
      "Gradient Descent(268/999): loss=[[10.04803841]], w0=-1.4488913896697797, w1=0.9472170540893577\n",
      "Gradient Descent(269/999): loss=[[10.04409754]], w0=-1.4533021424372015, w1=0.9476601620474799\n",
      "Gradient Descent(270/999): loss=[[10.04017086]], w0=-1.4577049444001158, w1=0.9481024712610484\n",
      "Gradient Descent(271/999): loss=[[10.03625833]], w0=-1.4620998098906097, w1=0.9485439831698768\n",
      "Gradient Descent(272/999): loss=[[10.0323599]], w0=-1.4664867532149353, w1=0.9489846992111832\n",
      "Gradient Descent(273/999): loss=[[10.0284755]], w0=-1.470865788653556, w1=0.9494246208195952\n",
      "Gradient Descent(274/999): loss=[[10.02460509]], w0=-1.4752369304611939, w1=0.9498637494271542\n",
      "Gradient Descent(275/999): loss=[[10.02074863]], w0=-1.479600192866875, w1=0.9503020864633202\n",
      "Gradient Descent(276/999): loss=[[10.01690606]], w0=-1.4839555900739763, w1=0.9507396333549765\n",
      "Gradient Descent(277/999): loss=[[10.01307732]], w0=-1.4883031362602719, w1=0.9511763915264342\n",
      "Gradient Descent(278/999): loss=[[10.00926238]], w0=-1.4926428455779792, w1=0.9516123623994371\n",
      "Gradient Descent(279/999): loss=[[10.00546118]], w0=-1.4969747321538047, w1=0.9520475473931662\n",
      "Gradient Descent(280/999): loss=[[10.00167367]], w0=-1.5012988100889904, w1=0.9524819479242441\n",
      "Gradient Descent(281/999): loss=[[9.99789981]], w0=-1.505615093459359, w1=0.9529155654067399\n",
      "Gradient Descent(282/999): loss=[[9.99413953]], w0=-1.5099235963153608, w1=0.9533484012521737\n",
      "Gradient Descent(283/999): loss=[[9.99039281]], w0=-1.514224332682118, w1=0.9537804568695212\n",
      "Gradient Descent(284/999): loss=[[9.98665957]], w0=-1.518517316559472, w1=0.9542117336652183\n",
      "Gradient Descent(285/999): loss=[[9.98293979]], w0=-1.522802561922028, w1=0.9546422330431656\n",
      "Gradient Descent(286/999): loss=[[9.9792334]], w0=-1.5270800827192001, w1=0.9550719564047332\n",
      "Gradient Descent(287/999): loss=[[9.97554036]], w0=-1.5313498928752576, w1=0.955500905148765\n",
      "Gradient Descent(288/999): loss=[[9.97186062]], w0=-1.5356120062893701, w1=0.9559290806715831\n",
      "Gradient Descent(289/999): loss=[[9.96819414]], w0=-1.5398664368356523, w1=0.956356484366993\n",
      "Gradient Descent(290/999): loss=[[9.96454087]], w0=-1.5441131983632097, w1=0.9567831176262876\n",
      "Gradient Descent(291/999): loss=[[9.96090075]], w0=-1.5483523046961836, w1=0.9572089818382518\n",
      "Gradient Descent(292/999): loss=[[9.95727375]], w0=-1.5525837696337954, w1=0.957634078389167\n",
      "Gradient Descent(293/999): loss=[[9.95365981]], w0=-1.5568076069503929, w1=0.9580584086628159\n",
      "Gradient Descent(294/999): loss=[[9.95005888]], w0=-1.5610238303954935, w1=0.9584819740404867\n",
      "Gradient Descent(295/999): loss=[[9.94647093]], w0=-1.5652324536938302, w1=0.9589047759009777\n",
      "Gradient Descent(296/999): loss=[[9.9428959]], w0=-1.569433490545396, w1=0.9593268156206016\n",
      "Gradient Descent(297/999): loss=[[9.93933375]], w0=-1.573626954625488, w1=0.9597480945731905\n",
      "Gradient Descent(298/999): loss=[[9.93578443]], w0=-1.5778128595847523, w1=0.9601686141300999\n",
      "Gradient Descent(299/999): loss=[[9.93224789]], w0=-1.5819912190492287, w1=0.9605883756602133\n",
      "Gradient Descent(300/999): loss=[[9.92872409]], w0=-1.5861620466203945, w1=0.9610073805299465\n",
      "Gradient Descent(301/999): loss=[[9.92521299]], w0=-1.5903253558752093, w1=0.9614256301032524\n",
      "Gradient Descent(302/999): loss=[[9.92171453]], w0=-1.5944811603661584, w1=0.9618431257416252\n",
      "Gradient Descent(303/999): loss=[[9.91822867]], w0=-1.598629473621298, w1=0.9622598688041049\n",
      "Gradient Descent(304/999): loss=[[9.91475537]], w0=-1.6027703091442984, w1=0.9626758606472816\n",
      "Gradient Descent(305/999): loss=[[9.91129458]], w0=-1.6069036804144885, w1=0.9630911026253001\n",
      "Gradient Descent(306/999): loss=[[9.90784625]], w0=-1.611029600886899, w1=0.9635055960898642\n",
      "Gradient Descent(307/999): loss=[[9.90441034]], w0=-1.6151480839923067, w1=0.963919342390241\n",
      "Gradient Descent(308/999): loss=[[9.90098681]], w0=-1.6192591431372787, w1=0.9643323428732659\n",
      "Gradient Descent(309/999): loss=[[9.89757562]], w0=-1.6233627917042148, w1=0.9647445988833457\n",
      "Gradient Descent(310/999): loss=[[9.89417671]], w0=-1.627459043051392, w1=0.9651561117624645\n",
      "Gradient Descent(311/999): loss=[[9.89079004]], w0=-1.6315479105130077, w1=0.965566882850187\n",
      "Gradient Descent(312/999): loss=[[9.88741557]], w0=-1.6356294073992232, w1=0.9659769134836632\n",
      "Gradient Descent(313/999): loss=[[9.88405325]], w0=-1.6397035469962071, w1=0.966386204997633\n",
      "Gradient Descent(314/999): loss=[[9.88070305]], w0=-1.643770342566178, w1=0.9667947587244299\n",
      "Gradient Descent(315/999): loss=[[9.87736491]], w0=-1.6478298073474484, w1=0.9672025759939861\n",
      "Gradient Descent(316/999): loss=[[9.8740388]], w0=-1.6518819545544674, w1=0.9676096581338364\n",
      "Gradient Descent(317/999): loss=[[9.87072466]], w0=-1.6559267973778635, w1=0.9680160064691224\n",
      "Gradient Descent(318/999): loss=[[9.86742247]], w0=-1.6599643489844884, w1=0.9684216223225973\n",
      "Gradient Descent(319/999): loss=[[9.86413217]], w0=-1.6639946225174589, w1=0.9688265070146297\n",
      "Gradient Descent(320/999): loss=[[9.86085372]], w0=-1.6680176310962, w1=0.969230661863208\n",
      "Gradient Descent(321/999): loss=[[9.85758708]], w0=-1.6720333878164881, w1=0.9696340881839453\n",
      "Gradient Descent(322/999): loss=[[9.8543322]], w0=-1.676041905750493, w1=0.9700367872900825\n",
      "Gradient Descent(323/999): loss=[[9.85108905]], w0=-1.6800431979468202, w1=0.9704387604924937\n",
      "Gradient Descent(324/999): loss=[[9.84785758]], w0=-1.6840372774305545, w1=0.9708400090996899\n",
      "Gradient Descent(325/999): loss=[[9.84463775]], w0=-1.6880241572033017, w1=0.9712405344178237\n",
      "Gradient Descent(326/999): loss=[[9.84142952]], w0=-1.6920038502432304, w1=0.9716403377506925\n",
      "Gradient Descent(327/999): loss=[[9.83823284]], w0=-1.695976369505115, w1=0.9720394203997442\n",
      "Gradient Descent(328/999): loss=[[9.83504768]], w0=-1.6999417279203783, w1=0.9724377836640803\n",
      "Gradient Descent(329/999): loss=[[9.83187399]], w0=-1.7038999383971323, w1=0.9728354288404608\n",
      "Gradient Descent(330/999): loss=[[9.82871174]], w0=-1.707851013820221, w1=0.973232357223308\n",
      "Gradient Descent(331/999): loss=[[9.82556087]], w0=-1.7117949670512624, w1=0.973628570104711\n",
      "Gradient Descent(332/999): loss=[[9.82242135]], w0=-1.71573181092869, w1=0.9740240687744296\n",
      "Gradient Descent(333/999): loss=[[9.81929314]], w0=-1.7196615582677952, w1=0.9744188545198987\n",
      "Gradient Descent(334/999): loss=[[9.8161762]], w0=-1.723584221860768, w1=0.9748129286262328\n",
      "Gradient Descent(335/999): loss=[[9.81307048]], w0=-1.7274998144767397, w1=0.9752062923762296\n",
      "Gradient Descent(336/999): loss=[[9.80997595]], w0=-1.731408348861824, w1=0.9755989470503743\n",
      "Gradient Descent(337/999): loss=[[9.80689257]], w0=-1.7353098377391583, w1=0.9759908939268442\n",
      "Gradient Descent(338/999): loss=[[9.8038203]], w0=-1.7392042938089454, w1=0.9763821342815124\n",
      "Gradient Descent(339/999): loss=[[9.80075909]], w0=-1.743091729748495, w1=0.9767726693879522\n",
      "Gradient Descent(340/999): loss=[[9.7977089]], w0=-1.7469721582122641, w1=0.977162500517441\n",
      "Gradient Descent(341/999): loss=[[9.79466971]], w0=-1.7508455918318997, w1=0.9775516289389647\n",
      "Gradient Descent(342/999): loss=[[9.79164146]], w0=-1.7547120432162784, w1=0.977940055919222\n",
      "Gradient Descent(343/999): loss=[[9.78862411]], w0=-1.7585715249515483, w1=0.9783277827226277\n",
      "Gradient Descent(344/999): loss=[[9.78561764]], w0=-1.7624240496011698, w1=0.9787148106113178\n",
      "Gradient Descent(345/999): loss=[[9.78262199]], w0=-1.7662696297059566, w1=0.979101140845153\n",
      "Gradient Descent(346/999): loss=[[9.77963714]], w0=-1.770108277784116, w1=0.979486774681723\n",
      "Gradient Descent(347/999): loss=[[9.77666304]], w0=-1.77394000633129, w1=0.9798717133763506\n",
      "Gradient Descent(348/999): loss=[[9.77369964]], w0=-1.7777648278205966, w1=0.980255958182096\n",
      "Gradient Descent(349/999): loss=[[9.77074693]], w0=-1.7815827547026695, w1=0.98063951034976\n",
      "Gradient Descent(350/999): loss=[[9.76780485]], w0=-1.7853937994056985, w1=0.9810223711278895\n",
      "Gradient Descent(351/999): loss=[[9.76487336]], w0=-1.7891979743354711, w1=0.98140454176278\n",
      "Gradient Descent(352/999): loss=[[9.76195244]], w0=-1.7929952918754117, w1=0.9817860234984812\n",
      "Gradient Descent(353/999): loss=[[9.75904203]], w0=-1.7967857643866227, w1=0.9821668175767997\n",
      "Gradient Descent(354/999): loss=[[9.75614211]], w0=-1.8005694042079243, w1=0.9825469252373038\n",
      "Gradient Descent(355/999): loss=[[9.75325264]], w0=-1.8043462236558947, w1=0.9829263477173273\n",
      "Gradient Descent(356/999): loss=[[9.75037357]], w0=-1.8081162350249103, w1=0.983305086251974\n",
      "Gradient Descent(357/999): loss=[[9.74750487]], w0=-1.8118794505871858, w1=0.9836831420741207\n",
      "Gradient Descent(358/999): loss=[[9.7446465]], w0=-1.815635882592814, w1=0.9840605164144222\n",
      "Gradient Descent(359/999): loss=[[9.74179844]], w0=-1.819385543269806, w1=0.9844372105013148\n",
      "Gradient Descent(360/999): loss=[[9.73896062]], w0=-1.8231284448241303, w1=0.9848132255610205\n",
      "Gradient Descent(361/999): loss=[[9.73613304]], w0=-1.8268645994397532, w1=0.9851885628175507\n",
      "Gradient Descent(362/999): loss=[[9.73331563]], w0=-1.8305940192786783, w1=0.9855632234927107\n",
      "Gradient Descent(363/999): loss=[[9.73050838]], w0=-1.8343167164809857, w1=0.9859372088061034\n",
      "Gradient Descent(364/999): loss=[[9.72771123]], w0=-1.8380327031648724, w1=0.9863105199751329\n",
      "Gradient Descent(365/999): loss=[[9.72492416]], w0=-1.8417419914266906, w1=0.986683158215009\n",
      "Gradient Descent(366/999): loss=[[9.72214713]], w0=-1.845444593340988, w1=0.9870551247387511\n",
      "Gradient Descent(367/999): loss=[[9.7193801]], w0=-1.8491405209605467, w1=0.9874264207571918\n",
      "Gradient Descent(368/999): loss=[[9.71662304]], w0=-1.8528297863164227, w1=0.9877970474789811\n",
      "Gradient Descent(369/999): loss=[[9.71387591]], w0=-1.8565124014179843, w1=0.9881670061105906\n",
      "Gradient Descent(370/999): loss=[[9.71113868]], w0=-1.8601883782529525, w1=0.9885362978563166\n",
      "Gradient Descent(371/999): loss=[[9.7084113]], w0=-1.8638577287874387, w1=0.988904923918285\n",
      "Gradient Descent(372/999): loss=[[9.70569375]], w0=-1.8675204649659847, w1=0.9892728854964544\n",
      "Gradient Descent(373/999): loss=[[9.70298598]], w0=-1.8711765987116007, w1=0.9896401837886206\n",
      "Gradient Descent(374/999): loss=[[9.70028797]], w0=-1.8748261419258045, w1=0.9900068199904203\n",
      "Gradient Descent(375/999): loss=[[9.69759968]], w0=-1.878469106488661, w1=0.9903727952953345\n",
      "Gradient Descent(376/999): loss=[[9.69492107]], w0=-1.882105504258819, w1=0.9907381108946934\n",
      "Gradient Descent(377/999): loss=[[9.69225211]], w0=-1.885735347073552, w1=0.9911027679776796\n",
      "Gradient Descent(378/999): loss=[[9.68959276]], w0=-1.8893586467487953, w1=0.9914667677313317\n",
      "Gradient Descent(379/999): loss=[[9.68694299]], w0=-1.8929754150791847, w1=0.9918301113405489\n",
      "Gradient Descent(380/999): loss=[[9.68430277]], w0=-1.896585663838095, w1=0.9921927999880944\n",
      "Gradient Descent(381/999): loss=[[9.68167205]], w0=-1.9001894047776786, w1=0.9925548348545994\n",
      "Gradient Descent(382/999): loss=[[9.67905082]], w0=-1.9037866496289035, w1=0.9929162171185668\n",
      "Gradient Descent(383/999): loss=[[9.67643902]], w0=-1.9073774101015915, w1=0.9932769479563754\n",
      "Gradient Descent(384/999): loss=[[9.67383663]], w0=-1.910961697884456, w1=0.9936370285422832\n",
      "Gradient Descent(385/999): loss=[[9.67124361]], w0=-1.9145395246451407, w1=0.9939964600484313\n",
      "Gradient Descent(386/999): loss=[[9.66865994]], w0=-1.9181109020302571, w1=0.9943552436448486\n",
      "Gradient Descent(387/999): loss=[[9.66608557]], w0=-1.921675841665423, w1=0.9947133804994541\n",
      "Gradient Descent(388/999): loss=[[9.66352047]], w0=-1.9252343551552993, w1=0.9950708717780621\n",
      "Gradient Descent(389/999): loss=[[9.66096461]], w0=-1.9287864540836286, w1=0.9954277186443851\n",
      "Gradient Descent(390/999): loss=[[9.65841796]], w0=-1.9323321500132729, w1=0.995783922260038\n",
      "Gradient Descent(391/999): loss=[[9.65588048]], w0=-1.9358714544862508, w1=0.9961394837845419\n",
      "Gradient Descent(392/999): loss=[[9.65335214]], w0=-1.9394043790237754, w1=0.9964944043753273\n",
      "Gradient Descent(393/999): loss=[[9.65083291]], w0=-1.9429309351262916, w1=0.9968486851877391\n",
      "Gradient Descent(394/999): loss=[[9.64832275]], w0=-1.9464511342735138, w1=0.9972023273750388\n",
      "Gradient Descent(395/999): loss=[[9.64582163]], w0=-1.9499649879244632, w1=0.9975553320884093\n",
      "Gradient Descent(396/999): loss=[[9.64332953]], w0=-1.9534725075175046, w1=0.9979077004769586\n",
      "Gradient Descent(397/999): loss=[[9.64084639]], w0=-1.9569737044703845, w1=0.9982594336877233\n",
      "Gradient Descent(398/999): loss=[[9.63837221]], w0=-1.9604685901802676, w1=0.998610532865672\n",
      "Gradient Descent(399/999): loss=[[9.63590693]], w0=-1.9639571760237742, w1=0.9989609991537096\n",
      "Gradient Descent(400/999): loss=[[9.63345054]], w0=-1.9674394733570169, w1=0.9993108336926809\n",
      "Gradient Descent(401/999): loss=[[9.63100299]], w0=-1.970915493515638, w1=0.9996600376213741\n",
      "Gradient Descent(402/999): loss=[[9.62856426]], w0=-1.9743852478148467, w1=1.0000086120765248\n",
      "Gradient Descent(403/999): loss=[[9.62613431]], w0=-1.9778487475494546, w1=1.000356558192819\n",
      "Gradient Descent(404/999): loss=[[9.62371311]], w0=-1.9813060039939139, w1=1.0007038771028982\n",
      "Gradient Descent(405/999): loss=[[9.62130064]], w0=-1.984757028402353, w1=1.0010505699373613\n",
      "Gradient Descent(406/999): loss=[[9.61889685]], w0=-1.9882018320086143, w1=1.0013966378247696\n",
      "Gradient Descent(407/999): loss=[[9.61650173]], w0=-1.9916404260262899, w1=1.0017420818916503\n",
      "Gradient Descent(408/999): loss=[[9.61411523]], w0=-1.9950728216487579, w1=1.0020869032624995\n",
      "Gradient Descent(409/999): loss=[[9.61173733]], w0=-1.9984990300492198, w1=1.0024311030597866\n",
      "Gradient Descent(410/999): loss=[[9.60936799]], w0=-2.001919062380736, w1=1.0027746824039572\n",
      "Gradient Descent(411/999): loss=[[9.60700718]], w0=-2.0053329297762628, w1=1.0031176424134378\n",
      "Gradient Descent(412/999): loss=[[9.60465488]], w0=-2.008740643348688, w1=1.0034599842046386\n",
      "Gradient Descent(413/999): loss=[[9.60231106]], w0=-2.0121422141908676, w1=1.0038017088919569\n",
      "Gradient Descent(414/999): loss=[[9.59997567]], w0=-2.015537653375661, w1=1.004142817587782\n",
      "Gradient Descent(415/999): loss=[[9.5976487]], w0=-2.018926971955968, w1=1.0044833114024971\n",
      "Gradient Descent(416/999): loss=[[9.59533011]], w0=-2.0223101809647654, w1=1.004823191444485\n",
      "Gradient Descent(417/999): loss=[[9.59301987]], w0=-2.025687291415141, w1=1.0051624588201293\n",
      "Gradient Descent(418/999): loss=[[9.59071795]], w0=-2.0290583143003307, w1=1.00550111463382\n",
      "Gradient Descent(419/999): loss=[[9.58842432]], w0=-2.0324232605937538, w1=1.005839159987956\n",
      "Gradient Descent(420/999): loss=[[9.58613895]], w0=-2.0357821412490495, w1=1.0061765959829492\n",
      "Gradient Descent(421/999): loss=[[9.58386182]], w0=-2.039134967200112, w1=1.006513423717228\n",
      "Gradient Descent(422/999): loss=[[9.58159289]], w0=-2.042481749361125, w1=1.0068496442872403\n",
      "Gradient Descent(423/999): loss=[[9.57933213]], w0=-2.0458224986266003, w1=1.0071852587874581\n",
      "Gradient Descent(424/999): loss=[[9.57707951]], w0=-2.0491572258714092, w1=1.0075202683103803\n",
      "Gradient Descent(425/999): loss=[[9.57483501]], w0=-2.0524859419508217, w1=1.0078546739465364\n",
      "Gradient Descent(426/999): loss=[[9.57259859]], w0=-2.055808657700539, w1=1.0081884767844902\n",
      "Gradient Descent(427/999): loss=[[9.57037023]], w0=-2.0591253839367307, w1=1.008521677910843\n",
      "Gradient Descent(428/999): loss=[[9.56814989]], w0=-2.0624361314560686, w1=1.008854278410238\n",
      "Gradient Descent(429/999): loss=[[9.56593755]], w0=-2.0657409110357627, w1=1.0091862793653623\n",
      "Gradient Descent(430/999): loss=[[9.56373318]], w0=-2.069039733433596, w1=1.0095176818569525\n",
      "Gradient Descent(431/999): loss=[[9.56153675]], w0=-2.07233260938796, w1=1.0098484869637963\n",
      "Gradient Descent(432/999): loss=[[9.55934823]], w0=-2.075619549617888, w1=1.0101786957627366\n",
      "Gradient Descent(433/999): loss=[[9.5571676]], w0=-2.078900564823093, w1=1.0105083093286757\n",
      "Gradient Descent(434/999): loss=[[9.55499482]], w0=-2.082175665684, w1=1.0108373287345784\n",
      "Gradient Descent(435/999): loss=[[9.55282986]], w0=-2.08544486286178, w1=1.0111657550514748\n",
      "Gradient Descent(436/999): loss=[[9.55067271]], w0=-2.0887081669983885, w1=1.0114935893484647\n",
      "Gradient Descent(437/999): loss=[[9.54852332]], w0=-2.091965588716597, w1=1.011820832692721\n",
      "Gradient Descent(438/999): loss=[[9.54638167]], w0=-2.095217138620028, w1=1.0121474861494921\n",
      "Gradient Descent(439/999): loss=[[9.54424774]], w0=-2.09846282729319, w1=1.0124735507821072\n",
      "Gradient Descent(440/999): loss=[[9.5421215]], w0=-2.1017026653015125, w1=1.012799027651978\n",
      "Gradient Descent(441/999): loss=[[9.54000291]], w0=-2.1049366631913795, w1=1.0131239178186033\n",
      "Gradient Descent(442/999): loss=[[9.53789196]], w0=-2.108164831490164, w1=1.0134482223395718\n",
      "Gradient Descent(443/999): loss=[[9.53578861]], w0=-2.111387180706263, w1=1.013771942270566\n",
      "Gradient Descent(444/999): loss=[[9.53369283]], w0=-2.11460372132913, w1=1.014095078665365\n",
      "Gradient Descent(445/999): loss=[[9.53160461]], w0=-2.117814463829311, w1=1.0144176325758494\n",
      "Gradient Descent(446/999): loss=[[9.5295239]], w0=-2.121019418658478, w1=1.0147396050520026\n",
      "Gradient Descent(447/999): loss=[[9.52745069]], w0=-2.1242185962494626, w1=1.0150609971419156\n",
      "Gradient Descent(448/999): loss=[[9.52538495]], w0=-2.1274120070162903, w1=1.0153818098917904\n",
      "Gradient Descent(449/999): loss=[[9.52332665]], w0=-2.1305996613542137, w1=1.0157020443459428\n",
      "Gradient Descent(450/999): loss=[[9.52127576]], w0=-2.1337815696397477, w1=1.0160217015468063\n",
      "Gradient Descent(451/999): loss=[[9.51923226]], w0=-2.1369577422307025, w1=1.0163407825349353\n",
      "Gradient Descent(452/999): loss=[[9.51719612]], w0=-2.140128189466217, w1=1.0166592883490084\n",
      "Gradient Descent(453/999): loss=[[9.51516731]], w0=-2.1432929216667933, w1=1.016977220025832\n",
      "Gradient Descent(454/999): loss=[[9.51314581]], w0=-2.1464519491343292, w1=1.0172945786003436\n",
      "Gradient Descent(455/999): loss=[[9.5111316]], w0=-2.149605282152153, w1=1.0176113651056147\n",
      "Gradient Descent(456/999): loss=[[9.50912463]], w0=-2.1527529309850553, w1=1.0179275805728554\n",
      "Gradient Descent(457/999): loss=[[9.5071249]], w0=-2.155894905879325, w1=1.018243226031416\n",
      "Gradient Descent(458/999): loss=[[9.50513237]], w0=-2.159031217062779, w1=1.0185583025087923\n",
      "Gradient Descent(459/999): loss=[[9.50314702]], w0=-2.1621618747448, w1=1.0188728110306269\n",
      "Gradient Descent(460/999): loss=[[9.50116881]], w0=-2.165286889116365, w1=1.0191867526207143\n",
      "Gradient Descent(461/999): loss=[[9.49919774]], w0=-2.1684062703500824, w1=1.0195001283010032\n",
      "Gradient Descent(462/999): loss=[[9.49723376]], w0=-2.1715200286002228, w1=1.0198129390916004\n",
      "Gradient Descent(463/999): loss=[[9.49527686]], w0=-2.174628174002753, w1=1.0201251860107736\n",
      "Gradient Descent(464/999): loss=[[9.493327]], w0=-2.1777307166753683, w1=1.0204368700749549\n",
      "Gradient Descent(465/999): loss=[[9.49138417]], w0=-2.180827666717527, w1=1.0207479922987446\n",
      "Gradient Descent(466/999): loss=[[9.48944834]], w0=-2.1839190342104806, w1=1.021058553694914\n",
      "Gradient Descent(467/999): loss=[[9.48751948]], w0=-2.1870048292173094, w1=1.0213685552744083\n",
      "Gradient Descent(468/999): loss=[[9.48559757]], w0=-2.1900850617829537, w1=1.0216779980463508\n",
      "Gradient Descent(469/999): loss=[[9.48368258]], w0=-2.1931597419342466, w1=1.0219868830180456\n",
      "Gradient Descent(470/999): loss=[[9.48177449]], w0=-2.1962288796799467, w1=1.0222952111949812\n",
      "Gradient Descent(471/999): loss=[[9.47987327]], w0=-2.1992924850107713, w1=1.0226029835808335\n",
      "Gradient Descent(472/999): loss=[[9.47797889]], w0=-2.2023505678994284, w1=1.0229102011774691\n",
      "Gradient Descent(473/999): loss=[[9.47609135]], w0=-2.205403138300649, w1=1.023216864984949\n",
      "Gradient Descent(474/999): loss=[[9.4742106]], w0=-2.2084502061512206, w1=1.0235229760015305\n",
      "Gradient Descent(475/999): loss=[[9.47233662]], w0=-2.2114917813700172, w1=1.0238285352236731\n",
      "Gradient Descent(476/999): loss=[[9.4704694]], w0=-2.2145278738580343, w1=1.0241335436460386\n",
      "Gradient Descent(477/999): loss=[[9.4686089]], w0=-2.2175584934984194, w1=1.0244380022614963\n",
      "Gradient Descent(478/999): loss=[[9.4667551]], w0=-2.220583650156505, w1=1.024741912061126\n",
      "Gradient Descent(479/999): loss=[[9.46490798]], w0=-2.22360335367984, w1=1.025045274034221\n",
      "Gradient Descent(480/999): loss=[[9.46306751]], w0=-2.226617613898222, w1=1.0253480891682911\n",
      "Gradient Descent(481/999): loss=[[9.46123368]], w0=-2.22962644062373, w1=1.0256503584490657\n",
      "Gradient Descent(482/999): loss=[[9.45940644]], w0=-2.2326298436507557, w1=1.025952082860498\n",
      "Gradient Descent(483/999): loss=[[9.45758579]], w0=-2.2356278327560353, w1=1.026253263384767\n",
      "Gradient Descent(484/999): loss=[[9.4557717]], w0=-2.238620417698681, w1=1.0265539010022813\n",
      "Gradient Descent(485/999): loss=[[9.45396414]], w0=-2.2416076082202143, w1=1.0268539966916828\n",
      "Gradient Descent(486/999): loss=[[9.45216309]], w0=-2.2445894140445963, w1=1.0271535514298484\n",
      "Gradient Descent(487/999): loss=[[9.45036853]], w0=-2.247565844878259, w1=1.027452566191895\n",
      "Gradient Descent(488/999): loss=[[9.44858043]], w0=-2.2505369104101387, w1=1.0277510419511808\n",
      "Gradient Descent(489/999): loss=[[9.44679878]], w0=-2.2535026203117057, w1=1.0280489796793102\n",
      "Gradient Descent(490/999): loss=[[9.44502354]], w0=-2.256462984236997, w1=1.0283463803461357\n",
      "Gradient Descent(491/999): loss=[[9.44325469]], w0=-2.2594180118226475, w1=1.0286432449197622\n",
      "Gradient Descent(492/999): loss=[[9.44149222]], w0=-2.26236771268792, w1=1.0289395743665486\n",
      "Gradient Descent(493/999): loss=[[9.43973609]], w0=-2.2653120964347386, w1=1.0292353696511127\n",
      "Gradient Descent(494/999): loss=[[9.4379863]], w0=-2.268251172647719, w1=1.0295306317363329\n",
      "Gradient Descent(495/999): loss=[[9.4362428]], w0=-2.2711849508941993, w1=1.029825361583352\n",
      "Gradient Descent(496/999): loss=[[9.43450558]], w0=-2.2741134407242716, w1=1.0301195601515802\n",
      "Gradient Descent(497/999): loss=[[9.43277462]], w0=-2.2770366516708136, w1=1.0304132283986989\n",
      "Gradient Descent(498/999): loss=[[9.4310499]], w0=-2.2799545932495184, w1=1.0307063672806624\n",
      "Gradient Descent(499/999): loss=[[9.42933138]], w0=-2.2828672749589267, w1=1.0309989777517024\n",
      "Gradient Descent(500/999): loss=[[9.42761906]], w0=-2.2857747062804568, w1=1.03129106076433\n",
      "Gradient Descent(501/999): loss=[[9.42591291]], w0=-2.288676896678436, w1=1.0315826172693394\n",
      "Gradient Descent(502/999): loss=[[9.4242129]], w0=-2.2915738556001313, w1=1.0318736482158113\n",
      "Gradient Descent(503/999): loss=[[9.42251901]], w0=-2.2944655924757797, w1=1.032164154551115\n",
      "Gradient Descent(504/999): loss=[[9.42083123]], w0=-2.2973521167186197, w1=1.0324541372209128\n",
      "Gradient Descent(505/999): loss=[[9.41914952]], w0=-2.3002334377249216, w1=1.0327435971691619\n",
      "Gradient Descent(506/999): loss=[[9.41747387]], w0=-2.303109564874018, w1=1.0330325353381176\n",
      "Gradient Descent(507/999): loss=[[9.41580426]], w0=-2.3059805075283335, w1=1.0333209526683376\n",
      "Gradient Descent(508/999): loss=[[9.41414066]], w0=-2.3088462750334173, w1=1.033608850098683\n",
      "Gradient Descent(509/999): loss=[[9.41248306]], w0=-2.3117068767179716, w1=1.0338962285663238\n",
      "Gradient Descent(510/999): loss=[[9.41083142]], w0=-2.3145623218938827, w1=1.0341830890067396\n",
      "Gradient Descent(511/999): loss=[[9.40918573]], w0=-2.3174126198562517, w1=1.0344694323537242\n",
      "Gradient Descent(512/999): loss=[[9.40754598]], w0=-2.3202577798834243, w1=1.034755259539388\n",
      "Gradient Descent(513/999): loss=[[9.40591212]], w0=-2.3230978112370213, w1=1.0350405714941615\n",
      "Gradient Descent(514/999): loss=[[9.40428416]], w0=-2.325932723161968, w1=1.0353253691467976\n",
      "Gradient Descent(515/999): loss=[[9.40266205]], w0=-2.3287625248865247, w1=1.0356096534243748\n",
      "Gradient Descent(516/999): loss=[[9.40104579]], w0=-2.3315872256223176, w1=1.0358934252523015\n",
      "Gradient Descent(517/999): loss=[[9.39943535]], w0=-2.334406834564368, w1=1.0361766855543169\n",
      "Gradient Descent(518/999): loss=[[9.39783071]], w0=-2.3372213608911214, w1=1.0364594352524954\n",
      "Gradient Descent(519/999): loss=[[9.39623185]], w0=-2.3400308137644794, w1=1.0367416752672496\n",
      "Gradient Descent(520/999): loss=[[9.39463875]], w0=-2.3428352023298276, w1=1.0370234065173325\n",
      "Gradient Descent(521/999): loss=[[9.39305139]], w0=-2.3456345357160666, w1=1.0373046299198414\n",
      "Gradient Descent(522/999): loss=[[9.39146975]], w0=-2.348428823035641, w1=1.03758534639022\n",
      "Gradient Descent(523/999): loss=[[9.3898938]], w0=-2.35121807338457, w1=1.0378655568422621\n",
      "Gradient Descent(524/999): loss=[[9.38832353]], w0=-2.354002295842475, w1=1.0381452621881142\n",
      "Gradient Descent(525/999): loss=[[9.38675892]], w0=-2.356781499472612, w1=1.0384244633382789\n",
      "Gradient Descent(526/999): loss=[[9.38519994]], w0=-2.359555693321899, w1=1.0387031612016169\n",
      "Gradient Descent(527/999): loss=[[9.38364657]], w0=-2.3623248864209456, w1=1.038981356685351\n",
      "Gradient Descent(528/999): loss=[[9.38209881]], w0=-2.3650890877840833, w1=1.0392590506950687\n",
      "Gradient Descent(529/999): loss=[[9.38055661]], w0=-2.3678483064093947, w1=1.039536244134725\n",
      "Gradient Descent(530/999): loss=[[9.37901997]], w0=-2.370602551278742, w1=1.039812937906645\n",
      "Gradient Descent(531/999): loss=[[9.37748887]], w0=-2.3733518313577973, w1=1.040089132911528\n",
      "Gradient Descent(532/999): loss=[[9.37596328]], w0=-2.3760961555960702, w1=1.0403648300484494\n",
      "Gradient Descent(533/999): loss=[[9.37444319]], w0=-2.378835532926939, w1=1.0406400302148637\n",
      "Gradient Descent(534/999): loss=[[9.37292857]], w0=-2.381569972267678, w1=1.0409147343066083\n",
      "Gradient Descent(535/999): loss=[[9.37141941]], w0=-2.3842994825194883, w1=1.041188943217905\n",
      "Gradient Descent(536/999): loss=[[9.36991568]], w0=-2.3870240725675242, w1=1.0414626578413637\n",
      "Gradient Descent(537/999): loss=[[9.36841737]], w0=-2.3897437512809248, w1=1.041735879067986\n",
      "Gradient Descent(538/999): loss=[[9.36692446]], w0=-2.392458527512841, w1=1.042008607787167\n",
      "Gradient Descent(539/999): loss=[[9.36543692]], w0=-2.395168410100466, w1=1.0422808448866987\n",
      "Gradient Descent(540/999): loss=[[9.36395474]], w0=-2.3978734078650623, w1=1.042552591252772\n",
      "Gradient Descent(541/999): loss=[[9.3624779]], w0=-2.4005735296119917, w1=1.0428238477699816\n",
      "Gradient Descent(542/999): loss=[[9.36100638]], w0=-2.4032687841307427, w1=1.0430946153213267\n",
      "Gradient Descent(543/999): loss=[[9.35954016]], w0=-2.405959180194961, w1=1.0433648947882153\n",
      "Gradient Descent(544/999): loss=[[9.35807922]], w0=-2.408644726562476, w1=1.043634687050466\n",
      "Gradient Descent(545/999): loss=[[9.35662355]], w0=-2.4113254319753312, w1=1.0439039929863123\n",
      "Gradient Descent(546/999): loss=[[9.35517311]], w0=-2.414001305159811, w1=1.0441728134724042\n",
      "Gradient Descent(547/999): loss=[[9.35372791]], w0=-2.41667235482647, w1=1.044441149383811\n",
      "Gradient Descent(548/999): loss=[[9.3522879]], w0=-2.419338589670162, w1=1.0447090015940252\n",
      "Gradient Descent(549/999): loss=[[9.35085309]], w0=-2.4220000183700656, w1=1.0449763709749644\n",
      "Gradient Descent(550/999): loss=[[9.34942344]], w0=-2.4246566495897164, w1=1.045243258396975\n",
      "Gradient Descent(551/999): loss=[[9.34799894]], w0=-2.4273084919770316, w1=1.045509664728834\n",
      "Gradient Descent(552/999): loss=[[9.34657957]], w0=-2.4299555541643407, w1=1.0457755908377522\n",
      "Gradient Descent(553/999): loss=[[9.34516532]], w0=-2.4325978447684125, w1=1.0460410375893778\n",
      "Gradient Descent(554/999): loss=[[9.34375616]], w0=-2.4352353723904825, w1=1.0463060058477982\n",
      "Gradient Descent(555/999): loss=[[9.34235207]], w0=-2.4378681456162825, w1=1.046570496475543\n",
      "Gradient Descent(556/999): loss=[[9.34095304]], w0=-2.4404961730160672, w1=1.0468345103335874\n",
      "Gradient Descent(557/999): loss=[[9.33955905]], w0=-2.443119463144643, w1=1.0470980482813548\n",
      "Gradient Descent(558/999): loss=[[9.33817009]], w0=-2.4457380245413947, w1=1.0473611111767187\n",
      "Gradient Descent(559/999): loss=[[9.33678612]], w0=-2.4483518657303147, w1=1.0476236998760067\n",
      "Gradient Descent(560/999): loss=[[9.33540714]], w0=-2.45096099522003, w1=1.0478858152340025\n",
      "Gradient Descent(561/999): loss=[[9.33403313]], w0=-2.4535654215038303, w1=1.0481474581039494\n",
      "Gradient Descent(562/999): loss=[[9.33266407]], w0=-2.456165153059694, w1=1.0484086293375523\n",
      "Gradient Descent(563/999): loss=[[9.33129993]], w0=-2.458760198350319, w1=1.0486693297849807\n",
      "Gradient Descent(564/999): loss=[[9.32994071]], w0=-2.461350565823147, w1=1.048929560294872\n",
      "Gradient Descent(565/999): loss=[[9.32858639]], w0=-2.4639362639103926, w1=1.0491893217143338\n",
      "Gradient Descent(566/999): loss=[[9.32723695]], w0=-2.466517301029071, w1=1.0494486148889461\n",
      "Gradient Descent(567/999): loss=[[9.32589236]], w0=-2.469093685581025, w1=1.0497074406627656\n",
      "Gradient Descent(568/999): loss=[[9.32455262]], w0=-2.471665425952951, w1=1.0499657998783265\n",
      "Gradient Descent(569/999): loss=[[9.32321771]], w0=-2.474232530516429, w1=1.0502236933766451\n",
      "Gradient Descent(570/999): loss=[[9.3218876]], w0=-2.4767950076279486, w1=1.0504811219972212\n",
      "Gradient Descent(571/999): loss=[[9.32056228]], w0=-2.4793528656289343, w1=1.0507380865780418\n",
      "Gradient Descent(572/999): loss=[[9.31924174]], w0=-2.4819061128457762, w1=1.0509945879555826\n",
      "Gradient Descent(573/999): loss=[[9.31792595]], w0=-2.4844547575898543, w1=1.0512506269648123\n",
      "Gradient Descent(574/999): loss=[[9.31661491]], w0=-2.4869988081575665, w1=1.0515062044391936\n",
      "Gradient Descent(575/999): loss=[[9.31530858]], w0=-2.489538272830356, w1=1.0517613212106878\n",
      "Gradient Descent(576/999): loss=[[9.31400697]], w0=-2.4920731598747383, w1=1.0520159781097556\n",
      "Gradient Descent(577/999): loss=[[9.31271003]], w0=-2.4946034775423267, w1=1.0522701759653612\n",
      "Gradient Descent(578/999): loss=[[9.31141778]], w0=-2.497129234069861, w1=1.0525239156049746\n",
      "Gradient Descent(579/999): loss=[[9.31013017]], w0=-2.4996504376792332, w1=1.052777197854574\n",
      "Gradient Descent(580/999): loss=[[9.3088472]], w0=-2.5021670965775145, w1=1.0530300235386487\n",
      "Gradient Descent(581/999): loss=[[9.30756886]], w0=-2.504679218956982, w1=1.0532823934802016\n",
      "Gradient Descent(582/999): loss=[[9.30629512]], w0=-2.507186812995146, w1=1.0535343085007525\n",
      "Gradient Descent(583/999): loss=[[9.30502597]], w0=-2.509689886854775, w1=1.05378576942034\n",
      "Gradient Descent(584/999): loss=[[9.30376138]], w0=-2.512188448683924, w1=1.0540367770575247\n",
      "Gradient Descent(585/999): loss=[[9.30250136]], w0=-2.514682506615961, w1=1.0542873322293915\n",
      "Gradient Descent(586/999): loss=[[9.30124587]], w0=-2.5171720687695913, w1=1.0545374357515525\n",
      "Gradient Descent(587/999): loss=[[9.2999949]], w0=-2.5196571432488866, w1=1.0547870884381496\n",
      "Gradient Descent(588/999): loss=[[9.29874844]], w0=-2.52213773814331, w1=1.055036291101857\n",
      "Gradient Descent(589/999): loss=[[9.29750647]], w0=-2.524613861527742, w1=1.055285044553884\n",
      "Gradient Descent(590/999): loss=[[9.29626898]], w0=-2.5270855214625083, w1=1.055533349603978\n",
      "Gradient Descent(591/999): loss=[[9.29503594]], w0=-2.5295527259934047, w1=1.0557812070604262\n",
      "Gradient Descent(592/999): loss=[[9.29380734]], w0=-2.5320154831517234, w1=1.0560286177300593\n",
      "Gradient Descent(593/999): loss=[[9.29258317]], w0=-2.5344738009542795, w1=1.056275582418253\n",
      "Gradient Descent(594/999): loss=[[9.2913634]], w0=-2.5369276874034377, w1=1.0565221019289317\n",
      "Gradient Descent(595/999): loss=[[9.29014803]], w0=-2.5393771504871365, w1=1.0567681770645707\n",
      "Gradient Descent(596/999): loss=[[9.28893704]], w0=-2.541822198178916, w1=1.0570138086261986\n",
      "Gradient Descent(597/999): loss=[[9.28773041]], w0=-2.5442628384379438, w1=1.0572589974134\n",
      "Gradient Descent(598/999): loss=[[9.28652813]], w0=-2.546699079209039, w1=1.0575037442243178\n",
      "Gradient Descent(599/999): loss=[[9.28533017]], w0=-2.549130928422701, w1=1.057748049855657\n",
      "Gradient Descent(600/999): loss=[[9.28413654]], w0=-2.5515583939951316, w1=1.0579919151026862\n",
      "Gradient Descent(601/999): loss=[[9.2829472]], w0=-2.5539814838282653, w1=1.05823534075924\n",
      "Gradient Descent(602/999): loss=[[9.28176214]], w0=-2.556400205809791, w1=1.0584783276177225\n",
      "Gradient Descent(603/999): loss=[[9.28058136]], w0=-2.55881456781318, w1=1.0587208764691094\n",
      "Gradient Descent(604/999): loss=[[9.27940482]], w0=-2.5612245776977107, w1=1.0589629881029503\n",
      "Gradient Descent(605/999): loss=[[9.27823253]], w0=-2.563630243308494, w1=1.0592046633073717\n",
      "Gradient Descent(606/999): loss=[[9.27706445]], w0=-2.5660315724765, w1=1.0594459028690795\n",
      "Gradient Descent(607/999): loss=[[9.27590059]], w0=-2.5684285730185823, w1=1.0596867075733616\n",
      "Gradient Descent(608/999): loss=[[9.27474091]], w0=-2.570821252737504, w1=1.0599270782040897\n",
      "Gradient Descent(609/999): loss=[[9.27358542]], w0=-2.573209619421962, w1=1.0601670155437235\n",
      "Gradient Descent(610/999): loss=[[9.27243408]], w0=-2.575593680846615, w1=1.0604065203733115\n",
      "Gradient Descent(611/999): loss=[[9.2712869]], w0=-2.5779734447721063, w1=1.0606455934724945\n",
      "Gradient Descent(612/999): loss=[[9.27014384]], w0=-2.58034891894509, w1=1.0608842356195078\n",
      "Gradient Descent(613/999): loss=[[9.2690049]], w0=-2.582720111098256, w1=1.0611224475911842\n",
      "Gradient Descent(614/999): loss=[[9.26787007]], w0=-2.585087028950355, w1=1.0613602301629559\n",
      "Gradient Descent(615/999): loss=[[9.26673932]], w0=-2.5874496802062246, w1=1.0615975841088572\n",
      "Gradient Descent(616/999): loss=[[9.26561264]], w0=-2.589808072556813, w1=1.0618345102015276\n",
      "Gradient Descent(617/999): loss=[[9.26449003]], w0=-2.5921622136792055, w1=1.0620710092122134\n",
      "Gradient Descent(618/999): loss=[[9.26337145]], w0=-2.594512111236648, w1=1.0623070819107707\n",
      "Gradient Descent(619/999): loss=[[9.26225691]], w0=-2.5968577728785727, w1=1.0625427290656682\n",
      "Gradient Descent(620/999): loss=[[9.26114638]], w0=-2.5991992062406233, w1=1.0627779514439888\n",
      "Gradient Descent(621/999): loss=[[9.26003985]], w0=-2.6015364189446797, w1=1.0630127498114332\n",
      "Gradient Descent(622/999): loss=[[9.25893731]], w0=-2.603869418598882, w1=1.0632471249323217\n",
      "Gradient Descent(623/999): loss=[[9.25783873]], w0=-2.606198212797657, w1=1.0634810775695966\n",
      "Gradient Descent(624/999): loss=[[9.25674412]], w0=-2.6085228091217405, w1=1.0637146084848252\n",
      "Gradient Descent(625/999): loss=[[9.25565344]], w0=-2.610843215138204, w1=1.0639477184382018\n",
      "Gradient Descent(626/999): loss=[[9.2545667]], w0=-2.6131594384004786, w1=1.0641804081885504\n",
      "Gradient Descent(627/999): loss=[[9.25348387]], w0=-2.6154714864483792, w1=1.0644126784933277\n",
      "Gradient Descent(628/999): loss=[[9.25240494]], w0=-2.61777936680813, w1=1.064644530108624\n",
      "Gradient Descent(629/999): loss=[[9.2513299]], w0=-2.6200830869923886, w1=1.0648759637891674\n",
      "Gradient Descent(630/999): loss=[[9.25025872]], w0=-2.622382654500269, w1=1.0651069802883253\n",
      "Gradient Descent(631/999): loss=[[9.24919141]], w0=-2.624678076817369, w1=1.0653375803581073\n",
      "Gradient Descent(632/999): loss=[[9.24812794]], w0=-2.6269693614157923, w1=1.065567764749167\n",
      "Gradient Descent(633/999): loss=[[9.24706831]], w0=-2.629256515754173, w1=1.0657975342108053\n",
      "Gradient Descent(634/999): loss=[[9.24601248]], w0=-2.6315395472777006, w1=1.0660268894909721\n",
      "Gradient Descent(635/999): loss=[[9.24496047]], w0=-2.633818463418144, w1=1.0662558313362696\n",
      "Gradient Descent(636/999): loss=[[9.24391224]], w0=-2.6360932715938756, w1=1.0664843604919532\n",
      "Gradient Descent(637/999): loss=[[9.24286778]], w0=-2.6383639792098954, w1=1.0667124777019357\n",
      "Gradient Descent(638/999): loss=[[9.24182709]], w0=-2.640630593657855, w1=1.0669401837087886\n",
      "Gradient Descent(639/999): loss=[[9.24079015]], w0=-2.6428931223160825, w1=1.0671674792537453\n",
      "Gradient Descent(640/999): loss=[[9.23975694]], w0=-2.645151572549605, w1=1.0673943650767024\n",
      "Gradient Descent(641/999): loss=[[9.23872746]], w0=-2.647405951710174, w1=1.067620841916223\n",
      "Gradient Descent(642/999): loss=[[9.23770168]], w0=-2.649656267136288, w1=1.0678469105095392\n",
      "Gradient Descent(643/999): loss=[[9.23667959]], w0=-2.6519025261532185, w1=1.0680725715925536\n",
      "Gradient Descent(644/999): loss=[[9.23566119]], w0=-2.6541447360730315, w1=1.068297825899843\n",
      "Gradient Descent(645/999): loss=[[9.23464646]], w0=-2.6563829041946128, w1=1.0685226741646592\n",
      "Gradient Descent(646/999): loss=[[9.23363538]], w0=-2.6586170378036904, w1=1.0687471171189327\n",
      "Gradient Descent(647/999): loss=[[9.23262794]], w0=-2.6608471441728603, w1=1.0689711554932748\n",
      "Gradient Descent(648/999): loss=[[9.23162414]], w0=-2.663073230561608, w1=1.0691947900169796\n",
      "Gradient Descent(649/999): loss=[[9.23062394]], w0=-2.6652953042163334, w1=1.0694180214180262\n",
      "Gradient Descent(650/999): loss=[[9.22962735]], w0=-2.667513372370374, w1=1.0696408504230823\n",
      "Gradient Descent(651/999): loss=[[9.22863435]], w0=-2.669727442244029, w1=1.069863277757505\n",
      "Gradient Descent(652/999): loss=[[9.22764493]], w0=-2.6719375210445815, w1=1.0700853041453442\n",
      "Gradient Descent(653/999): loss=[[9.22665907]], w0=-2.6741436159663237, w1=1.0703069303093442\n",
      "Gradient Descent(654/999): loss=[[9.22567676]], w0=-2.6763457341905785, w1=1.0705281569709473\n",
      "Gradient Descent(655/999): loss=[[9.22469799]], w0=-2.678543882885724, w1=1.0707489848502947\n",
      "Gradient Descent(656/999): loss=[[9.22372275]], w0=-2.680738069207217, w1=1.0709694146662294\n",
      "Gradient Descent(657/999): loss=[[9.22275102]], w0=-2.682928300297616, w1=1.071189447136299\n",
      "Gradient Descent(658/999): loss=[[9.22178278]], w0=-2.685114583286604, w1=1.0714090829767575\n",
      "Gradient Descent(659/999): loss=[[9.22081804]], w0=-2.6872969252910113, w1=1.0716283229025678\n",
      "Gradient Descent(660/999): loss=[[9.21985677]], w0=-2.689475333414841, w1=1.0718471676274042\n",
      "Gradient Descent(661/999): loss=[[9.21889896]], w0=-2.6916498147492893, w1=1.0720656178636538\n",
      "Gradient Descent(662/999): loss=[[9.21794461]], w0=-2.693820376372771, w1=1.0722836743224204\n",
      "Gradient Descent(663/999): loss=[[9.21699369]], w0=-2.69598702535094, w1=1.0725013377135257\n",
      "Gradient Descent(664/999): loss=[[9.21604619]], w0=-2.698149768736715, w1=1.0727186087455118\n",
      "Gradient Descent(665/999): loss=[[9.21510211]], w0=-2.7003086135703005, w1=1.0729354881256434\n",
      "Gradient Descent(666/999): loss=[[9.21416143]], w0=-2.7024635668792096, w1=1.0731519765599107\n",
      "Gradient Descent(667/999): loss=[[9.21322414]], w0=-2.7046146356782894, w1=1.0733680747530312\n",
      "Gradient Descent(668/999): loss=[[9.21229022]], w0=-2.7067618269697404, w1=1.0735837834084516\n",
      "Gradient Descent(669/999): loss=[[9.21135967]], w0=-2.708905147743142, w1=1.0737991032283511\n",
      "Gradient Descent(670/999): loss=[[9.21043247]], w0=-2.7110446049754735, w1=1.0740140349136431\n",
      "Gradient Descent(671/999): loss=[[9.20950861]], w0=-2.7131802056311383, w1=1.0742285791639774\n",
      "Gradient Descent(672/999): loss=[[9.20858807]], w0=-2.715311956661985, w1=1.0744427366777425\n",
      "Gradient Descent(673/999): loss=[[9.20767085]], w0=-2.7174398650073317, w1=1.074656508152068\n",
      "Gradient Descent(674/999): loss=[[9.20675694]], w0=-2.719563937593987, w1=1.0748698942828274\n",
      "Gradient Descent(675/999): loss=[[9.20584632]], w0=-2.7216841813362733, w1=1.075082895764639\n",
      "Gradient Descent(676/999): loss=[[9.20493898]], w0=-2.7238006031360498, w1=1.0752955132908697\n",
      "Gradient Descent(677/999): loss=[[9.2040349]], w0=-2.7259132098827337, w1=1.075507747553636\n",
      "Gradient Descent(678/999): loss=[[9.20313408]], w0=-2.728022008453324, w1=1.0757195992438071\n",
      "Gradient Descent(679/999): loss=[[9.20223651]], w0=-2.730127005712423, w1=1.0759310690510069\n",
      "Gradient Descent(680/999): loss=[[9.20134217]], w0=-2.7322282085122587, w1=1.0761421576636157\n",
      "Gradient Descent(681/999): loss=[[9.20045105]], w0=-2.734325623692708, w1=1.0763528657687735\n",
      "Gradient Descent(682/999): loss=[[9.19956314]], w0=-2.7364192580813174, w1=1.0765631940523814\n",
      "Gradient Descent(683/999): loss=[[9.19867843]], w0=-2.7385091184933263, w1=1.0767731431991043\n",
      "Gradient Descent(684/999): loss=[[9.19779691]], w0=-2.74059521173169, w1=1.0769827138923727\n",
      "Gradient Descent(685/999): loss=[[9.19691856]], w0=-2.742677544587099, w1=1.0771919068143851\n",
      "Gradient Descent(686/999): loss=[[9.19604338]], w0=-2.7447561238380045, w1=1.0774007226461104\n",
      "Gradient Descent(687/999): loss=[[9.19517134]], w0=-2.746830956250638, w1=1.07760916206729\n",
      "Gradient Descent(688/999): loss=[[9.19430245]], w0=-2.7489020485790343, w1=1.07781722575644\n",
      "Gradient Descent(689/999): loss=[[9.19343669]], w0=-2.750969407565054, w1=1.0780249143908534\n",
      "Gradient Descent(690/999): loss=[[9.19257405]], w0=-2.7530330399384044, w1=1.0782322286466022\n",
      "Gradient Descent(691/999): loss=[[9.19171451]], w0=-2.755092952416662, w1=1.0784391691985398\n",
      "Gradient Descent(692/999): loss=[[9.19085807]], w0=-2.7571491517052937, w1=1.0786457367203033\n",
      "Gradient Descent(693/999): loss=[[9.19000472]], w0=-2.75920164449768, w1=1.0788519318843153\n",
      "Gradient Descent(694/999): loss=[[9.18915443]], w0=-2.7612504374751357, w1=1.0790577553617862\n",
      "Gradient Descent(695/999): loss=[[9.18830722]], w0=-2.7632955373069312, w1=1.079263207822717\n",
      "Gradient Descent(696/999): loss=[[9.18746305]], w0=-2.7653369506503163, w1=1.0794682899358998\n",
      "Gradient Descent(697/999): loss=[[9.18662192]], w0=-2.7673746841505387, w1=1.0796730023689225\n",
      "Gradient Descent(698/999): loss=[[9.18578383]], w0=-2.769408744440869, w1=1.079877345788169\n",
      "Gradient Descent(699/999): loss=[[9.18494875]], w0=-2.771439138142619, w1=1.0800813208588218\n",
      "Gradient Descent(700/999): loss=[[9.18411668]], w0=-2.773465871865167, w1=1.0802849282448646\n",
      "Gradient Descent(701/999): loss=[[9.1832876]], w0=-2.775488952205976, w1=1.080488168609084\n",
      "Gradient Descent(702/999): loss=[[9.18246152]], w0=-2.7775083857506164, w1=1.0806910426130723\n",
      "Gradient Descent(703/999): loss=[[9.18163841]], w0=-2.7795241790727876, w1=1.0808935509172286\n",
      "Gradient Descent(704/999): loss=[[9.18081826]], w0=-2.7815363387343397, w1=1.0810956941807621\n",
      "Gradient Descent(705/999): loss=[[9.18000107]], w0=-2.783544871285294, w1=1.0812974730616935\n",
      "Gradient Descent(706/999): loss=[[9.17918682]], w0=-2.7855497832638654, w1=1.0814988882168572\n",
      "Gradient Descent(707/999): loss=[[9.1783755]], w0=-2.787551081196482, w1=1.081699940301904\n",
      "Gradient Descent(708/999): loss=[[9.17756711]], w0=-2.7895487715978082, w1=1.0819006299713023\n",
      "Gradient Descent(709/999): loss=[[9.17676163]], w0=-2.7915428609707647, w1=1.0821009578783414\n",
      "Gradient Descent(710/999): loss=[[9.17595904]], w0=-2.79353335580655, w1=1.0823009246751323\n",
      "Gradient Descent(711/999): loss=[[9.17515935]], w0=-2.7955202625846622, w1=1.0825005310126108\n",
      "Gradient Descent(712/999): loss=[[9.17436255]], w0=-2.7975035877729186, w1=1.0826997775405394\n",
      "Gradient Descent(713/999): loss=[[9.17356861]], w0=-2.7994833378274784, w1=1.0828986649075094\n",
      "Gradient Descent(714/999): loss=[[9.17277753]], w0=-2.801459519192863, w1=1.0830971937609428\n",
      "Gradient Descent(715/999): loss=[[9.1719893]], w0=-2.8034321383019756, w1=1.0832953647470946\n",
      "Gradient Descent(716/999): loss=[[9.1712039]], w0=-2.8054012015761254, w1=1.0834931785110549\n",
      "Gradient Descent(717/999): loss=[[9.17042134]], w0=-2.8073667154250455, w1=1.0836906356967508\n",
      "Gradient Descent(718/999): loss=[[9.1696416]], w0=-2.8093286862469147, w1=1.083887736946949\n",
      "Gradient Descent(719/999): loss=[[9.16886466]], w0=-2.811287120428379, w1=1.0840844829032572\n",
      "Gradient Descent(720/999): loss=[[9.16809053]], w0=-2.813242024344571, w1=1.0842808742061265\n",
      "Gradient Descent(721/999): loss=[[9.16731918]], w0=-2.815193404359133, w1=1.0844769114948543\n",
      "Gradient Descent(722/999): loss=[[9.16655061]], w0=-2.817141266824235, w1=1.0846725954075842\n",
      "Gradient Descent(723/999): loss=[[9.16578481]], w0=-2.8190856180805968, w1=1.084867926581311\n",
      "Gradient Descent(724/999): loss=[[9.16502177]], w0=-2.8210264644575087, w1=1.0850629056518801\n",
      "Gradient Descent(725/999): loss=[[9.16426147]], w0=-2.8229638122728518, w1=1.0852575332539915\n",
      "Gradient Descent(726/999): loss=[[9.16350392]], w0=-2.8248976678331186, w1=1.0854518100212005\n",
      "Gradient Descent(727/999): loss=[[9.16274909]], w0=-2.8268280374334336, w1=1.085645736585921\n",
      "Gradient Descent(728/999): loss=[[9.16199698]], w0=-2.8287549273575734, w1=1.085839313579426\n",
      "Gradient Descent(729/999): loss=[[9.16124758]], w0=-2.8306783438779877, w1=1.0860325416318515\n",
      "Gradient Descent(730/999): loss=[[9.16050088]], w0=-2.8325982932558196, w1=1.0862254213721971\n",
      "Gradient Descent(731/999): loss=[[9.15975687]], w0=-2.834514781740926, w1=1.0864179534283287\n",
      "Gradient Descent(732/999): loss=[[9.15901554]], w0=-2.8364278155718976, w1=1.0866101384269806\n",
      "Gradient Descent(733/999): loss=[[9.15827688]], w0=-2.8383374009760796, w1=1.086801976993757\n",
      "Gradient Descent(734/999): loss=[[9.15754088]], w0=-2.8402435441695912, w1=1.0869934697531343\n",
      "Gradient Descent(735/999): loss=[[9.15680753]], w0=-2.8421462513573474, w1=1.087184617328464\n",
      "Gradient Descent(736/999): loss=[[9.15607682]], w0=-2.844045528733078, w1=1.087375420341973\n",
      "Gradient Descent(737/999): loss=[[9.15534875]], w0=-2.8459413824793476, w1=1.0875658794147673\n",
      "Gradient Descent(738/999): loss=[[9.1546233]], w0=-2.8478338187675765, w1=1.0877559951668327\n",
      "Gradient Descent(739/999): loss=[[9.15390046]], w0=-2.84972284375806, w1=1.087945768217038\n",
      "Gradient Descent(740/999): loss=[[9.15318022]], w0=-2.8516084635999897, w1=1.088135199183136\n",
      "Gradient Descent(741/999): loss=[[9.15246258]], w0=-2.8534906844314714, w1=1.0883242886817661\n",
      "Gradient Descent(742/999): loss=[[9.15174752]], w0=-2.8553695123795473, w1=1.088513037328456\n",
      "Gradient Descent(743/999): loss=[[9.15103504]], w0=-2.8572449535602154, w1=1.088701445737624\n",
      "Gradient Descent(744/999): loss=[[9.15032513]], w0=-2.859117014078448, w1=1.0888895145225812\n",
      "Gradient Descent(745/999): loss=[[9.14961777]], w0=-2.860985700028213, w1=1.0890772442955325\n",
      "Gradient Descent(746/999): loss=[[9.14891296]], w0=-2.8628510174924937, w1=1.0892646356675795\n",
      "Gradient Descent(747/999): loss=[[9.14821069]], w0=-2.864712972543308, w1=1.0894516892487223\n",
      "Gradient Descent(748/999): loss=[[9.14751094]], w0=-2.866571571241728, w1=1.0896384056478612\n",
      "Gradient Descent(749/999): loss=[[9.14681372]], w0=-2.868426819637901, w1=1.0898247854727994\n",
      "Gradient Descent(750/999): loss=[[9.14611901]], w0=-2.8702787237710674, w1=1.090010829330244\n",
      "Gradient Descent(751/999): loss=[[9.1454268]], w0=-2.872127289669582, w1=1.0901965378258085\n",
      "Gradient Descent(752/999): loss=[[9.14473709]], w0=-2.8739725233509326, w1=1.090381911564015\n",
      "Gradient Descent(753/999): loss=[[9.14404986]], w0=-2.8758144308217597, w1=1.0905669511482956\n",
      "Gradient Descent(754/999): loss=[[9.1433651]], w0=-2.877653018077877, w1=1.0907516571809948\n",
      "Gradient Descent(755/999): loss=[[9.14268282]], w0=-2.879488291104289, w1=1.0909360302633713\n",
      "Gradient Descent(756/999): loss=[[9.14200299]], w0=-2.8813202558752127, w1=1.0911200709956\n",
      "Gradient Descent(757/999): loss=[[9.1413256]], w0=-2.8831489183540957, w1=1.0913037799767737\n",
      "Gradient Descent(758/999): loss=[[9.14065066]], w0=-2.8849742844936355, w1=1.0914871578049057\n",
      "Gradient Descent(759/999): loss=[[9.13997815]], w0=-2.8867963602357998, w1=1.091670205076931\n",
      "Gradient Descent(760/999): loss=[[9.13930806]], w0=-2.8886151515118454, w1=1.0918529223887086\n",
      "Gradient Descent(761/999): loss=[[9.13864039]], w0=-2.890430664242337, w1=1.0920353103350233\n",
      "Gradient Descent(762/999): loss=[[9.13797512]], w0=-2.892242904337167, w1=1.0922173695095885\n",
      "Gradient Descent(763/999): loss=[[9.13731224]], w0=-2.8940518776955746, w1=1.0923991005050462\n",
      "Gradient Descent(764/999): loss=[[9.13665176]], w0=-2.8958575902061656, w1=1.0925805039129708\n",
      "Gradient Descent(765/999): loss=[[9.13599365]], w0=-2.8976600477469305, w1=1.0927615803238704\n",
      "Gradient Descent(766/999): loss=[[9.13533791]], w0=-2.8994592561852643, w1=1.0929423303271883\n",
      "Gradient Descent(767/999): loss=[[9.13468454]], w0=-2.9012552213779856, w1=1.0931227545113058\n",
      "Gradient Descent(768/999): loss=[[9.13403352]], w0=-2.9030479491713552, w1=1.0933028534635427\n",
      "Gradient Descent(769/999): loss=[[9.13338484]], w0=-2.904837445401096, w1=1.093482627770161\n",
      "Gradient Descent(770/999): loss=[[9.1327385]], w0=-2.906623715892411, w1=1.0936620780163653\n",
      "Gradient Descent(771/999): loss=[[9.13209449]], w0=-2.9084067664600024, w1=1.0938412047863058\n",
      "Gradient Descent(772/999): loss=[[9.13145279]], w0=-2.9101866029080914, w1=1.0940200086630791\n",
      "Gradient Descent(773/999): loss=[[9.13081341]], w0=-2.9119632310304366, w1=1.0941984902287314\n",
      "Gradient Descent(774/999): loss=[[9.13017634]], w0=-2.9137366566103524, w1=1.0943766500642593\n",
      "Gradient Descent(775/999): loss=[[9.12954155]], w0=-2.9155068854207284, w1=1.0945544887496121\n",
      "Gradient Descent(776/999): loss=[[9.12890906]], w0=-2.917273923224048, w1=1.094732006863694\n",
      "Gradient Descent(777/999): loss=[[9.12827884]], w0=-2.9190377757724075, w1=1.0949092049843652\n",
      "Gradient Descent(778/999): loss=[[9.12765089]], w0=-2.920798448807534, w1=1.0950860836884444\n",
      "Gradient Descent(779/999): loss=[[9.1270252]], w0=-2.922555948060804, w1=1.0952626435517108\n",
      "Gradient Descent(780/999): loss=[[9.12640177]], w0=-2.9243102792532647, w1=1.0954388851489054\n",
      "Gradient Descent(781/999): loss=[[9.12578058]], w0=-2.9260614480956484, w1=1.0956148090537334\n",
      "Gradient Descent(782/999): loss=[[9.12516163]], w0=-2.9278094602883944, w1=1.0957904158388658\n",
      "Gradient Descent(783/999): loss=[[9.12454491]], w0=-2.929554321521666, w1=1.0959657060759411\n",
      "Gradient Descent(784/999): loss=[[9.12393041]], w0=-2.93129603747537, w1=1.0961406803355676\n",
      "Gradient Descent(785/999): loss=[[9.12331813]], w0=-2.933034613819174, w1=1.0963153391873246\n",
      "Gradient Descent(786/999): loss=[[9.12270805]], w0=-2.9347700562125256, w1=1.0964896831997653\n",
      "Gradient Descent(787/999): loss=[[9.12210016]], w0=-2.936502370304671, w1=1.0966637129404178\n",
      "Gradient Descent(788/999): loss=[[9.12149447]], w0=-2.9382315617346726, w1=1.096837428975787\n",
      "Gradient Descent(789/999): loss=[[9.12089096]], w0=-2.939957636131428, w1=1.0970108318713567\n",
      "Gradient Descent(790/999): loss=[[9.12028962]], w0=-2.9416805991136887, w1=1.0971839221915918\n",
      "Gradient Descent(791/999): loss=[[9.11969045]], w0=-2.9434004562900773, w1=1.0973567004999385\n",
      "Gradient Descent(792/999): loss=[[9.11909343]], w0=-2.9451172132591066, w1=1.0975291673588288\n",
      "Gradient Descent(793/999): loss=[[9.11849857]], w0=-2.9468308756091974, w1=1.0977013233296797\n",
      "Gradient Descent(794/999): loss=[[9.11790585]], w0=-2.948541448918697, w1=1.097873168972897\n",
      "Gradient Descent(795/999): loss=[[9.11731527]], w0=-2.9502489387558963, w1=1.0980447048478763\n",
      "Gradient Descent(796/999): loss=[[9.11672681]], w0=-2.9519533506790503, w1=1.0982159315130038\n",
      "Gradient Descent(797/999): loss=[[9.11614047]], w0=-2.953654690236394, w1=1.0983868495256603\n",
      "Gradient Descent(798/999): loss=[[9.11555624]], w0=-2.955352962966161, w1=1.0985574594422216\n",
      "Gradient Descent(799/999): loss=[[9.11497412]], w0=-2.957048174396602, w1=1.09872776181806\n",
      "Gradient Descent(800/999): loss=[[9.11439409]], w0=-2.9587403300460022, w1=1.0988977572075473\n",
      "Gradient Descent(801/999): loss=[[9.11381616]], w0=-2.9604294354226997, w1=1.0990674461640562\n",
      "Gradient Descent(802/999): loss=[[9.1132403]], w0=-2.9621154960251035, w1=1.0992368292399608\n",
      "Gradient Descent(803/999): loss=[[9.11266652]], w0=-2.963798517341711, w1=1.0994059069866409\n",
      "Gradient Descent(804/999): loss=[[9.11209481]], w0=-2.965478504851126, w1=1.0995746799544812\n",
      "Gradient Descent(805/999): loss=[[9.11152515]], w0=-2.9671554640220763, w1=1.0997431486928748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(806/999): loss=[[9.11095755]], w0=-2.968829400313433, w1=1.0999113137502248\n",
      "Gradient Descent(807/999): loss=[[9.11039199]], w0=-2.9705003191742256, w1=1.100079175673945\n",
      "Gradient Descent(808/999): loss=[[9.10982847]], w0=-2.972168226043662, w1=1.1002467350104628\n",
      "Gradient Descent(809/999): loss=[[9.10926698]], w0=-2.973833126351145, w1=1.100413992305221\n",
      "Gradient Descent(810/999): loss=[[9.10870751]], w0=-2.9754950255162913, w1=1.1005809481026785\n",
      "Gradient Descent(811/999): loss=[[9.10815006]], w0=-2.977153928948947, w1=1.1007476029463132\n",
      "Gradient Descent(812/999): loss=[[9.10759461]], w0=-2.978809842049207, w1=1.100913957378623\n",
      "Gradient Descent(813/999): loss=[[9.10704117]], w0=-2.9804627702074318, w1=1.1010800119411286\n",
      "Gradient Descent(814/999): loss=[[9.10648971]], w0=-2.9821127188042658, w1=1.1012457671743734\n",
      "Gradient Descent(815/999): loss=[[9.10594025]], w0=-2.983759693210654, w1=1.1014112236179276\n",
      "Gradient Descent(816/999): loss=[[9.10539276]], w0=-2.985403698787859, w1=1.1015763818103879\n",
      "Gradient Descent(817/999): loss=[[9.10484725]], w0=-2.9870447408874803, w1=1.1017412422893806\n",
      "Gradient Descent(818/999): loss=[[9.1043037]], w0=-2.9886828248514705, w1=1.101905805591563\n",
      "Gradient Descent(819/999): loss=[[9.10376211]], w0=-2.9903179560121522, w1=1.1020700722526242\n",
      "Gradient Descent(820/999): loss=[[9.10322247]], w0=-2.9919501396922366, w1=1.1022340428072888\n",
      "Gradient Descent(821/999): loss=[[9.10268477]], w0=-2.9935793812048392, w1=1.1023977177893167\n",
      "Gradient Descent(822/999): loss=[[9.10214901]], w0=-2.9952056858534997, w1=1.102561097731506\n",
      "Gradient Descent(823/999): loss=[[9.10161518]], w0=-2.996829058932196, w1=1.1027241831656942\n",
      "Gradient Descent(824/999): loss=[[9.10108327]], w0=-2.9984495057253646, w1=1.10288697462276\n",
      "Gradient Descent(825/999): loss=[[9.10055328]], w0=-3.000067031507915, w1=1.1030494726326259\n",
      "Gradient Descent(826/999): loss=[[9.1000252]], w0=-3.001681641545249, w1=1.1032116777242582\n",
      "Gradient Descent(827/999): loss=[[9.09949902]], w0=-3.0032933410932765, w1=1.10337359042567\n",
      "Gradient Descent(828/999): loss=[[9.09897473]], w0=-3.0049021353984338, w1=1.1035352112639227\n",
      "Gradient Descent(829/999): loss=[[9.09845234]], w0=-3.006508029697699, w1=1.103696540765128\n",
      "Gradient Descent(830/999): loss=[[9.09793182]], w0=-3.008111029218611, w1=1.1038575794544485\n",
      "Gradient Descent(831/999): loss=[[9.09741318]], w0=-3.0097111391792852, w1=1.1040183278561004\n",
      "Gradient Descent(832/999): loss=[[9.09689641]], w0=-3.0113083647884307, w1=1.1041787864933554\n",
      "Gradient Descent(833/999): loss=[[9.0963815]], w0=-3.0129027112453675, w1=1.1043389558885413\n",
      "Gradient Descent(834/999): loss=[[9.09586844]], w0=-3.014494183740043, w1=1.1044988365630448\n",
      "Gradient Descent(835/999): loss=[[9.09535723]], w0=-3.0160827874530503, w1=1.1046584290373125\n",
      "Gradient Descent(836/999): loss=[[9.09484787]], w0=-3.0176685275556423, w1=1.104817733830853\n",
      "Gradient Descent(837/999): loss=[[9.09434033]], w0=-3.019251409209752, w1=1.1049767514622382\n",
      "Gradient Descent(838/999): loss=[[9.09383463]], w0=-3.020831437568006, w1=1.1051354824491055\n",
      "Gradient Descent(839/999): loss=[[9.09333075]], w0=-3.022408617773744, w1=1.1052939273081595\n",
      "Gradient Descent(840/999): loss=[[9.09282868]], w0=-3.023982954961034, w1=1.1054520865551725\n",
      "Gradient Descent(841/999): loss=[[9.09232842]], w0=-3.0255544542546886, w1=1.1056099607049876\n",
      "Gradient Descent(842/999): loss=[[9.09182996]], w0=-3.0271231207702836, w1=1.10576755027152\n",
      "Gradient Descent(843/999): loss=[[9.0913333]], w0=-3.0286889596141724, w1=1.1059248557677583\n",
      "Gradient Descent(844/999): loss=[[9.09083843]], w0=-3.0302519758835045, w1=1.1060818777057666\n",
      "Gradient Descent(845/999): loss=[[9.09034534]], w0=-3.0318121746662405, w1=1.1062386165966855\n",
      "Gradient Descent(846/999): loss=[[9.08985402]], w0=-3.0333695610411704, w1=1.1063950729507346\n",
      "Gradient Descent(847/999): loss=[[9.08936448]], w0=-3.034924140077929, w1=1.1065512472772134\n",
      "Gradient Descent(848/999): loss=[[9.0888767]], w0=-3.036475916837012, w1=1.106707140084504\n",
      "Gradient Descent(849/999): loss=[[9.08839067]], w0=-3.038024896369793, w1=1.106862751880072\n",
      "Gradient Descent(850/999): loss=[[9.0879064]], w0=-3.0395710837185415, w1=1.1070180831704672\n",
      "Gradient Descent(851/999): loss=[[9.08742387]], w0=-3.041114483916436, w1=1.1071731344613276\n",
      "Gradient Descent(852/999): loss=[[9.08694308]], w0=-3.042655101987583, w1=1.1073279062573789\n",
      "Gradient Descent(853/999): loss=[[9.08646402]], w0=-3.044192942947033, w1=1.1074823990624372\n",
      "Gradient Descent(854/999): loss=[[9.08598669]], w0=-3.0457280118007954, w1=1.1076366133794109\n",
      "Gradient Descent(855/999): loss=[[9.08551107]], w0=-3.047260313545857, w1=1.1077905497103013\n",
      "Gradient Descent(856/999): loss=[[9.08503717]], w0=-3.0487898531701956, w1=1.1079442085562048\n",
      "Gradient Descent(857/999): loss=[[9.08456498]], w0=-3.050316635652799, w1=1.1080975904173147\n",
      "Gradient Descent(858/999): loss=[[9.08409448]], w0=-3.051840665963679, w1=1.1082506957929228\n",
      "Gradient Descent(859/999): loss=[[9.08362568]], w0=-3.0533619490638895, w1=1.1084035251814206\n",
      "Gradient Descent(860/999): loss=[[9.08315857]], w0=-3.0548804899055404, w1=1.1085560790803013\n",
      "Gradient Descent(861/999): loss=[[9.08269314]], w0=-3.0563962934318156, w1=1.1087083579861612\n",
      "Gradient Descent(862/999): loss=[[9.08222939]], w0=-3.057909364576988, w1=1.108860362394702\n",
      "Gradient Descent(863/999): loss=[[9.08176731]], w0=-3.059419708266437, w1=1.109012092800731\n",
      "Gradient Descent(864/999): loss=[[9.0813069]], w0=-3.060927329416663, w1=1.1091635496981642\n",
      "Gradient Descent(865/999): loss=[[9.08084814]], w0=-3.062432232935303, w1=1.1093147335800269\n",
      "Gradient Descent(866/999): loss=[[9.08039103]], w0=-3.063934423721149, w1=1.109465644938456\n",
      "Gradient Descent(867/999): loss=[[9.07993557]], w0=-3.0654339066641616, w1=1.1096162842647008\n",
      "Gradient Descent(868/999): loss=[[9.07948175]], w0=-3.0669306866454873, w1=1.1097666520491256\n",
      "Gradient Descent(869/999): loss=[[9.07902957]], w0=-3.068424768537473, w1=1.1099167487812107\n",
      "Gradient Descent(870/999): loss=[[9.07857901]], w0=-3.0699161572036835, w1=1.1100665749495537\n",
      "Gradient Descent(871/999): loss=[[9.07813008]], w0=-3.0714048574989166, w1=1.1102161310418717\n",
      "Gradient Descent(872/999): loss=[[9.07768276]], w0=-3.072890874269218, w1=1.1103654175450026\n",
      "Gradient Descent(873/999): loss=[[9.07723706]], w0=-3.074374212351899, w1=1.110514434944907\n",
      "Gradient Descent(874/999): loss=[[9.07679296]], w0=-3.0758548765755505, w1=1.1106631837266687\n",
      "Gradient Descent(875/999): loss=[[9.07635046]], w0=-3.07733287176006, w1=1.1108116643744983\n",
      "Gradient Descent(876/999): loss=[[9.07590956]], w0=-3.0788082027166257, w1=1.1109598773717324\n",
      "Gradient Descent(877/999): loss=[[9.07547024]], w0=-3.0802808742477743, w1=1.1111078232008371\n",
      "Gradient Descent(878/999): loss=[[9.0750325]], w0=-3.0817508911473745, w1=1.1112555023434085\n",
      "Gradient Descent(879/999): loss=[[9.07459635]], w0=-3.083218258200654, w1=1.1114029152801745\n",
      "Gradient Descent(880/999): loss=[[9.07416176]], w0=-3.0846829801842155, w1=1.1115500624909966\n",
      "Gradient Descent(881/999): loss=[[9.07372874]], w0=-3.0861450618660498, w1=1.1116969444548714\n",
      "Gradient Descent(882/999): loss=[[9.07329727]], w0=-3.087604508005554, w1=1.111843561649932\n",
      "Gradient Descent(883/999): loss=[[9.07286737]], w0=-3.0890613233535458, w1=1.1119899145534495\n",
      "Gradient Descent(884/999): loss=[[9.07243901]], w0=-3.0905155126522788, w1=1.1121360036418346\n",
      "Gradient Descent(885/999): loss=[[9.07201219]], w0=-3.0919670806354587, w1=1.1122818293906396\n",
      "Gradient Descent(886/999): loss=[[9.07158691]], w0=-3.0934160320282578, w1=1.112427392274559\n",
      "Gradient Descent(887/999): loss=[[9.07116316]], w0=-3.0948623715473307, w1=1.112572692767432\n",
      "Gradient Descent(888/999): loss=[[9.07074094]], w0=-3.0963061039008304, w1=1.1127177313422438\n",
      "Gradient Descent(889/999): loss=[[9.07032024]], w0=-3.0977472337884224, w1=1.1128625084711268\n",
      "Gradient Descent(890/999): loss=[[9.06990106]], w0=-3.0991857659013013, w1=1.1130070246253623\n",
      "Gradient Descent(891/999): loss=[[9.06948338]], w0=-3.100621704922205, w1=1.1131512802753818\n",
      "Gradient Descent(892/999): loss=[[9.06906721]], w0=-3.1020550555254296, w1=1.1132952758907695\n",
      "Gradient Descent(893/999): loss=[[9.06865254]], w0=-3.1034858223768462, w1=1.1134390119402626\n",
      "Gradient Descent(894/999): loss=[[9.06823936]], w0=-3.1049140101339154, w1=1.1135824888917536\n",
      "Gradient Descent(895/999): loss=[[9.06782767]], w0=-3.1063396234457015, w1=1.1137257072122915\n",
      "Gradient Descent(896/999): loss=[[9.06741746]], w0=-3.107762666952889, w1=1.113868667368083\n",
      "Gradient Descent(897/999): loss=[[9.06700874]], w0=-3.109183145287797, w1=1.1140113698244951\n",
      "Gradient Descent(898/999): loss=[[9.06660148]], w0=-3.1106010630743945, w1=1.1141538150460557\n",
      "Gradient Descent(899/999): loss=[[9.06619569]], w0=-3.112016424928315, w1=1.114296003496455\n",
      "Gradient Descent(900/999): loss=[[9.06579136]], w0=-3.1134292354568713, w1=1.114437935638548\n",
      "Gradient Descent(901/999): loss=[[9.06538849]], w0=-3.114839499259073, w1=1.1145796119343547\n",
      "Gradient Descent(902/999): loss=[[9.06498707]], w0=-3.1162472209256378, w1=1.1147210328450625\n",
      "Gradient Descent(903/999): loss=[[9.06458709]], w0=-3.117652405039009, w1=1.1148621988310277\n",
      "Gradient Descent(904/999): loss=[[9.06418856]], w0=-3.1190550561733694, w1=1.1150031103517763\n",
      "Gradient Descent(905/999): loss=[[9.06379146]], w0=-3.120455178894656, w1=1.1151437678660066\n",
      "Gradient Descent(906/999): loss=[[9.06339579]], w0=-3.121852777760576, w1=1.1152841718315896\n",
      "Gradient Descent(907/999): loss=[[9.06300155]], w0=-3.1232478573206204, w1=1.115424322705571\n",
      "Gradient Descent(908/999): loss=[[9.06260872]], w0=-3.1246404221160793, w1=1.1155642209441723\n",
      "Gradient Descent(909/999): loss=[[9.06221731]], w0=-3.1260304766800573, w1=1.1157038670027934\n",
      "Gradient Descent(910/999): loss=[[9.06182731]], w0=-3.127418025537487, w1=1.1158432613360127\n",
      "Gradient Descent(911/999): loss=[[9.06143872]], w0=-3.128803073205144, w1=1.1159824043975897\n",
      "Gradient Descent(912/999): loss=[[9.06105153]], w0=-3.1301856241916632, w1=1.116121296640465\n",
      "Gradient Descent(913/999): loss=[[9.06066573]], w0=-3.1315656829975516, w1=1.1162599385167638\n",
      "Gradient Descent(914/999): loss=[[9.06028132]], w0=-3.132943254115203, w1=1.1163983304777958\n",
      "Gradient Descent(915/999): loss=[[9.05989829]], w0=-3.1343183420289145, w1=1.116536472974057\n",
      "Gradient Descent(916/999): loss=[[9.05951665]], w0=-3.1356909512148987, w1=1.1166743664552319\n",
      "Gradient Descent(917/999): loss=[[9.05913637]], w0=-3.1370610861413, w1=1.1168120113701938\n",
      "Gradient Descent(918/999): loss=[[9.05875747]], w0=-3.1384287512682083, w1=1.116949408167007\n",
      "Gradient Descent(919/999): loss=[[9.05837994]], w0=-3.1397939510476736, w1=1.1170865572929285\n",
      "Gradient Descent(920/999): loss=[[9.05800376]], w0=-3.1411566899237213, w1=1.1172234591944088\n",
      "Gradient Descent(921/999): loss=[[9.05762894]], w0=-3.1425169723323654, w1=1.1173601143170935\n",
      "Gradient Descent(922/999): loss=[[9.05725547]], w0=-3.143874802701624, w1=1.1174965231058251\n",
      "Gradient Descent(923/999): loss=[[9.05688334]], w0=-3.145230185451533, w1=1.1176326860046442\n",
      "Gradient Descent(924/999): loss=[[9.05651255]], w0=-3.1465831249941605, w1=1.1177686034567906\n",
      "Gradient Descent(925/999): loss=[[9.0561431]], w0=-3.1479336257336223, w1=1.117904275904706\n",
      "Gradient Descent(926/999): loss=[[9.05577498]], w0=-3.1492816920660944, w1=1.1180397037900336\n",
      "Gradient Descent(927/999): loss=[[9.05540819]], w0=-3.1506273283798287, w1=1.118174887553621\n",
      "Gradient Descent(928/999): loss=[[9.05504272]], w0=-3.151970539055167, w1=1.1183098276355212\n",
      "Gradient Descent(929/999): loss=[[9.05467856]], w0=-3.1533113284645546, w1=1.1184445244749937\n",
      "Gradient Descent(930/999): loss=[[9.05431572]], w0=-3.1546497009725556, w1=1.118578978510506\n",
      "Gradient Descent(931/999): loss=[[9.05395418]], w0=-3.1559856609358663, w1=1.1187131901797358\n",
      "Gradient Descent(932/999): loss=[[9.05359394]], w0=-3.15731921270333, w1=1.1188471599195713\n",
      "Gradient Descent(933/999): loss=[[9.05323501]], w0=-3.1586503606159497, w1=1.1189808881661139\n",
      "Gradient Descent(934/999): loss=[[9.05287736]], w0=-3.1599791090069047, w1=1.119114375354678\n",
      "Gradient Descent(935/999): loss=[[9.052521]], w0=-3.161305462201563, w1=1.1192476219197935\n",
      "Gradient Descent(936/999): loss=[[9.05216593]], w0=-3.1626294245174944, w1=1.119380628295208\n",
      "Gradient Descent(937/999): loss=[[9.05181214]], w0=-3.163951000264488, w1=1.1195133949138856\n",
      "Gradient Descent(938/999): loss=[[9.05145962]], w0=-3.165270193744562, w1=1.1196459222080113\n",
      "Gradient Descent(939/999): loss=[[9.05110837]], w0=-3.166587009251982, w1=1.1197782106089904\n",
      "Gradient Descent(940/999): loss=[[9.05075838]], w0=-3.167901451073271, w1=1.1199102605474507\n",
      "Gradient Descent(941/999): loss=[[9.05040966]], w0=-3.169213523487225, w1=1.1200420724532438\n",
      "Gradient Descent(942/999): loss=[[9.05006219]], w0=-3.1705232307649287, w1=1.1201736467554462\n",
      "Gradient Descent(943/999): loss=[[9.04971597]], w0=-3.1718305771697666, w1=1.1203049838823613\n",
      "Gradient Descent(944/999): loss=[[9.049371]], w0=-3.1731355669574377, w1=1.1204360842615202\n",
      "Gradient Descent(945/999): loss=[[9.04902728]], w0=-3.174438204375971, w1=1.1205669483196836\n",
      "Gradient Descent(946/999): loss=[[9.04868479]], w0=-3.175738493665737, w1=1.1206975764828424\n",
      "Gradient Descent(947/999): loss=[[9.04834353]], w0=-3.1770364390594628, w1=1.1208279691762202\n",
      "Gradient Descent(948/999): loss=[[9.0480035]], w0=-3.1783320447822456, w1=1.1209581268242739\n",
      "Gradient Descent(949/999): loss=[[9.0476647]], w0=-3.1796253150515663, w1=1.121088049850695\n",
      "Gradient Descent(950/999): loss=[[9.04732712]], w0=-3.180916254077304, w1=1.1212177386784117\n",
      "Gradient Descent(951/999): loss=[[9.04699076]], w0=-3.182204866061748, w1=1.1213471937295894\n",
      "Gradient Descent(952/999): loss=[[9.0466556]], w0=-3.1834911551996137, w1=1.1214764154256327\n",
      "Gradient Descent(953/999): loss=[[9.04632166]], w0=-3.1847751256780543, w1=1.1216054041871868\n",
      "Gradient Descent(954/999): loss=[[9.04598891]], w0=-3.186056781676676, w1=1.1217341604341384\n",
      "Gradient Descent(955/999): loss=[[9.04565737]], w0=-3.18733612736755, w1=1.1218626845856172\n",
      "Gradient Descent(956/999): loss=[[9.04532702]], w0=-3.188613166915228, w1=1.1219909770599976\n",
      "Gradient Descent(957/999): loss=[[9.04499786]], w0=-3.1898879044767536, w1=1.1221190382748998\n",
      "Gradient Descent(958/999): loss=[[9.04466988]], w0=-3.1911603442016774, w1=1.1222468686471911\n",
      "Gradient Descent(959/999): loss=[[9.04434309]], w0=-3.1924304902320704, w1=1.1223744685929875\n",
      "Gradient Descent(960/999): loss=[[9.04401747]], w0=-3.1936983467025364, w1=1.1225018385276548\n",
      "Gradient Descent(961/999): loss=[[9.04369303]], w0=-3.1949639177402265, w1=1.1226289788658101\n",
      "Gradient Descent(962/999): loss=[[9.04336975]], w0=-3.196227207464853, w1=1.1227558900213233\n",
      "Gradient Descent(963/999): loss=[[9.04304764]], w0=-3.1974882199887005, w1=1.1228825724073177\n",
      "Gradient Descent(964/999): loss=[[9.04272669]], w0=-3.198746959416642, w1=1.1230090264361725\n",
      "Gradient Descent(965/999): loss=[[9.0424069]], w0=-3.2000034298461504, w1=1.1231352525195233\n",
      "Gradient Descent(966/999): loss=[[9.04208825]], w0=-3.201257635367313, w1=1.1232612510682638\n",
      "Gradient Descent(967/999): loss=[[9.04177076]], w0=-3.202509580062844, w1=1.1233870224925466\n",
      "Gradient Descent(968/999): loss=[[9.04145441]], w0=-3.2037592680080986, w1=1.1235125672017858\n",
      "Gradient Descent(969/999): loss=[[9.04113919]], w0=-3.205006703271085, w1=1.1236378856046567\n",
      "Gradient Descent(970/999): loss=[[9.04082512]], w0=-3.206251889912479, w1=1.123762978109098\n",
      "Gradient Descent(971/999): loss=[[9.04051217]], w0=-3.2074948319856365, w1=1.1238878451223138\n",
      "Gradient Descent(972/999): loss=[[9.04020036]], w0=-3.208735533536607, w1=1.1240124870507733\n",
      "Gradient Descent(973/999): loss=[[9.03988966]], w0=-3.209973998604146, w1=1.1241369043002134\n",
      "Gradient Descent(974/999): loss=[[9.03958008]], w0=-3.211210231219729, w1=1.1242610972756395\n",
      "Gradient Descent(975/999): loss=[[9.03927162]], w0=-3.2124442354075655, w1=1.124385066381327\n",
      "Gradient Descent(976/999): loss=[[9.03896427]], w0=-3.2136760151846095, w1=1.1245088120208229\n",
      "Gradient Descent(977/999): loss=[[9.03865803]], w0=-3.2149055745605746, w1=1.124632334596946\n",
      "Gradient Descent(978/999): loss=[[9.03835289]], w0=-3.2161329175379465, w1=1.1247556345117895\n",
      "Gradient Descent(979/999): loss=[[9.03804885]], w0=-3.217358048111996, w1=1.1248787121667216\n",
      "Gradient Descent(980/999): loss=[[9.03774591]], w0=-3.2185809702707924, w1=1.1250015679623875\n",
      "Gradient Descent(981/999): loss=[[9.03744405]], w0=-3.2198016879952154, w1=1.1251242022987094\n",
      "Gradient Descent(982/999): loss=[[9.03714329]], w0=-3.2210202052589696, w1=1.125246615574889\n",
      "Gradient Descent(983/999): loss=[[9.0368436]], w0=-3.222236526028596, w1=1.1253688081894087\n",
      "Gradient Descent(984/999): loss=[[9.036545]], w0=-3.2234506542634853, w1=1.1254907805400318\n",
      "Gradient Descent(985/999): loss=[[9.03624747]], w0=-3.224662593915892, w1=1.1256125330238056\n",
      "Gradient Descent(986/999): loss=[[9.03595101]], w0=-3.2258723489309458, w1=1.1257340660370607\n",
      "Gradient Descent(987/999): loss=[[9.03565562]], w0=-3.2270799232466643, w1=1.125855379975414\n",
      "Gradient Descent(988/999): loss=[[9.0353613]], w0=-3.2282853207939675, w1=1.125976475233769\n",
      "Gradient Descent(989/999): loss=[[9.03506803]], w0=-3.229488545496689, w1=1.1260973522063173\n",
      "Gradient Descent(990/999): loss=[[9.03477582]], w0=-3.2306896012715893, w1=1.12621801128654\n",
      "Gradient Descent(991/999): loss=[[9.03448467]], w0=-3.231888492028369, w1=1.126338452867209\n",
      "Gradient Descent(992/999): loss=[[9.03419456]], w0=-3.23308522166968, w1=1.126458677340388\n",
      "Gradient Descent(993/999): loss=[[9.0339055]], w0=-3.2342797940911403, w1=1.1265786850974338\n",
      "Gradient Descent(994/999): loss=[[9.03361748]], w0=-3.2354722131813456, w1=1.1266984765289985\n",
      "Gradient Descent(995/999): loss=[[9.03333049]], w0=-3.2366624828218815, w1=1.126818052025029\n",
      "Gradient Descent(996/999): loss=[[9.03304454]], w0=-3.237850606887337, w1=1.1269374119747702\n",
      "Gradient Descent(997/999): loss=[[9.03275962]], w0=-3.239036589245317, w1=1.1270565567667645\n",
      "Gradient Descent(998/999): loss=[[9.03247573]], w0=-3.240220433756454, w1=1.1271754867888548\n",
      "Gradient Descent(999/999): loss=[[9.03219286]], w0=-3.2414021442744225, w1=1.1272942024281842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.24140214,  1.1272942 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.03191101]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
