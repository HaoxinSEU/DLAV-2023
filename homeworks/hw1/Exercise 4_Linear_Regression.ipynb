{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DLAV-2023' already exists and is not an empty directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/vita-epfl/DLAV-2023.git\n",
    "path = os.getcwd() + '/DLAV-2023/homeworks/hw1/data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHgCAYAAABelVD0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAutElEQVR4nO3dfZClWV0n+O+5VdlZCdlAmgUIVc20O8XsLLjV5UwtOFvODOCuAYwWOqWGjM4ys4ZohDgS81Ll6O4I6z92KU7oyLqBwooG4+iaYrUO7mjQGCxEiFZjdUKLCrpAZ9ELTVoNnViVnVX37B95s8nKzswn3+597s38fCIyKvO5b6dP3r75vb/7e84ptdYAAAAb67Q9AAAAGHZCMwAANBCaAQCggdAMAAANhGYAAGggNAMAQIPDbQ9gK44ePVrvvvvutocBAMA+98ADD3y+1vrstcdHIjTffffduXz5ctvDAABgnyulfGq949ozAACgQd9CcynlrlLK+0opHyulPFRK+cHe8TeVUq6WUq70vl7drzEAAMBe6Gd7xs0k/6rW+uFSyp1JHiil/F7vsn9fa/3JPj42AADsmb6F5lrrI0ke6X3/eCnlY0mO9evxAACgXwbS01xKuTvJ1yT5UO/QG0ops6WUd5RSpgYxBgAA2Km+h+ZSymSSmSRvrLV+McnPJfmbSU5luRL9lg1u9/pSyuVSyuVHH32038MEAIAN9TU0l1LGshyY31Vr/Y0kqbV+ttZ6q9baTfLzSV6y3m1rrW+rtZ6utZ5+9rOfslQeAAAMTD9XzyhJ3p7kY7XWn1p1/HmrrvYtST7arzEAAMBe6OfqGWeS/NMkHymlXOkd++Ekry2lnEpSk3wyyff2cQwAALBr/Vw94wNJyjoXvadfjwkAAP1gR0AAAGggNAMAQAOhGQAAGgjNAADQQGgGAIAGQjMAAENjfmExDz78WOYXFtseym36uU4zAABs2aUrV3NhZjZjnU6Wut1cPHcyZ08da3tYSVSaAQAYAvMLi7kwM5sbS908vngzN5a6OT8zOzQVZ6EZAIDWzV27nrHO7dF0rNPJ3LXrLY3odkIzAACtOz41kaVu97ZjS91ujk9NtDSi2wnNAAC0bnpyPBfPncyRsU7uHD+cI2OdXDx3MtOT420PLYkTAQEAGBJnTx3LmRNHM3fteo5PTQxNYE6EZgAAhsj05PhQheUV2jMAAKCB0AwAAA2EZgAAaCA0AwBAA6EZAAAaCM0AANBAaAYAgAZCMwAANBCaAQCggdAMAAANhGYAAGggNAMAQAOhGQAAGgjNAADQQGgGAEbe/MJiHnz4scwvLLY9FPapw20PAABgNy5duZoLM7MZ63Sy1O3m4rmTOXvqWNvDYp9RaQYARtb8wmIuzMzmxlI3jy/ezI2lbs7PzKo4s+eEZgBgZM1du56xzu1xZqzTydy16y2NiP1KaAYARtbxqYksdbu3HVvqdnN8aqKlEbFfCc0AwMianhzPxXMnc2SskzvHD+fIWCcXz53M9OR420Njn3EiIAAw0s6eOpYzJ45m7tr1HJ+aEJjpC6EZABh505PjwjJ9pT0DAAAaCM0AANBAaAYAgAZCMwAANBCaAQCggdAMANCi+YXFPPjwY7b+HnKWnAMAaMmlK1dzYWY2Y51OlrrdXDx3MmdPHWt7WKxDpRkAoAXzC4u5MDObG0vdPL54MzeWujk/M6viPKSEZgCAFsxdu56xzu1RbKzTydy16y2NiM0IzQAALTg+NZGlbve2Y0vdbo5PTbQ0IjYjNAMAtGB6cjwXz53MkbFO7hw/nCNjnVw8d9J24EPKiYAAAC05e+pYzpw4mrlr13N8akJgHmJCMwBAi6Ynx4XlEaA9AwAAGgjNAADQQGgGAIAGQjMAwBCwnfZwcyIgAEDLbKc9/FSaAQBaZDvt0SA0AwC0yHbao0FoBgBoke20R4PQDADQIttpjwYnAgIAtMx22sNPaAYAGAK20x5u2jMAAKCB0AwAAA2EZgAAaCA0AwBAA6EZAAAaCM0AANCgb6G5lHJXKeV9pZSPlVIeKqX8YO/4V5RSfq+U8vHev1P9GgMAAOyFflaabyb5V7XW/ybJ1yb5/lLKi5L8UJL31lpfmOS9vZ8BAGBo9S0011ofqbV+uPf940k+luRYktckeWfvau9M8s39GgMAAOyFgfQ0l1LuTvI1ST6U5Lm11keS5WCd5DmDGAMAAOxU30NzKWUyyUySN9Zav7iN272+lHK5lHL50Ucf7d8AAQCgQV9DcyllLMuB+V211t/oHf5sKeV5vcufl+Rz69221vq2WuvpWuvpZz/72f0cJgAAbKqfq2eUJG9P8rFa60+tuui+JK/rff+6JJf6NQYAANgLh/t432eS/NMkHymlXOkd++EkP57k10op353k00m+rY9jAACAXetbaK61fiBJ2eDir+/X4wIAwF6zIyAAADQQmgEAoIHQDAAADYRmAABoIDQDAEADoRkAABoIzQAA0EBoBgCABkIzAAA0EJoBAKCB0AwAAA2EZgAAaCA0AwBAA6EZAAAaCM0AANBAaAYAgAZCMwAANBCaAQCggdAMAAANhGYAAGggNAMAQAOhGQAAGgjNAAAkSeYXFvPgw49lfmGx7aEMncNtDwAAgPZdunI1F2ZmM9bpZKnbzcVzJ3P21LG2hzU0VJoBAA64+YXFXJiZzY2lbh5fvJkbS92cn5lVcV5FaAYAOODmrl3PWOf2WDjW6WTu2vWWRjR8hGYAgAPu+NRElrrd244tdbs5PjXR0oiGj9A8gjTpAwB7aXpyPBfPncyRsU7uHD+cI2OdXDx3MtOT420PbWg4EXDEaNIHAPrh7KljOXPiaOauXc/xqQmBeQ2heYSsbtK/keWPUM7PzObMiaOe2ADArk1PjssUG9CeMUI06QMAtENoHiGa9AEA2iE0jxBN+gAA7dDTPGI06QMADJ7QPII06QMADJb2DABg5NnDgH5TaQYARpo9DBgElWYAYGSt3sPg8cWbubHUzfmZWRVn9pzQDACMLHsYMChCMwAwsuxhwKAIzQDAyLKHAYPiREAAYKTZw4BBEJoBgJFnDwP6TXsGAAA0EJoBAKCB0AwAAA2EZgAAaCA0AwBAA6EZAAAaCM0AANBAaAYAgAZCMwAANBCaAQCggdAMAAANhGYAAGggNAMAQAOhGQAAGgjNAADQQGgGYFPzC4t58OHHMr+w2PZQAFpzuO0BADC8Ll25mgszsxnrdLLU7ebiuZM5e+pY28MCGDiVZgDWNb+wmAszs7mx1M3jizdzY6mb8zOzKs7AgSQ0A7CuuWvXM9a5/c/EWKeTuWvXWxoRQHuEZgDWdXxqIkvd7m3HlrrdHJ+aaGlEAO0RmgFY1/TkeC6eO5kjY53cOX44R8Y6uXjuZKYnx9seGsDAOREQgA2dPXUsZ04czdy16zk+NSEwAwdW3yrNpZR3lFI+V0r56KpjbyqlXC2lXOl9vbpfjw/A3pieHM89dz1LYAYOtH62Z/xikleuc/zf11pP9b7e08fHBwCAPdG30FxrfX+Sv+rX/QMAwKC0cSLgG0ops732jamNrlRKeX0p5XIp5fKjjz46yPEBAMBtBh2afy7J30xyKskjSd6y0RVrrW+rtZ6utZ5+9rOfPaDhAQDAUw00NNdaP1trvVVr7Sb5+SQvGeTjA4yK+YXFPPjwY3bfAxgSA11yrpTyvFrrI70fvyXJRze7PsBBdOnK1VyYmc1Yp5OlbjcXz53M2VPH2h4WwIHWt9BcSvmVJC9LcrSUMpfkR5O8rJRyKklN8skk39uvxwcYRfMLi7kwM5sbS93cyPJufOdnZnPmxFFLvgG0qG+hudb62nUOv71fjwewH8xdu56xTufJwJwkY51O5q5dF5oBWmQbbYAhcnxqIkvd7m3HlrrdHJ+aaGlEACRCM8BQmZ4cz8VzJ3NkrJM7xw/nyFgnF8+dVGUGaNlATwQEoNnZU8dy5sTRzF27nuNTEwIzwBAQmgGG0PTkuLAMMES0ZwAAQAOhGQAAGgjNAADQQGgGAIAGQjMAADQQmgEAoIHQDAAADYRmSDK/sJgHH34s8wuLbQ8FABhCNjfhwLt05WouzMxmrNPJUrebi+dO5uypY20PCwAYIirNHGjzC4u5MDObG0vdPL54MzeWujk/M6viDADcRmjmQJu7dj1jndv/NxjrdDJ37XpLI2K/0gIEMNq0Z3CgHZ+ayFK3e9uxpW43x6cmWhoR+5EWIIDRp9LMgTY9OZ6L507myFgnd44fzpGxTi6eO5npyfG2h8Y+oQUIYH9QaebAO3vqWM6cOJq5a9dzfGpCYGZPrbQA3ciXP9FYaQHyXAMYHUIzZLniLMDQD1qAAPYH7RkAfaQFCGB/UGkG6DMtQACjT2gGGAAtQACjTXsGAAA0EJoBAKCB0AwAAA2EZgAAaCA0AwBAA6EZAAAaCM0AANBAaAYAgAZCMwAANBCaAQCggdAMAAANhGYAAGggNMMAzS8s5sGHH8v8wmLbQwEAtuFw2wOAg+LSlau5MDObsU4nS91uLp47mbOnjrU9LBgq8wuLmbt2PcenJjI9Od72cACeJDTDKv36gz2/sJgLM7O5sdTNjXSTJOdnZnPmxFHBAHq8sQSGmdDMgbNRMO7nH+y5a9cz1uk8GZiTZKzTydy160IzxBtLYPgJzWzLqH90ulEw7vcf7ONTE1nqdm87ttTt5vjUxK7vG/YDbyyBYedEQLbs0pWrOXPv/fmuX/hQztx7f+67crXtIW3L6mD8+OLN3Fjq5vzM7JNvBMY6t//vsPIHey9MT47n4rmTOTLWyZ3jh3NkrJOL504KA9DjjSUw7FSa2ZL98NHpZpWsQfzBPnvqWM6cODrSlXrol5U3lufXfBLk/xNgWAjNbMl++Oh0s2A8qD/Y05PjIzNfMGjeWALDTGhmS/bDR6dNwdgfbGifN5bAsBKa2ZL98tFpUzD2BxsAWI/QzJbtl0qsYAwAbJfQzLYInADAQWTJOQAAaLCl0FxKee9WjgEAwH60aXtGKeVIkqclOVpKmUpSehc9I8nz+zw2AAAYCk09zd+b5I1ZDsgfXnX8i0ne2qcxAQDAUNk0NNdafzrJT5dSfqDW+h8GNCYAABgqTe0Zr6i13p/kainlH6+9vNb6G30bGQAADImm9ox/kOT+JN+0zmU1idAMAMC+1xSar/X+fXut9QP9HgwAAAyjpiXn/nnv35/p90AAAGBYNVWaP1ZK+WSSZ5dSZlcdL0lqrfVk30YGAABDomn1jNeWUr4yyX9JcnYwQwIAgOHSVGlOrfX/S3JPKeWOJH+rd/jPaq1LfR0ZAAAMicbQnCSllH+Y5JeSfDLLrRl3lVJeV2t9fx/HBgAAQ2FLoTnJTyX5hlrrnyVJKeVvJfmVJH+3XwMDAIBh0bR6xoqxlcCcJLXWP08y1p8hAQDAcNlqpfmBUsrbk/xy7+fvTPJAf4YEAADDZauh+fuSfH+Sf5Hlnub3J/nf+zUoAAAYJo2huZTSSfJArfWrs9zbDMA2zS8sZu7a9Ryfmsj05HjbwwFgm7ay5Fy3lPJgKeUFtdZPb/WOSynvSPKNST7XC9wppXxFkl9NcneWV+L49lrrtY3uA2A/uHTlai7MzGas08lSt5uL507m7KljbQ8LgG3Y6omAz0vyUCnlvaWU+1a+Gm7zi0leuebYDyV5b631hUne2/sZYN+aX1jMhZnZ3Fjq5vHFm7mx1M35mdnMLyy2PTQAtmGrPc1v3u4d11rfX0q5e83h1yR5We/7dyb5/SQXtnvfAKNi7tr1jHU6uZHuk8fGOp3MXbuuTQNghGwamkspR7J8EuCJJB9J8vZa681dPN5za62PJEmt9ZFSynN2cV8AQ+/41ESWut3bji11uzk+NdHSiADYiab2jHcmOZ3lwPyqJG/p+4h6SimvL6VcLqVcfvTRRwf1sAB7anpyPBfPncyRsU7uHD+cI2OdXDx3UpUZYMQ0tWe8qNb63yZJb53mP9zl4322lPK8XpX5eUk+t9EVa61vS/K2JDl9+nTd5eMCtObsqWM5c+Ko1TMARlhTpXlp5ZtdtmWsuC/J63rfvy7JpT24T4ChNz05nnvuepbADDCimirN95RSvtj7viSZ6P1cktRa6zM2umEp5VeyfNLf0VLKXJIfTfLjSX6tlPLdST6d5Nt2OX4AAOi7TUNzrfXQTu+41vraDS76+p3eJwAAtGGr6zQDAMCBJTQDAEADoRkAABoIzQAA0EBoBgCABkJzn80vLObBhx/L/MJi20MBAGCHmtZpZhcuXbmaCzOzGet0stTt5uK5kzl76ljbwwL2gfmFRTsMAgyQ0Nwn8wuLuTAzmxtL3dxIN0lyfmY2Z04c9QcO2BVvyAEGT3tGn8xdu56xzu3TO9bpZO7a9ZZGBOwHq9+QP754MzeWujk/M6sFDKDPhOY+OT41kaVu97ZjS91ujk9NtDQiYD/whhygHUJzn0xPjufiuZM5MtbJneOHc2Ssk4vnTmrNAHbFG3KAduhp7qOzp47lzImjTtYB9szKG/Lza3qavb4A9JfQ3GfTk+P+mAF7yhtygMETmgFGkDfkAIOlpxkAABoIzQAA0EBoBgCABkIzAAA0EJoBAKCB0AwAAA2EZgAAaCA0AwBAA6EZ2BfmFxbz4MOPZX5hse2hALAP2REQGHmXrlzNhZnZjHU6Wep2c/HcyZw9daztYQGwj6g0AyNtfmExF2Zmc2Opm8cXb+bGUjfnZ2ZVnAHYU0IzHFD7pZ1h7tr1jHVufykb63Qyd+16SyMCYD/SngEH0H5qZzg+NZGlbve2Y0vdbo5PTbQ0IgD2I5VmOGD2WzvD9OR4Lp47mSNjndw5fjhHxjq5eO5kpifH2x4aAPuISjOMoPmFxcxdu57jUxPbDocr7Qw38uXq7Eo7w6gGzbOnjuXMiaM7nhMAaCI0w4jZbWvFfm1nmJ4cF5YB6BvtGTBC9qK1QjsDAGyfSjOMkL1qrdDOAADbIzTDCNnL1grtDACwddozhsx+WTt32I3qPGutAIB2qDQPkf20du4wG/V51loBAIOn0jwk9tvaucNqv8zz9OR47rnrWQIzAAyI0DwkbAU8GOZ58Ea1FQYAVtOeMST269q5w8Y8D9aot8IAwAqV5iHhBK/BMM+Ds19aYQAgUWkeKmdPHcuLnveMXHn4sZy661k58dw72x7SvtTGiXS72fZ6VO3H7boBOLiE5iFy0D7K3k6Q3OvQOcg1iof599rPMK8VBoD9RGgeEqs/yl6pzJ2fmc2ZE0f3ZVVuO0FymENnk2H6va4NyP2e15VWmPNrHmM/Pp8B2P+E5iHR74+yh6k9YDtBcphC504MS4vC2oD8v/6jF+XH/vOf9H1erSkNwH4hNA+Jfn6UPWyV2u0Eyd2EzmF4ozAMLQrrvfF48289lDsOr7/03l7Ple26AdgPrJ4xJPq1qsMwrmCwnSC509B56crVnLn3/nzXL3woZ+69P/ddubr7ge/AMKzWse7a1Ic6eeJWve2YfmMA2JhK8xDpx0fZw9IesNp2el130hc7bC0dbbcorPfG41at+dFvelF+7Lf/RL8xAGyB0Dxk9vqj7GFoD1jPdoLkdkPnsL5RGLY3KWdPHcsrX/yVrbewAMAoEJr3uWFewWA7QXI71x3WNwpt2uiNh35jANgaofkAaLs9YNCG+Y1CmwRkANg5ofmAOGiB6aC9UQAA+ktoZt86aG8UAID+seTcATW/sJgHH36s1aXnAABGhUrzATRsm50AAAw7leYDZhg3OwEAGHZC8wGz7u5wvTWMAQBYn9B8wBzUNYz1cAMAuyE0b2I/Bq2VNYyPjHVy5/jhHBnr7Ps1jC9duZoz996f7/qFD+XMvffnvitX2x4SADBinAi4gf18stxBWsN4dQ/3yrba52dmc+bE0X393w0A7C2V5nUchJPlpifHc89dz9r3wVEPNwCwF4TmdYx60NqPbSU7dVB7uAGAvSU0r2OUg9Yg+3dHIZyPUg/3KMznMDN/APSTnuZ1rASt82t6mocxaK22Xv/uv/n1L/fvzi8s7lkf8yj1fI9CD/cozecwMn8A9JvQvIFRCFprrbSVrATmJFm82c1//NCn84Lpp+1ZqBjFk+umJ8eHdmyjOJ/DxPwBMAhC8yaGOWit5/jURJ641X3K8f9w/5+nlE4Wb+5NqFgvnK/0fI/SfO3UXlbsE/O5W+YPgEEQmhvsdUDqpw984vO5uU5oPtw5lJTbj+0mVIxyz/du9aMN4CDP514wfwAMQisnApZSPllK+Ugp5Uop5XIbY9iKUdoUY+Uj6lv1qZfdqt3c6t5+wW5CxSidXLeX+rUU4UGdz71i/gAYhDYrzS+vtX6+xcff1Kj1Sa73EXWS3HGo5Ce+9Z4k2dMTG/e653sUKvr9bAMYxR76YWL+AOg37RkbGLU+yfU+or7jcCfv+YGvy4nn3pkkex4q9qrne1RWPuh3G8Co9dAPG/MHQD+1tU5zTfK7pZQHSimvb2kMmxq1Psn1PqL+yW89+WRgXrlOv3cB3O5auaO0+6I2AAA4uNqqNJ+ptX6mlPKcJL9XSvnTWuv7V1+hF6ZfnyQveMELBj7AUVyrue2PqHdSMR61in7bcwwAtKOV0Fxr/Uzv38+VUt6d5CVJ3r/mOm9L8rYkOX369Dqnt/XfKAaktj6i3mkP+KhV9BNtAABwEA28PaOU8vRSyp0r3yf5hiQfHfQ4tmoQLQ37wUrFeLWVivFmtDwAAKOgjUrzc5O8u5Sy8vj/sdb6f7cwjlaMwioRO7GbivF2K/r7dQ4BgOE18NBca/3LJPcM+nGHwaisErETu+0B32rLw36eQwBgeJVaW2kX3pbTp0/Xy5eHdg+ULZlfWMyZe+/PjaUvV2OPjHXywQuv2FfV0n5WgQ/KHAIA7SmlPFBrPb32eFtLzh04O+35HTVb6QHf7rJ0Kw7KHAIAw8fmJgMyiqtE7LX5hcW860Ofzlvf9/HccejQU9ormqrU5hAAaIvQ3EdrQ+Corfu8ly5duZrzvz6bxZvLoXfx5s0kX16W7gOf+Hxjr3I/59DJhQDAZoTmPtnohLVRW/d5L6ys4bwSmFcb63Ty0Ge+uOU1nvsxh04uBACa6Gnug822hj6I6z6v14u8Yrndom6rV3mjOdxJr/QobeMNALRHpbkPRm1r6H6aX1jMF64/kSdu3XrKZeOHSy6eO5kXP/+Zu+5V3mm12O8KANgKobkPnLC2bHWQ7dbkcCeZGDucJ25184aXn8g/eekLngymu+lV3ukW3kl7vys91AAwWoTmPthPJ/3tNNytF2THD3fy1u/8O3nx85+R6cnxJ9spjk9N7KpXeTfV4jZ+V3qoAWD0CM2b2E01cD+c9LebcLdekL3jUCfPnBjL9OT4hve9k3nabbV4kL+r3VTFAYD2OBFwA5euXM2Ze+/Pd/3Ch3Lm3vtz35Wr276PUT7pb7cnyG0WZPf65LuVavGRsU7uHD+cI2OdbVeLB/W7skELAIwmleZ1qAbu/gS5zdoeHnz4sT0/+W5UKvv63QFgNAnN69hPKyrstMVkL8LdRkG2X8FxenJ86H8/+6nfHQAOEqF5HfulGribnuS9CnfrBdmDHhxHpSoOAHxZqbW2PYZGp0+frpcvXx7oY9535epTQl1T4BymZcTmFxZz5t77c2Ppy+H/yFgnH7zwim2vgtGv/6Zhmi8AgCQppTxQaz299rhK8wa2Ww0ctmXE9qrFpJ8tD6PQTgEAkFg9Y1NbXVFhq6tB7GSb551qc9OOQf03AgAMikrzHthKVXfQlWibdgAA7B2heQ80VXU3WsLuRc97Rr70xK2+9fTatAMAYG8IzXugqaq7XiU6SV71M+/PWOdQbtVufuJb79lRVbbpZLpB9Q3vp2X6AADWEpr3yGZV3affcSiLt24PzCurWizdupUk+Ze/dmXbVdlhaofYL8v0AQCsx4mAe2i9EwcvXbmab/zZD6T0lvY7MtbJHYeeOu03u8lDn/nilh9rr7ei3q292MoaAGBYqTT30epgu6LbrXnz2Rfn3777o+vcYutrZg9jO4RNOwCA/Upo7qP1gu344UM5NjWRsUMlS7e+HJLHDpW8+PnP3PJ9D2s7hLWXAYD9SHtGg92sO7xRsH3x85+Zt3zbPRk/3MnT7jiU8cOdvOXb7tn2piPaIQAABsM22pvYixPtNtuOey+2kbYVNQDA3tloG22heQPzC4s5c+/9t/UjHxnr5IMXXrHtcCrYAgCMho1Cs57mDezliXb6fAEARpue5g0M64l2AAAMntC8gVE80W43Jy0CALAx7RmbGNS6w3vR8zxMuwMCAOw3QnODfvcj70XYXb2JykoP9vmZ2W1vyw0AwPq0Z7Ror7bCXjlpcbWVkxYBANg9oblFexV2nbQIANBfQnOL9irsjuJJiwAAo0RP8xb0a3OSlbC7dsfAnTzGoE5aBAA4iITmBk0n6u02UO9l2LWJCgBAfwjNm2halWKvlnkTdgEAhpvQvInNttJOsutl3lZXqVceb+33wjQAQPuE5k1sdqLeRitczF27vqWgu7pKfePmrdRaMzF2+LbvbVICADAcrJ6xic1WpXj6HYdyY+n2QH1jqZun33Go8X7Xrs+8dKvmZjdP+X6n6zYDALC3VJobbHSi3peeuJXxQyWLt+qT1x0/VPKlJ2413ud6bR8bWWkH0aYBANAeoXkL1jtR7/jUREqnJKtCc+mULa2xvF7bx0ZsUgIA0D7tGTu0mw1F1t527FDJ4U6e8r1NSgAAhkOptTZfq2WnT5+uly9fbnsY69rNOs1WzwAAGC6llAdqrafXHteesUu7WWN57W03+n67+rWDIQDAQSU07zN7teEKAABfpqd5BMwvLObBhx9rXHpu7VJ2lqwDANgbKs1DbjuV4812MNSmAQCwcyrNQ2y7lePNdjAEAGDnhOYhtlI5Xm2lcrye3SyDt1NbbR0BABhl2jP6aGUVi6ffcShfeuLWtlez2EnleKMdDPvBSYcAwEEhNPfJSqBMkhtL3YwfKimdsq1guVI5/je//mAOlU5u1e6WKse7WQZvq1a3jqz0UJ+fmc2ZE0f1TwMA+472jD64LVAuLQfKxVt1R6tZLG89U5LS+3dIbLd1BABglAnNfbBeoFyxnWC5Er4Xb3bz10/cyuLN4VlCzkmHAMBBIjT3wXqBcsUTt7r5wvWlLQXfYa7mtnHSIQBAW/Q098n3v+xEfvZ9H08p5cme5m6SW91uvv9dH97SiXM7reYOahvtQZ50CADQJqF5i7YaRN/1B5/Km3/7T3LHoZKk5PtfdiKv+uqvzGe+cD3f80uXs3greXzxZpLmE+dWqrnn16xQsdnjD3pFi0GcdAgA0DaheQu2GkTf9Qefyo/85keTJE8s5+K89fc/kX/y0hfkS0/cyh2HDmXx5s0nr3+olLzvTz+Xl//t52wYPLdTzbWiBQBAf+hpbrDVXfnmFxbz5t966Cm3P9QpTwbeta0WX3riVt70Ww/lzL33574rVzccw/TkeO6561mNwXeYe6ABAEaZ0Nxgq0F07tr1jB166nQu3apPVohXTpx7+h2Hnrx8YfHWjpaiW48VLQAA+kNobrDVIHp8aiK3an3K7X/0m170ZIX47Klj+eCFV+TNZ1+cyfFDt11vLyrCW1nRwrbXAADbp6e5wVZPxlt9vUOlZOlWNz/6TS/Od770bzzlei//28/J/3Lpo7cd36uK8GY90La9BgDYmVLXqY4Om9OnT9fLly+3OoaV1TOefsehfOmJWxuelLd2lY2NVt2478rVpwTxfgbY+YXFnLn3/id3KEySI2OdfPDCKw7ESYKDWoYPABhtpZQHaq2n1x5Xad6i6cnxfOATn2+s1K5egm2zyu6g1zhe6c1eWVUj+XJLyH4PkSrsAMButdLTXEp5ZSnlz0opnyil/FAbY9iura6isZ3rb3VVjL1wUE8S3O7vDQBgPQMPzaWUQ0nemuRVSV6U5LWllBcNehzbtd3l3IZt+beDuu31sP0eAIDR1EZ7xkuSfKLW+pdJUkr5T0lek+RPWhjLlm23UjuMld2DuO31MP4eAIDR00Z7xrEkD6/6ea53bKhtt1I7rJXdQbaEDINh/T0AAKOljUpzWefYU5bwKKW8Psnrk+QFL3hBv8e0Jdut1B7Eyu4w8nsAAHarjdA8l+SuVT8fT/KZtVeqtb4tyduS5SXnBjO0ZqtXx+jH9ekPvwcAYDfaaM/4oyQvLKV8VSnljiTfkeS+FsYBAABbMvBKc631ZinlDUn+S5JDSd5Ra31o0OMAAICtamVzk1rre5K8p43HBgCA7WplcxMAABglQjMAADQQmrdpfmExDz78mG2YAQAOkFZ6mkfVpStXc2FmNmOdTpa63Vw8dzJnTw39viwAAOySSvMWzS8s5sLMbG4sdfP44s3cWOrm/MysijMAwAEgNG/R3LXrGevcPl1jnU7mrl1vaUQAAAyK0LxFx6cmstTt3nZsqdvN8amJlkYEAMCgCM1bND05novnTubIWCd3jh/OkbFOLp47aWtmAIADwImA23D21LGcOXE0c9eu5/jUhMAMAHBACM3bND05LiwDABww2jN2yHrNAAAHh0rzDlivGQDgYFFp3qZRWa9ZJRwAYO+oNG/TynrNN/Ll5edW1msell5nlXAAgL2l0rxNw75e86hUwgEARonQvE3Dvl6znQsBAPae9owdGOb1moe9Eg4AMIpUmndoenI899z1rKEKzMnwV8IBAEaRSvM+NMyVcACAUSQ071N2LgQA2DvaMwAAoIHQDAAADYRmAABoIDQDAEADoRkAABoIzQAA0EBoBgCABkIzAAA0EJoBAKCB0AwAAA2EZgAAaCA0b2B+YTEPPvxY5hcW2x4KAAAtO9z2AIbRpStXc2FmNmOdTpa63Vw8dzJnTx1re1gAALREpXmN+YXFXJiZzY2lbh5fvJkbS92cn5lVcQYAOMCE5jXmrl3PWOf2aRnrdDJ37XpLIwIAoG1C8xrHpyay1O3edmyp283xqYmWRgQAQNuE5jWmJ8dz8dzJHBnr5M7xwzky1snFcyczPTne9tAAAGiJEwHXcfbUsZw5cTRz167n+NSEwAwAcMAJzRuYnhwXlgEASKI9AwAAGgnNAADQQGgGAIAGQjMAADQQmgEAoIHQDAAADYRmAABoIDQDAEADoRkAABoIzQAA0EBoBgCABkIzAAA0EJoBAKCB0AwAAA2EZgAAaFBqrW2PoVEp5dEknxrwwx5N8vkBP+ZBY477zxz3l/ntP3PcX+a3/8xx/+31HP+NWuuz1x4cidDchlLK5Vrr6bbHsZ+Z4/4zx/1lfvvPHPeX+e0/c9x/g5pj7RkAANBAaAYAgAZC88be1vYADgBz3H/muL/Mb/+Z4/4yv/1njvtvIHOspxkAABqoNAMAQIMDH5pLKZ8spXyklHKllHJ5nctLKeVnSimfKKXMllL+ThvjHFWllP+6N7crX18spbxxzXVeVkr5wqrr/LuWhjsySinvKKV8rpTy0VXHvqKU8nullI/3/p3a4LavLKX8We85/UODG/Xo2GB+f6KU8qe914F3l1KetcFtN31NYdkGc/ymUsrVVa8Fr97gtp7DDTaY319dNbefLKVc2eC2nsNbUEq5q5TyvlLKx0opD5VSfrB33GvxHthkflt7LT7w7RmllE8mOV1rXXd9v96L9g8keXWSlyb56VrrSwc3wv2jlHIoydUkL621fmrV8Zcl+de11m9saWgjp5TyD5IsJPmlWutX945dTPJXtdYf770AT9VaL6y53aEkf57kf0wyl+SPkry21vonA/0PGHIbzO83JLm/1nqzlHJvkqyd3971PplNXlNYtsEcvynJQq31Jze5nefwFqw3v2suf0uSL9Ra/7d1LvtkPIcblVKel+R5tdYPl1LuTPJAkm9O8s/itXjXNpnf42nptfjAV5q34DVZftGptdY/SPKs3i+S7fv6JH+xOjCzM7XW9yf5qzWHX5Pknb3v35nlF5e1XpLkE7XWv6y1PpHkP/VuxyrrzW+t9XdrrTd7P/5Bll+42aENnsNb4Tm8BZvNbymlJPn2JL8y0EHtM7XWR2qtH+59/3iSjyU5Fq/Fe2Kj+W3ztVhoTmqS3y2lPFBKef06lx9L8vCqn+d6x9i+78jGL9J/r5TyYCnld0opLx7koPaR59ZaH0mWX2ySPGed63g+743/OcnvbHBZ02sKm3tD72PXd2zwsbbn8O79/SSfrbV+fIPLPYe3qZRyd5KvSfKheC3ec2vmd7WBvhYf3os7GXFnaq2fKaU8J8nvlVL+tPcOfUVZ5zYHu6dlB0opdyQ5m+TfrnPxh7O8ZeVCrx3mN5O8cIDDO0g8n3eplPIjSW4medcGV2l6TWFjP5fkx7L8nPyxJG/J8h/F1TyHd++12bzK7Dm8DaWUySQzSd5Ya/3iciG/+WbrHPM8Xsfa+V11fOCvxQe+0lxr/Uzv388leXeWPzJZbS7JXat+Pp7kM4MZ3b7yqiQfrrV+du0FtdYv1loXet+/J8lYKeXooAe4D3x2pXWo9+/n1rmO5/MulFJel+Qbk3xn3eCEkC28prCBWutna623aq3dJD+f9efOc3gXSimHk/zjJL+60XU8h7eulDKW5UD3rlrrb/QOey3eIxvMb2uvxQc6NJdSnt5rLk8p5elJviHJR9dc7b4k/1NZ9rVZPnHikQEPdT/YsLJRSvnKXo9dSikvyfLzcn6AY9sv7kvyut73r0tyaZ3r/FGSF5ZSvqpX/f+O3u1oUEp5ZZILSc7WWv96g+ts5TWFDaw5X+Rbsv7ceQ7vzv+Q5E9rrXPrXeg5vHW9v1tvT/KxWutPrbrIa/Ee2Gh+W30trrUe2K8k/1WSB3tfDyX5kd7x70vyfb3vS5K3JvmLJB/J8pmYrY99lL6SPC3LIfiZq46tnuM39Ob/wSw39f/3bY952L+y/AbkkSRLWa5YfHeS6STvTfLx3r9f0bvu85O8Z9VtX53ls7b/YuU572tL8/uJLPcgXul9/R9r53ej1xRfW57jX+69zs5mOUA8b+0c9372HN7B/PaO/+LKa++q63oO72yOvy7LLRWzq14XXu21uO/z29pr8YFfcg4AAJoc6PYMAADYCqEZAAAaCM0AANBAaAYAgAZCMwAANBCaAVpQSrlVSrlSSvloKeX/KqU8bY/v//dLKacbrvPG1Y9bSnlPKeVZezkOgP1CaAZox/Va66la61cneSLLa5cP2huzvI56kqTW+upa62MtjANg6AnNAO37f5KcKKV8RSnlN0sps6WUPyilnEySUsqbSim/XEq5v5Ty8VLK9/SOv6yU8tsrd1JK+dlSyj9be+ellJ8rpVwupTxUSnlz79i/yPJmAO8rpbyvd+yTK1vYl1L+Za8K/tFSyht7x+4upXyslPLzvfv63VLKRF9nBmBICM0ALSqlHE7yqizvhPfmJH9caz2Z5IeT/NKqq55M8o+S/L0k/66U8vxtPMyP1FpP9+7jH5ZSTtZafybJZ5K8vNb68jVj+rtJ/nmSlyb52iTfU0r5mt7FL0zy1lrri5M8luTcdv57AUaV0AzQjolSypUkl5N8Osnbs7xt7C8nSa31/iTTpZRn9q5/qdZ6vdb6+STvS/KSbTzWt5dSPpzkj5O8OMmLGq7/dUneXWv9Uq11IclvJPn7vcv+31rrld73DyS5exvjABhZh9seAMABdb3Wemr1gVJKWed6dc2/q4/fzO3FjyNrb1xK+aok/zrJf1drvVZK+cX1rrf2Zptctrjq+1tJtGcAB4JKM8DweH+S70yW+5WTfL7W+sXeZa8ppRwppUwneVmSP0ryqSQvKqWM9yrSX7/OfT4jyZeSfKGU8twst4KseDzJnRuM45tLKU8rpTw9ybdkue8a4MBSaQYYHm9K8n+WUmaT/HWS16267A+T/OckL0jyY7XWzyRJKeXXkswm+XiW2y9uU2t9sJTyx0keSvKXST646uK3JfmdUsojq/uaa60f7lWk/7B36BdqrX9cSrl7L/4jAUZRqXXtJ34ADJNSypuSLNRaf7LtsQAcVNozAACggUozAAA0UGkGAIAGQjMAADQQmgEAoIHQDAAADYRmAABoIDQDAECD/x/YvwfU1WN8uAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    error = y.reshape(-1,1) - np.dot(x,theta.reshape(-1,1))\n",
    "    return np.dot(error.T,error)/error.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.14546775]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # compute gradient and loss\n",
    "    error = y.reshape(-1,1) - np.dot(tx,w.reshape(-1,1))\n",
    "    gradient = -2 * np.dot(tx.T, error) / tx.shape[0]\n",
    "    loss = computeCost(tx, y, w)\n",
    "    return gradient, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and loss\n",
    "        [gradient, loss] = compute_gradient(y, X, theta)\n",
    "        \n",
    "        # update theta by gradient\n",
    "        theta = theta - alpha * gradient.reshape(-1,)\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=[[64.14546775]], w0=0.11678270103092779, w1=1.3065769949111339\n",
      "Gradient Descent(1/999): loss=[[33.53928474]], w0=0.018001608779719666, w1=0.46688514041747775\n",
      "Gradient Descent(2/999): loss=[[20.8151595]], w0=0.058230490259482424, w1=1.010398519281903\n",
      "Gradient Descent(3/999): loss=[[15.51836696]], w0=0.008955584732491167, w1=0.6624640652822505\n",
      "Gradient Descent(4/999): loss=[[13.30657507]], w0=0.017447688470967018, w1=0.8890358067492414\n",
      "Gradient Descent(5/999): loss=[[12.37621165]], w0=-0.01120565178577369, w1=0.7453450091050499\n",
      "Gradient Descent(6/999): loss=[[11.97816904]], w0=-0.0158361618250381, w1=0.840270271487082\n",
      "Gradient Descent(7/999): loss=[[11.80130755]], w0=-0.03586548478321534, w1=0.7814054019591586\n",
      "Gradient Descent(8/999): loss=[[11.71639584]], w0=-0.04588771003475005, w1=0.8216458872863022\n",
      "Gradient Descent(9/999): loss=[[11.66975757]], w0=-0.06227657702470259, w1=0.7980072763515388\n",
      "Gradient Descent(10/999): loss=[[11.63909787]], w0=-0.07447993992474643, w1=0.8155287856125967\n",
      "Gradient Descent(11/999): loss=[[11.61515605]], w0=-0.08929867579215699, w1=0.8065154361246224\n",
      "Gradient Descent(12/999): loss=[[11.59408515]], w0=-0.10235009435917988, w1=0.8145948887823524\n",
      "Gradient Descent(13/999): loss=[[11.57428676]], w0=-0.11645901891079324, w1=0.811650294700878\n",
      "Gradient Descent(14/999): loss=[[11.55509641]], w0=-0.12980521899565403, w1=0.8158022520353793\n",
      "Gradient Descent(15/999): loss=[[11.53623753]], w0=-0.14356207790797887, w1=0.8153725418897819\n",
      "Gradient Descent(16/999): loss=[[11.51759471]], w0=-0.15697367266513632, w1=0.8178874980936855\n",
      "Gradient Descent(17/999): loss=[[11.49911942]], w0=-0.17052746631980287, w1=0.8184966258951271\n",
      "Gradient Descent(18/999): loss=[[11.48079096]], w0=-0.18390959132206014, w1=0.8203259705114382\n",
      "Gradient Descent(19/999): loss=[[11.46260017]], w0=-0.19732261554827576, w1=0.8213609071086568\n",
      "Gradient Descent(20/999): loss=[[11.44454269]], w0=-0.21063627680288677, w1=0.8228998372158082\n",
      "Gradient Descent(21/999): loss=[[11.42661618]], w0=-0.22393481207017224, w1=0.8241060040160729\n",
      "Gradient Descent(22/999): loss=[[11.4088191]], w0=-0.237164218229248, w1=0.8255187039129652\n",
      "Gradient Descent(23/999): loss=[[11.39115029]], w0=-0.25035958323751545, w1=0.8267904031447669\n",
      "Gradient Descent(24/999): loss=[[11.37360871]], w0=-0.2634985771734507, w1=0.8281451065334089\n",
      "Gradient Descent(25/999): loss=[[11.35619342]], w0=-0.27659587340488007, w1=0.8294384772326225\n",
      "Gradient Descent(26/999): loss=[[11.33890349]], w0=-0.2896422966363097, w1=0.8307635647986902\n",
      "Gradient Descent(27/999): loss=[[11.321738]], w0=-0.3026440403935427, w1=0.8320604294011863\n",
      "Gradient Descent(28/999): loss=[[11.30469607]], w0=-0.3155973923913, w1=0.8333677287388969\n",
      "Gradient Descent(29/999): loss=[[11.28777679]], w0=-0.3285050233718192, w1=0.8346605768984672\n",
      "Gradient Descent(30/999): loss=[[11.27097928]], w0=-0.3413654893809772, w1=0.8359550400503488\n",
      "Gradient Descent(31/999): loss=[[11.25430268]], w0=-0.3541799972784866, w1=0.8372407917250402\n",
      "Gradient Descent(32/999): loss=[[11.2377461]], w0=-0.3669480445483487, w1=0.8385245147143746\n",
      "Gradient Descent(33/999): loss=[[11.22130868]], w0=-0.37967022932978095, w1=0.8398019296894985\n",
      "Gradient Descent(34/999): loss=[[11.20498958]], w0=-0.3923464394298649, w1=0.8410758220203125\n",
      "Gradient Descent(35/999): loss=[[11.18878792]], w0=-0.4049770194607667, w1=0.8423444239654388\n",
      "Gradient Descent(36/999): loss=[[11.17270288]], w0=-0.41756201865408726, w1=0.8436089020451879\n",
      "Gradient Descent(37/999): loss=[[11.15673361]], w0=-0.4301016756282442, w1=0.8448685316272172\n",
      "Gradient Descent(38/999): loss=[[11.14087927]], w0=-0.4425961059721868, w1=0.8461238067793512\n",
      "Gradient Descent(39/999): loss=[[11.12513905]], w0=-0.45504550359297824, w1=0.8473744360516899\n",
      "Gradient Descent(40/999): loss=[[11.10951211]], w0=-0.4674500109560824, w1=0.8486206341426101\n",
      "Gradient Descent(41/999): loss=[[11.09399764]], w0=-0.47980980271557033, w1=0.8498622894543948\n",
      "Gradient Descent(42/999): loss=[[11.07859484]], w0=-0.4921250318201305, w1=0.8510995005820563\n",
      "Gradient Descent(43/999): loss=[[11.0633029]], w0=-0.5043958642497893, w1=0.8523322305607661\n",
      "Gradient Descent(44/999): loss=[[11.04812102]], w0=-0.5166224566324605, w1=0.8535605296946278\n",
      "Gradient Descent(45/999): loss=[[11.03304841]], w0=-0.528804970672928, w1=0.8547843919497466\n",
      "Gradient Descent(46/999): loss=[[11.01808429]], w0=-0.5409435638571725, w1=0.8560038475054581\n",
      "Gradient Descent(47/999): loss=[[11.00322787]], w0=-0.5530383954466019, w1=0.8572189031078419\n",
      "Gradient Descent(48/999): loss=[[10.98847837]], w0=-0.5650896226183294, w1=0.8584295805106527\n",
      "Gradient Descent(49/999): loss=[[10.97383503]], w0=-0.5770974029560515, w1=0.8596358917014933\n",
      "Gradient Descent(50/999): loss=[[10.95929709]], w0=-0.5890618928481195, w1=0.8608378548682327\n",
      "Gradient Descent(51/999): loss=[[10.94486378]], w0=-0.6009832485233054, w1=0.8620354841094084\n",
      "Gradient Descent(52/999): loss=[[10.93053435]], w0=-0.6128616253866305, w1=0.8632287960659379\n",
      "Gradient Descent(53/999): loss=[[10.91630806]], w0=-0.6246971784507468, w1=0.8644178056474904\n",
      "Gradient Descent(54/999): loss=[[10.90218416]], w0=-0.6364900620612519, w1=0.8656025287870628\n",
      "Gradient Descent(55/999): loss=[[10.88816191]], w0=-0.6482404300770326, w1=0.8667829806660607\n",
      "Gradient Descent(56/999): loss=[[10.87424059]], w0=-0.6599484357573425, w1=0.8679591768585191\n",
      "Gradient Descent(57/999): loss=[[10.86041947]], w0=-0.6716142318378707, w1=0.8691311325940132\n",
      "Gradient Descent(58/999): loss=[[10.84669782]], w0=-0.683237970484998, w1=0.8702988632330222\n",
      "Gradient Descent(59/999): loss=[[10.83307493]], w0=-0.6948198033285465, w1=0.8714623839608985\n",
      "Gradient Descent(60/999): loss=[[10.81955009]], w0=-0.7063598814439306, w1=0.8726217099854241\n",
      "Gradient Descent(61/999): loss=[[10.8061226]], w0=-0.7178583553669055, w1=0.8737768564098156\n",
      "Gradient Descent(62/999): loss=[[10.79279174]], w0=-0.7293153750872958, w1=0.8749278383148955\n",
      "Gradient Descent(63/999): loss=[[10.77955684]], w0=-0.7407310900562598, w1=0.8760746707064573\n",
      "Gradient Descent(64/999): loss=[[10.76641719]], w0=-0.7521056491848178, w1=0.8772173685495103\n",
      "Gradient Descent(65/999): loss=[[10.75337211]], w0=-0.7634392008479995, w1=0.878355946746532\n",
      "Gradient Descent(66/999): loss=[[10.74042092]], w0=-0.7747318928853587, w1=0.8794904201518022\n",
      "Gradient Descent(67/999): loss=[[10.72756295]], w0=-0.7859838726038172, w1=0.880620803562486\n",
      "Gradient Descent(68/999): loss=[[10.71479752]], w0=-0.7971952867789964, w1=0.8817471117246971\n",
      "Gradient Descent(69/999): loss=[[10.70212397]], w0=-0.8083662816575123, w1=0.8828693593299077\n",
      "Gradient Descent(70/999): loss=[[10.68954163]], w0=-0.8194970029586379, w1=0.8839875610175774\n",
      "Gradient Descent(71/999): loss=[[10.67704986]], w0=-0.8305875958763619, w1=0.8851017313737736\n",
      "Gradient Descent(72/999): loss=[[10.66464799]], w0=-0.8416382050811811, w1=0.886211884932374\n",
      "Gradient Descent(73/999): loss=[[10.65233539]], w0=-0.8526489747220534, w1=0.8873180361746038\n",
      "Gradient Descent(74/999): loss=[[10.6401114]], w0=-0.8636200484282351, w1=0.8884201995296448\n",
      "Gradient Descent(75/999): loss=[[10.62797539]], w0=-0.8745515693111825, w1=0.8895183893745532\n",
      "Gradient Descent(76/999): loss=[[10.61592673]], w0=-0.8854436799664006, w1=0.8906126200346213\n",
      "Gradient Descent(77/999): loss=[[10.60396479]], w0=-0.8962965224753148, w1=0.8917029057834518\n",
      "Gradient Descent(78/999): loss=[[10.59208895]], w0=-0.9071102384071168, w1=0.8927892608432169\n",
      "Gradient Descent(79/999): loss=[[10.58029858]], w0=-0.9178849688206162, w1=0.8938716993847968\n",
      "Gradient Descent(80/999): loss=[[10.56859307]], w0=-0.9286208542660773, w1=0.8949502355279948\n",
      "Gradient Descent(81/999): loss=[[10.55697181]], w0=-0.9393180347870546, w1=0.8960248833417019\n",
      "Gradient Descent(82/999): loss=[[10.54543419]], w0=-0.949976649922218, w1=0.8970956568440931\n",
      "Gradient Descent(83/999): loss=[[10.53397961]], w0=-0.9605968387071744, w1=0.8981625700028019\n",
      "Gradient Descent(84/999): loss=[[10.52260748]], w0=-0.9711787396762803, w1=0.8992256367351077\n",
      "Gradient Descent(85/999): loss=[[10.51131719]], w0=-0.9817224908644495, w1=0.9002848709081145\n",
      "Gradient Descent(86/999): loss=[[10.50010816]], w0=-0.9922282298089533, w1=0.9013402863389334\n",
      "Gradient Descent(87/999): loss=[[10.48897982]], w0=-1.002696093551215, w1=0.9023918967948614\n",
      "Gradient Descent(88/999): loss=[[10.47793156]], w0=-1.0131262186385972, w1=0.9034397159935627\n",
      "Gradient Descent(89/999): loss=[[10.46696283]], w0=-1.0235187411261828, w1=0.9044837576032456\n",
      "Gradient Descent(90/999): loss=[[10.45607304]], w0=-1.0338737965785505, w1=0.9055240352428429\n",
      "Gradient Descent(91/999): loss=[[10.44526163]], w0=-1.0441915200715426, w1=0.9065605624821879\n",
      "Gradient Descent(92/999): loss=[[10.43452803]], w0=-1.0544720461940271, w1=0.9075933528421922\n",
      "Gradient Descent(93/999): loss=[[10.42387168]], w0=-1.0647155090496532, w1=0.9086224197950221\n",
      "Gradient Descent(94/999): loss=[[10.41329204]], w0=-1.0749220422586008, w1=0.909647776764274\n",
      "Gradient Descent(95/999): loss=[[10.40278854]], w0=-1.0850917789593233, w1=0.9106694371251497\n",
      "Gradient Descent(96/999): loss=[[10.39236064]], w0=-1.095224851810285, w1=0.9116874142046307\n",
      "Gradient Descent(97/999): loss=[[10.38200779]], w0=-1.1053213929916903, w1=0.9127017212816524\n",
      "Gradient Descent(98/999): loss=[[10.37172945]], w0=-1.115381534207209, w1=0.913712371587277\n",
      "Gradient Descent(99/999): loss=[[10.3615251]], w0=-1.1254054066856942, w1=0.914719378304866\n",
      "Gradient Descent(100/999): loss=[[10.35139418]], w0=-1.1353931411828935, w1=0.915722754570253\n",
      "Gradient Descent(101/999): loss=[[10.34133618]], w0=-1.1453448679831548, w1=0.9167225134719134\n",
      "Gradient Descent(102/999): loss=[[10.33135058]], w0=-1.1552607169011262, w1=0.9177186680511371\n",
      "Gradient Descent(103/999): loss=[[10.32143684]], w0=-1.1651408172834492, w1=0.9187112313021967\n",
      "Gradient Descent(104/999): loss=[[10.31159446]], w0=-1.1749852980104456, w1=0.9197002161725192\n",
      "Gradient Descent(105/999): loss=[[10.30182292]], w0=-1.1847942874977992, w1=0.9206856355628522\n",
      "Gradient Descent(106/999): loss=[[10.2921217]], w0=-1.1945679136982306, w1=0.921667502327435\n",
      "Gradient Descent(107/999): loss=[[10.28249031]], w0=-1.2043063041031663, w1=0.9226458292741638\n",
      "Gradient Descent(108/999): loss=[[10.27292824]], w0=-1.2140095857444015, w1=0.9236206291647606\n",
      "Gradient Descent(109/999): loss=[[10.263435]], w0=-1.223677885195758, w1=0.9245919147149386\n",
      "Gradient Descent(110/999): loss=[[10.25401008]], w0=-1.2333113285747341, w1=0.9255596985945684\n",
      "Gradient Descent(111/999): loss=[[10.24465299]], w0=-1.2429100415441507, w1=0.9265239934278436\n",
      "Gradient Descent(112/999): loss=[[10.23536325]], w0=-1.2524741493137903, w1=0.9274848117934449\n",
      "Gradient Descent(113/999): loss=[[10.22614037]], w0=-1.2620037766420298, w1=0.9284421662247044\n",
      "Gradient Descent(114/999): loss=[[10.21698387]], w0=-1.2714990478374681, w1=0.9293960692097694\n",
      "Gradient Descent(115/999): loss=[[10.20789328]], w0=-1.2809600867605484, w1=0.9303465331917646\n",
      "Gradient Descent(116/999): loss=[[10.19886811]], w0=-1.2903870168251728, w1=0.9312935705689555\n",
      "Gradient Descent(117/999): loss=[[10.18990791]], w0=-1.2997799610003127, w1=0.9322371936949088\n",
      "Gradient Descent(118/999): loss=[[10.18101219]], w0=-1.309139041811613, w1=0.9331774148786547\n",
      "Gradient Descent(119/999): loss=[[10.1721805]], w0=-1.3184643813429897, w1=0.9341142463848466\n",
      "Gradient Descent(120/999): loss=[[10.16341237]], w0=-1.3277561012382235, w1=0.9350477004339222\n",
      "Gradient Descent(121/999): loss=[[10.15470735]], w0=-1.3370143227025455, w1=0.9359777892022608\n",
      "Gradient Descent(122/999): loss=[[10.14606498]], w0=-1.3462391665042188, w1=0.9369045248223453\n",
      "Gradient Descent(123/999): loss=[[10.13748482]], w0=-1.355430752976114, w1=0.9378279193829172\n",
      "Gradient Descent(124/999): loss=[[10.12896641]], w0=-1.3645892020172785, w1=0.9387479849291366\n",
      "Gradient Descent(125/999): loss=[[10.12050931]], w0=-1.3737146330945005, w1=0.939664733462738\n",
      "Gradient Descent(126/999): loss=[[10.11211308]], w0=-1.3828071652438676, w1=0.9405781769421877\n",
      "Gradient Descent(127/999): loss=[[10.10377728]], w0=-1.3918669170723197, w1=0.9414883272828394\n",
      "Gradient Descent(128/999): loss=[[10.09550148]], w0=-1.4008940067591957, w1=0.9423951963570893\n",
      "Gradient Descent(129/999): loss=[[10.08728524]], w0=-1.4098885520577755, w1=0.9432987959945319\n",
      "Gradient Descent(130/999): loss=[[10.07912814]], w0=-1.4188506702968158, w1=0.9441991379821133\n",
      "Gradient Descent(131/999): loss=[[10.07102974]], w0=-1.4277804783820807, w1=0.9450962340642856\n",
      "Gradient Descent(132/999): loss=[[10.06298964]], w0=-1.4366780927978664, w1=0.9459900959431599\n",
      "Gradient Descent(133/999): loss=[[10.0550074]], w0=-1.4455436296085211, w1=0.9468807352786586\n",
      "Gradient Descent(134/999): loss=[[10.04708261]], w0=-1.4543772044599588, w1=0.9477681636886686\n",
      "Gradient Descent(135/999): loss=[[10.03921486]], w0=-1.4631789325811677, w1=0.9486523927491912\n",
      "Gradient Descent(136/999): loss=[[10.03140374]], w0=-1.4719489287857135, w1=0.949533433994495\n",
      "Gradient Descent(137/999): loss=[[10.02364884]], w0=-1.480687307473237, w1=0.9504112989172641\n",
      "Gradient Descent(138/999): loss=[[10.01594975]], w0=-1.4893941826309463, w1=0.9512859989687503\n",
      "Gradient Descent(139/999): loss=[[10.00830608]], w0=-1.4980696678351038, w1=0.9521575455589213\n",
      "Gradient Descent(140/999): loss=[[10.00071742]], w0=-1.5067138762525076, w1=0.9530259500566092\n",
      "Gradient Descent(141/999): loss=[[9.99318338]], w0=-1.515326920641968, w1=0.9538912237896596\n",
      "Gradient Descent(142/999): loss=[[9.98570356]], w0=-1.523908913355778, w1=0.954753378045079\n",
      "Gradient Descent(143/999): loss=[[9.97827758]], w0=-1.5324599663411793, w1=0.9556124240691822\n",
      "Gradient Descent(144/999): loss=[[9.97090505]], w0=-1.5409801911418222, w1=0.9564683730677386\n",
      "Gradient Descent(145/999): loss=[[9.96358558]], w0=-1.5494696988992205, w1=0.9573212362061191\n",
      "Gradient Descent(146/999): loss=[[9.95631879]], w0=-1.557928600354202, w1=0.9581710246094405\n",
      "Gradient Descent(147/999): loss=[[9.9491043]], w0=-1.5663570058483525, w1=0.9590177493627126\n",
      "Gradient Descent(148/999): loss=[[9.94194174]], w0=-1.5747550253254547, w1=0.9598614215109803\n",
      "Gradient Descent(149/999): loss=[[9.93483073]], w0=-1.5831227683329239, w1=0.9607020520594702\n",
      "Gradient Descent(150/999): loss=[[9.9277709]], w0=-1.5914603440232349, w1=0.9615396519737319\n",
      "Gradient Descent(151/999): loss=[[9.92076188]], w0=-1.5997678611553474, w1=0.9623742321797832\n",
      "Gradient Descent(152/999): loss=[[9.91380331]], w0=-1.6080454280961245, w1=0.9632058035642502\n",
      "Gradient Descent(153/999): loss=[[9.90689483]], w0=-1.6162931528217455, w1=0.964034376974512\n",
      "Gradient Descent(154/999): loss=[[9.90003606]], w0=-1.6245111429191152, w1=0.9648599632188396\n",
      "Gradient Descent(155/999): loss=[[9.89322666]], w0=-1.6326995055872668, w1=0.965682573066539\n",
      "Gradient Descent(156/999): loss=[[9.88646627]], w0=-1.6408583476387606, w1=0.9665022172480904\n",
      "Gradient Descent(157/999): loss=[[9.87975454]], w0=-1.648987775501077, w1=0.9673189064552885\n",
      "Gradient Descent(158/999): loss=[[9.87309112]], w0=-1.6570878952180048, w1=0.9681326513413822\n",
      "Gradient Descent(159/999): loss=[[9.86647565]], w0=-1.665158812451025, w1=0.9689434625212133\n",
      "Gradient Descent(160/999): loss=[[9.8599078]], w0=-1.6732006324806887, w1=0.9697513505713552\n",
      "Gradient Descent(161/999): loss=[[9.85338722]], w0=-1.68121346020799, w1=0.9705563260302505\n",
      "Gradient Descent(162/999): loss=[[9.84691357]], w0=-1.689197400155735, w1=0.9713583993983489\n",
      "Gradient Descent(163/999): loss=[[9.84048652]], w0=-1.6971525564699055, w1=0.9721575811382439\n",
      "Gradient Descent(164/999): loss=[[9.83410572]], w0=-1.7050790329210164, w1=0.9729538816748093\n",
      "Gradient Descent(165/999): loss=[[9.82777085]], w0=-1.7129769329054705, w1=0.9737473113953352\n",
      "Gradient Descent(166/999): loss=[[9.82148157]], w0=-1.7208463594469063, w1=0.9745378806496638\n",
      "Gradient Descent(167/999): loss=[[9.81523756]], w0=-1.7286874151975429, w1=0.9753255997503236\n",
      "Gradient Descent(168/999): loss=[[9.80903849]], w0=-1.736500202439518, w1=0.9761104789726652\n",
      "Gradient Descent(169/999): loss=[[9.80288403]], w0=-1.744284823086223, w1=0.9768925285549939\n",
      "Gradient Descent(170/999): loss=[[9.79677387]], w0=-1.7520413786836313, w1=0.977671758698704\n",
      "Gradient Descent(171/999): loss=[[9.79070769]], w0=-1.7597699704116245, w1=0.9784481795684128\n",
      "Gradient Descent(172/999): loss=[[9.78468517]], w0=-1.7674706990853108, w1=0.9792218012920906\n",
      "Gradient Descent(173/999): loss=[[9.778706]], w0=-1.7751436651563408, w1=0.9799926339611958\n",
      "Gradient Descent(174/999): loss=[[9.77276986]], w0=-1.7827889687142175, w1=0.9807606876308045\n",
      "Gradient Descent(175/999): loss=[[9.76687645]], w0=-1.7904067094876022, w1=0.9815259723197424\n",
      "Gradient Descent(176/999): loss=[[9.76102545]], w0=-1.797996986845615, w1=0.9822884980107156\n",
      "Gradient Descent(177/999): loss=[[9.75521656]], w0=-1.8055598997991316, w1=0.9830482746504403\n",
      "Gradient Descent(178/999): loss=[[9.74944949]], w0=-1.8130955470020744, w1=0.9838053121497734\n",
      "Gradient Descent(179/999): loss=[[9.74372392]], w0=-1.8206040267526995, w1=0.9845596203838406\n",
      "Gradient Descent(180/999): loss=[[9.73803956]], w0=-1.828085436994879, w1=0.9853112091921664\n",
      "Gradient Descent(181/999): loss=[[9.73239611]], w0=-1.8355398753193783, w1=0.9860600883788019\n",
      "Gradient Descent(182/999): loss=[[9.72679328]], w0=-1.8429674389651298, w1=0.9868062677124522\n",
      "Gradient Descent(183/999): loss=[[9.72123078]], w0=-1.8503682248205007, w1=0.9875497569266051\n",
      "Gradient Descent(184/999): loss=[[9.71570831]], w0=-1.857742329424557, w1=0.9882905657196567\n",
      "Gradient Descent(185/999): loss=[[9.71022559]], w0=-1.8650898489683232, w1=0.9890287037550388\n",
      "Gradient Descent(186/999): loss=[[9.70478233]], w0=-1.8724108792960361, w1=0.9897641806613446\n",
      "Gradient Descent(187/999): loss=[[9.69937825]], w0=-1.8797055159063965, w1=0.9904970060324546\n",
      "Gradient Descent(188/999): loss=[[9.69401306]], w0=-1.8869738539538132, w1=0.991227189427661\n",
      "Gradient Descent(189/999): loss=[[9.68868649]], w0=-1.8942159882496457, w1=0.9919547403717937\n",
      "Gradient Descent(190/999): loss=[[9.68339825]], w0=-1.9014320132634401, w1=0.9926796683553427\n",
      "Gradient Descent(191/999): loss=[[9.67814808]], w0=-1.908622023124162, w1=0.9934019828345843\n",
      "Gradient Descent(192/999): loss=[[9.6729357]], w0=-1.9157861116214236, w1=0.9941216932317017\n",
      "Gradient Descent(193/999): loss=[[9.66776083]], w0=-1.9229243722067082, w1=0.9948388089349103\n",
      "Gradient Descent(194/999): loss=[[9.6626232]], w0=-1.9300368979945879, w1=0.9955533392985791\n",
      "Gradient Descent(195/999): loss=[[9.65752256]], w0=-1.9371237817639393, w1=0.9962652936433523\n",
      "Gradient Descent(196/999): loss=[[9.65245862]], w0=-1.9441851159591532, w1=0.996974681256272\n",
      "Gradient Descent(197/999): loss=[[9.64743113]], w0=-1.9512209926913409, w1=0.9976815113908979\n",
      "Gradient Descent(198/999): loss=[[9.64243983]], w0=-1.9582315037395353, w1=0.9983857932674298\n",
      "Gradient Descent(199/999): loss=[[9.63748445]], w0=-1.965216740551888, w1=0.9990875360728265\n",
      "Gradient Descent(200/999): loss=[[9.63256474]], w0=-1.9721767942468635, w1=0.9997867489609258\n",
      "Gradient Descent(201/999): loss=[[9.62768043]], w0=-1.9791117556144258, w1=1.0004834410525647\n",
      "Gradient Descent(202/999): loss=[[9.62283128]], w0=-1.9860217151172237, w1=1.001177621435697\n",
      "Gradient Descent(203/999): loss=[[9.61801703]], w0=-1.9929067628917714, w1=1.001869299165514\n",
      "Gradient Descent(204/999): loss=[[9.61323743]], w0=-1.9997669887496234, w1=1.0025584832645598\n",
      "Gradient Descent(205/999): loss=[[9.60849224]], w0=-2.0066024821785464, w1=1.0032451827228517\n",
      "Gradient Descent(206/999): loss=[[9.60378119]], w0=-2.0134133323436862, w1=1.0039294064979956\n",
      "Gradient Descent(207/999): loss=[[9.59910405]], w0=-2.0201996280887315, w1=1.0046111635153032\n",
      "Gradient Descent(208/999): loss=[[9.59446058]], w0=-2.0269614579370727, w1=1.0052904626679104\n",
      "Gradient Descent(209/999): loss=[[9.58985052]], w0=-2.033698910092956, w1=1.0059673128168891\n",
      "Gradient Descent(210/999): loss=[[9.58527365]], w0=-2.040412072442634, w1=1.006641722791368\n",
      "Gradient Descent(211/999): loss=[[9.58072972]], w0=-2.0471010325555135, w1=1.0073137013886428\n",
      "Gradient Descent(212/999): loss=[[9.57621849]], w0=-2.0537658776852963, w1=1.007983257374295\n",
      "Gradient Descent(213/999): loss=[[9.57173973]], w0=-2.060406694771118, w1=1.0086503994823033\n",
      "Gradient Descent(214/999): loss=[[9.56729321]], w0=-2.067023570438682, w1=1.0093151364151594\n",
      "Gradient Descent(215/999): loss=[[9.56287869]], w0=-2.0736165910013886, w1=1.0099774768439806\n",
      "Gradient Descent(216/999): loss=[[9.55849594]], w0=-2.0801858424614634, w1=1.0106374294086233\n",
      "Gradient Descent(217/999): loss=[[9.55414474]], w0=-2.086731410511076, w1=1.0112950027177954\n",
      "Gradient Descent(218/999): loss=[[9.54982485]], w0=-2.09325338053346, w1=1.011950205349169\n",
      "Gradient Descent(219/999): loss=[[9.54553606]], w0=-2.099751837604026, w1=1.0126030458494915\n",
      "Gradient Descent(220/999): loss=[[9.54127813]], w0=-2.1062268664914714, w1=1.0132535327346985\n",
      "Gradient Descent(221/999): loss=[[9.53705086]], w0=-2.112678551658886, w1=1.013901674490023\n",
      "Gradient Descent(222/999): loss=[[9.532854]], w0=-2.1191069772648543, w1=1.0145474795701084\n",
      "Gradient Descent(223/999): loss=[[9.52868735]], w0=-2.125512227164553, w1=1.0151909563991162\n",
      "Gradient Descent(224/999): loss=[[9.5245507]], w0=-2.1318943849108445, w1=1.0158321133708377\n",
      "Gradient Descent(225/999): loss=[[9.52044381]], w0=-2.138253533755367, w1=1.0164709588488032\n",
      "Gradient Descent(226/999): loss=[[9.51636648]], w0=-2.1445897566496215, w1=1.0171075011663908\n",
      "Gradient Descent(227/999): loss=[[9.5123185]], w0=-2.1509031362460513, w1=1.0177417486269347\n",
      "Gradient Descent(228/999): loss=[[9.50829966]], w0=-2.1571937548991236, w1=1.0183737095038345\n",
      "Gradient Descent(229/999): loss=[[9.50430974]], w0=-2.163461694666401, w1=1.0190033920406623\n",
      "Gradient Descent(230/999): loss=[[9.50034853]], w0=-2.169707037309613, w1=1.0196308044512712\n",
      "Gradient Descent(231/999): loss=[[9.49641584]], w0=-2.1759298642957225, w1=1.0202559549199006\n",
      "Gradient Descent(232/999): loss=[[9.49251145]], w0=-2.1821302567979886, w1=1.0208788516012852\n",
      "Gradient Descent(233/999): loss=[[9.48863516]], w0=-2.1883082956970243, w1=1.0214995026207598\n",
      "Gradient Descent(234/999): loss=[[9.48478678]], w0=-2.1944640615818534, w1=1.0221179160743663\n",
      "Gradient Descent(235/999): loss=[[9.48096609]], w0=-2.200597634750961, w1=1.0227341000289578\n",
      "Gradient Descent(236/999): loss=[[9.4771729]], w0=-2.2067090952133395, w1=1.0233480625223061\n",
      "Gradient Descent(237/999): loss=[[9.47340701]], w0=-2.212798522689535, w1=1.0239598115632045\n",
      "Gradient Descent(238/999): loss=[[9.46966822]], w0=-2.2188659966126854, w1=1.0245693551315733\n",
      "Gradient Descent(239/999): loss=[[9.46595635]], w0=-2.224911596129556, w1=1.0251767011785635\n",
      "Gradient Descent(240/999): loss=[[9.46227119]], w0=-2.2309354001015738, w1=1.025781857626661\n",
      "Gradient Descent(241/999): loss=[[9.45861255]], w0=-2.236937487105855, w1=1.0263848323697893\n",
      "Gradient Descent(242/999): loss=[[9.45498025]], w0=-2.24291793543623, w1=1.026985633273413\n",
      "Gradient Descent(243/999): loss=[[9.45137409]], w0=-2.2488768231042657, w1=1.02758426817464\n",
      "Gradient Descent(244/999): loss=[[9.44779388]], w0=-2.254814227840281, w1=1.0281807448823244\n",
      "Gradient Descent(245/999): loss=[[9.44423945]], w0=-2.2607302270943634, w1=1.0287750711771668\n",
      "Gradient Descent(246/999): loss=[[9.44071059]], w0=-2.266624898037377, w1=1.0293672548118185\n",
      "Gradient Descent(247/999): loss=[[9.43720714]], w0=-2.272498317561971, w1=1.0299573035109792\n",
      "Gradient Descent(248/999): loss=[[9.4337289]], w0=-2.2783505622835816, w1=1.0305452249715015\n",
      "Gradient Descent(249/999): loss=[[9.43027569]], w0=-2.284181708541431, w1=1.0311310268624874\n",
      "Gradient Descent(250/999): loss=[[9.42684734]], w0=-2.289991832399525, w1=1.0317147168253917\n",
      "Gradient Descent(251/999): loss=[[9.42344366]], w0=-2.2957810096476434, w1=1.03229630247412\n",
      "Gradient Descent(252/999): loss=[[9.42006448]], w0=-2.3015493158023292, w1=1.032875791395128\n",
      "Gradient Descent(253/999): loss=[[9.41670962]], w0=-2.307296826107874, w1=1.033453191147521\n",
      "Gradient Descent(254/999): loss=[[9.41337891]], w0=-2.3130236155372996, w1=1.0340285092631523\n",
      "Gradient Descent(255/999): loss=[[9.41007217]], w0=-2.318729758793335, w1=1.0346017532467213\n",
      "Gradient Descent(256/999): loss=[[9.40678923]], w0=-2.3244153303093924, w1=1.0351729305758715\n",
      "Gradient Descent(257/999): loss=[[9.40352992]], w0=-2.3300804042505368, w1=1.035742048701288\n",
      "Gradient Descent(258/999): loss=[[9.40029407]], w0=-2.3357250545144534, w1=1.0363091150467956\n",
      "Gradient Descent(259/999): loss=[[9.39708151]], w0=-2.3413493547324133, w1=1.036874137009453\n",
      "Gradient Descent(260/999): loss=[[9.39389207]], w0=-2.346953378270232, w1=1.0374371219596532\n",
      "Gradient Descent(261/999): loss=[[9.39072558]], w0=-2.352537198229227, w1=1.0379980772412156\n",
      "Gradient Descent(262/999): loss=[[9.38758189]], w0=-2.358100887447172, w1=1.0385570101714854\n",
      "Gradient Descent(263/999): loss=[[9.38446082]], w0=-2.3636445184992465, w1=1.0391139280414259\n",
      "Gradient Descent(264/999): loss=[[9.38136221]], w0=-2.369168163698982, w1=1.0396688381157164\n",
      "Gradient Descent(265/999): loss=[[9.37828591]], w0=-2.374671895099207, w1=1.0402217476328444\n",
      "Gradient Descent(266/999): loss=[[9.37523175]], w0=-2.3801557844929846, w1=1.0407726638052022\n",
      "Gradient Descent(267/999): loss=[[9.37219957]], w0=-2.385619903414551, w1=1.0413215938191795\n",
      "Gradient Descent(268/999): loss=[[9.36918921]], w0=-2.3910643231402466, w1=1.0418685448352583\n",
      "Gradient Descent(269/999): loss=[[9.36620053]], w0=-2.3964891146894485, w1=1.042413523988105\n",
      "Gradient Descent(270/999): loss=[[9.36323335]], w0=-2.4018943488254942, w1=1.0429565383866641\n",
      "Gradient Descent(271/999): loss=[[9.36028753]], w0=-2.4072800960566068, w1=1.0434975951142522\n",
      "Gradient Descent(272/999): loss=[[9.35736291]], w0=-2.412646426636812, w1=1.0440367012286473\n",
      "Gradient Descent(273/999): loss=[[9.35445933]], w0=-2.4179934105668583, w1=1.0445738637621849\n",
      "Gradient Descent(274/999): loss=[[9.35157666]], w0=-2.423321117595127, w1=1.0451090897218454\n",
      "Gradient Descent(275/999): loss=[[9.34871474]], w0=-2.428629617218543, w1=1.0456423860893493\n",
      "Gradient Descent(276/999): loss=[[9.34587341]], w0=-2.4339189786834816, w1=1.0461737598212464\n",
      "Gradient Descent(277/999): loss=[[9.34305254]], w0=-2.4391892709866725, w1=1.0467032178490057\n",
      "Gradient Descent(278/999): loss=[[9.34025196]], w0=-2.4444405628760975, w1=1.0472307670791083\n",
      "Gradient Descent(279/999): loss=[[9.33747155]], w0=-2.4496729228518896, w1=1.0477564143931357\n",
      "Gradient Descent(280/999): loss=[[9.33471114]], w0=-2.4548864191672264, w1=1.0482801666478594\n",
      "Gradient Descent(281/999): loss=[[9.3319706]], w0=-2.4600811198292183, w1=1.0488020306753318\n",
      "Gradient Descent(282/999): loss=[[9.32924979]], w0=-2.4652570925997974, w1=1.0493220132829735\n",
      "Gradient Descent(283/999): loss=[[9.32654856]], w0=-2.470414404996602, w1=1.0498401212536639\n",
      "Gradient Descent(284/999): loss=[[9.32386677]], w0=-2.475553124293855, w1=1.0503563613458284\n",
      "Gradient Descent(285/999): loss=[[9.32120429]], w0=-2.4806733175232436, w1=1.0508707402935273\n",
      "Gradient Descent(286/999): loss=[[9.31856097]], w0=-2.485775051474793, w1=1.0513832648065424\n",
      "Gradient Descent(287/999): loss=[[9.31593667]], w0=-2.490858392697738, w1=1.0518939415704667\n",
      "Gradient Descent(288/999): loss=[[9.31333126]], w0=-2.4959234075013894, w1=1.0524027772467899\n",
      "Gradient Descent(289/999): loss=[[9.3107446]], w0=-2.5009701619560007, w1=1.052909778472985\n",
      "Gradient Descent(290/999): loss=[[9.30817656]], w0=-2.50599872189363, w1=1.0534149518625968\n",
      "Gradient Descent(291/999): loss=[[9.305627]], w0=-2.511009152908998, w1=1.0539183040053262\n",
      "Gradient Descent(292/999): loss=[[9.30309579]], w0=-2.5160015203603434, w1=1.0544198414671175\n",
      "Gradient Descent(293/999): loss=[[9.3005828]], w0=-2.5209758893702765, w1=1.054919570790243\n",
      "Gradient Descent(294/999): loss=[[9.2980879]], w0=-2.5259323248266274, w1=1.0554174984933893\n",
      "Gradient Descent(295/999): loss=[[9.29561096]], w0=-2.530870891383294, w1=1.0559136310717412\n",
      "Gradient Descent(296/999): loss=[[9.29315184]], w0=-2.535791653461084, w1=1.0564079749970685\n",
      "Gradient Descent(297/999): loss=[[9.29071042]], w0=-2.540694675248556, w1=1.0569005367178073\n",
      "Gradient Descent(298/999): loss=[[9.28828657]], w0=-2.5455800207028565, w1=1.0573913226591471\n",
      "Gradient Descent(299/999): loss=[[9.28588017]], w0=-2.5504477535505536, w1=1.057880339223113\n",
      "Gradient Descent(300/999): loss=[[9.28349109]], w0=-2.55529793728847, w1=1.0583675927886493\n",
      "Gradient Descent(301/999): loss=[[9.2811192]], w0=-2.560130635184509, w1=1.0588530897117032\n",
      "Gradient Descent(302/999): loss=[[9.27876439]], w0=-2.564945910278482, w1=1.059336836325308\n",
      "Gradient Descent(303/999): loss=[[9.27642652]], w0=-2.5697438253829294, w1=1.0598188389396637\n",
      "Gradient Descent(304/999): loss=[[9.27410548]], w0=-2.5745244430839405, w1=1.060299103842223\n",
      "Gradient Descent(305/999): loss=[[9.27180115]], w0=-2.5792878257419694, w1=1.0607776372977689\n",
      "Gradient Descent(306/999): loss=[[9.2695134]], w0=-2.5840340354926488, w1=1.0612544455485005\n",
      "Gradient Descent(307/999): loss=[[9.26724211]], w0=-2.588763134247601, w1=1.061729534814111\n",
      "Gradient Descent(308/999): loss=[[9.26498718]], w0=-2.593475183695245, w1=1.062202911291872\n",
      "Gradient Descent(309/999): loss=[[9.26274847]], w0=-2.5981702453016005, w1=1.0626745811567113\n",
      "Gradient Descent(310/999): loss=[[9.26052588]], w0=-2.6028483803110913, w1=1.0631445505612962\n",
      "Gradient Descent(311/999): loss=[[9.25831928]], w0=-2.607509649747343, w1=1.063612825636111\n",
      "Gradient Descent(312/999): loss=[[9.25612856]], w0=-2.612154114413979, w1=1.0640794124895399\n",
      "Gradient Descent(313/999): loss=[[9.25395362]], w0=-2.6167818348954146, w1=1.064544317207944\n",
      "Gradient Descent(314/999): loss=[[9.25179432]], w0=-2.621392871557646, w1=1.065007545855743\n",
      "Gradient Descent(315/999): loss=[[9.24965057]], w0=-2.6259872845490393, w1=1.065469104475492\n",
      "Gradient Descent(316/999): loss=[[9.24752225]], w0=-2.630565133801113, w1=1.065928999087963\n",
      "Gradient Descent(317/999): loss=[[9.24540924]], w0=-2.635126479029322, w1=1.0663872356922202\n",
      "Gradient Descent(318/999): loss=[[9.24331145]], w0=-2.6396713797338354, w1=1.0668438202657016\n",
      "Gradient Descent(319/999): loss=[[9.24122875]], w0=-2.644199895200312, w1=1.0672987587642944\n",
      "Gradient Descent(320/999): loss=[[9.23916104]], w0=-2.648712084500676, w1=1.0677520571224144\n",
      "Gradient Descent(321/999): loss=[[9.23710821]], w0=-2.6532080064938843, w1=1.068203721253082\n",
      "Gradient Descent(322/999): loss=[[9.23507016]], w0=-2.6576877198266966, w1=1.0686537570480001\n",
      "Gradient Descent(323/999): loss=[[9.23304678]], w0=-2.6621512829344405, w1=1.069102170377632\n",
      "Gradient Descent(324/999): loss=[[9.23103796]], w0=-2.6665987540417717, w1=1.0695489670912752\n",
      "Gradient Descent(325/999): loss=[[9.2290436]], w0=-2.671030191163436, w1=1.0699941530171404\n",
      "Gradient Descent(326/999): loss=[[9.22706359]], w0=-2.675445652105025, w1=1.0704377339624265\n",
      "Gradient Descent(327/999): loss=[[9.22509784]], w0=-2.679845194463729, w1=1.0708797157133956\n",
      "Gradient Descent(328/999): loss=[[9.22314623]], w0=-2.6842288756290897, w1=1.07132010403545\n",
      "Gradient Descent(329/999): loss=[[9.22120867]], w0=-2.6885967527837495, w1=1.0717589046732063\n",
      "Gradient Descent(330/999): loss=[[9.21928505]], w0=-2.6929488829041954, w1=1.0721961233505706\n",
      "Gradient Descent(331/999): loss=[[9.21737528]], w0=-2.6972853227615032, w1=1.072631765770813\n",
      "Gradient Descent(332/999): loss=[[9.21547925]], w0=-2.7016061289220787, w1=1.073065837616643\n",
      "Gradient Descent(333/999): loss=[[9.21359688]], w0=-2.705911357748395, w1=1.0734983445502821\n",
      "Gradient Descent(334/999): loss=[[9.21172804]], w0=-2.710201065399727, w1=1.0739292922135386\n",
      "Gradient Descent(335/999): loss=[[9.20987266]], w0=-2.714475307832885, w1=1.0743586862278813\n",
      "Gradient Descent(336/999): loss=[[9.20803064]], w0=-2.718734140802945, w1=1.074786532194512\n",
      "Gradient Descent(337/999): loss=[[9.20620187]], w0=-2.7229776198639737, w1=1.0752128356944395\n",
      "Gradient Descent(338/999): loss=[[9.20438626]], w0=-2.7272058003697563, w1=1.0756376022885519\n",
      "Gradient Descent(339/999): loss=[[9.20258372]], w0=-2.731418737474516, w1=1.0760608375176888\n",
      "Gradient Descent(340/999): loss=[[9.20079416]], w0=-2.7356164861336345, w1=1.0764825469027153\n",
      "Gradient Descent(341/999): loss=[[9.19901747]], w0=-2.7397991011043694, w1=1.0769027359445908\n",
      "Gradient Descent(342/999): loss=[[9.19725358]], w0=-2.743966636946568, w1=1.0773214101244446\n",
      "Gradient Descent(343/999): loss=[[9.19550238]], w0=-2.7481191480233775, w1=1.0777385749036439\n",
      "Gradient Descent(344/999): loss=[[9.19376378]], w0=-2.7522566885019573, w1=1.0781542357238678\n",
      "Gradient Descent(345/999): loss=[[9.19203769]], w0=-2.7563793123541824, w1=1.0785683980071765\n",
      "Gradient Descent(346/999): loss=[[9.19032403]], w0=-2.7604870733573503, w1=1.0789810671560827\n",
      "Gradient Descent(347/999): loss=[[9.1886227]], w0=-2.7645800250948795, w1=1.079392248553622\n",
      "Gradient Descent(348/999): loss=[[9.18693362]], w0=-2.768658220957011, w1=1.0798019475634235\n",
      "Gradient Descent(349/999): loss=[[9.1852567]], w0=-2.7727217141415035, w1=1.0802101695297788\n",
      "Gradient Descent(350/999): loss=[[9.18359184]], w0=-2.7767705576543276, w1=1.0806169197777127\n",
      "Gradient Descent(351/999): loss=[[9.18193897]], w0=-2.780804804310357, w1=1.0810222036130517\n",
      "Gradient Descent(352/999): loss=[[9.18029799]], w0=-2.7848245067340573, w1=1.0814260263224953\n",
      "Gradient Descent(353/999): loss=[[9.17866883]], w0=-2.788829717360174, w1=1.0818283931736812\n",
      "Gradient Descent(354/999): loss=[[9.17705139]], w0=-2.792820488434415, w1=1.0822293094152582\n",
      "Gradient Descent(355/999): loss=[[9.17544559]], w0=-2.796796872014131, w1=1.0826287802769516\n",
      "Gradient Descent(356/999): loss=[[9.17385134]], w0=-2.800758919968998, w1=1.083026810969633\n",
      "Gradient Descent(357/999): loss=[[9.17226858]], w0=-2.8047066839816903, w1=1.0834234066853876\n",
      "Gradient Descent(358/999): loss=[[9.1706972]], w0=-2.8086402155485573, w1=1.0838185725975824\n",
      "Gradient Descent(359/999): loss=[[9.16913714]], w0=-2.8125595659802936, w1=1.084212313860933\n",
      "Gradient Descent(360/999): loss=[[9.1675883]], w0=-2.8164647864026087, w1=1.0846046356115715\n",
      "Gradient Descent(361/999): loss=[[9.16605061]], w0=-2.8203559277568946, w1=1.0849955429671136\n",
      "Gradient Descent(362/999): loss=[[9.16452399]], w0=-2.8242330408008898, w1=1.0853850410267243\n",
      "Gradient Descent(363/999): loss=[[9.16300835]], w0=-2.8280961761093413, w1=1.0857731348711859\n",
      "Gradient Descent(364/999): loss=[[9.16150363]], w0=-2.8319453840746647, w1=1.0861598295629624\n",
      "Gradient Descent(365/999): loss=[[9.16000973]], w0=-2.835780714907601, w1=1.0865451301462685\n",
      "Gradient Descent(366/999): loss=[[9.15852659]], w0=-2.8396022186378715, w1=1.0869290416471313\n",
      "Gradient Descent(367/999): loss=[[9.15705412]], w0=-2.8434099451148316, w1=1.08731156907346\n",
      "Gradient Descent(368/999): loss=[[9.15559225]], w0=-2.8472039440081196, w1=1.087692717415108\n",
      "Gradient Descent(369/999): loss=[[9.1541409]], w0=-2.8509842648083055, w1=1.0880724916439404\n",
      "Gradient Descent(370/999): loss=[[9.1527]], w0=-2.854750956827536, w1=1.0884508967138966\n",
      "Gradient Descent(371/999): loss=[[9.15126946]], w0=-2.8585040692001784, w1=1.0888279375610574\n",
      "Gradient Descent(372/999): loss=[[9.14984923]], w0=-2.862243650883461, w1=1.0892036191037067\n",
      "Gradient Descent(373/999): loss=[[9.14843921]], w0=-2.8659697506581128, w1=1.0895779462423985\n",
      "Gradient Descent(374/999): loss=[[9.14703935]], w0=-2.869682417128997, w1=1.0899509238600182\n",
      "Gradient Descent(375/999): loss=[[9.14564956]], w0=-2.873381698725749, w1=1.0903225568218478\n",
      "Gradient Descent(376/999): loss=[[9.14426977]], w0=-2.8770676437034046, w1=1.0906928499756294\n",
      "Gradient Descent(377/999): loss=[[9.14289991]], w0=-2.8807403001430316, w1=1.0910618081516277\n",
      "Gradient Descent(378/999): loss=[[9.14153991]], w0=-2.8843997159523562, w1=1.0914294361626935\n",
      "Gradient Descent(379/999): loss=[[9.1401897]], w0=-2.888045938866388, w1=1.091795738804326\n",
      "Gradient Descent(380/999): loss=[[9.13884921]], w0=-2.8916790164480433, w1=1.0921607208547364\n",
      "Gradient Descent(381/999): loss=[[9.13751837]], w0=-2.8952989960887643, w1=1.0925243870749088\n",
      "Gradient Descent(382/999): loss=[[9.13619711]], w0=-2.898905925009138, w1=1.0928867422086637\n",
      "Gradient Descent(383/999): loss=[[9.13488535]], w0=-2.9024998502595123, w1=1.0932477909827187\n",
      "Gradient Descent(384/999): loss=[[9.13358304]], w0=-2.90608081872061, w1=1.093607538106751\n",
      "Gradient Descent(385/999): loss=[[9.1322901]], w0=-2.9096488771041393, w1=1.093965988273458\n",
      "Gradient Descent(386/999): loss=[[9.13100646]], w0=-2.913204071953404, w1=1.0943231461586198\n",
      "Gradient Descent(387/999): loss=[[9.12973207]], w0=-2.9167464496439104, w1=1.094679016421159\n",
      "Gradient Descent(388/999): loss=[[9.12846684]], w0=-2.920276056383972, w1=1.0950336037032018\n",
      "Gradient Descent(389/999): loss=[[9.12721073]], w0=-2.9237929382153123, w1=1.095386912630139\n",
      "Gradient Descent(390/999): loss=[[9.12596365]], w0=-2.9272971410136663, w1=1.0957389478106851\n",
      "Gradient Descent(391/999): loss=[[9.12472555]], w0=-2.9307887104893777, w1=1.0960897138369408\n",
      "Gradient Descent(392/999): loss=[[9.12349636]], w0=-2.9342676921879955, w1=1.09643921528445\n",
      "Gradient Descent(393/999): loss=[[9.12227602]], w0=-2.9377341314908687, w1=1.0967874567122615\n",
      "Gradient Descent(394/999): loss=[[9.12106446]], w0=-2.941188073615738, w1=1.0971344426629879\n",
      "Gradient Descent(395/999): loss=[[9.11986162]], w0=-2.9446295636173243, w1=1.0974801776628644\n",
      "Gradient Descent(396/999): loss=[[9.11866744]], w0=-2.948058646387919, w1=1.0978246662218092\n",
      "Gradient Descent(397/999): loss=[[9.11748185]], w0=-2.951475366657967, w1=1.09816791283348\n",
      "Gradient Descent(398/999): loss=[[9.11630479]], w0=-2.9548797689966526, w1=1.0985099219753351\n",
      "Gradient Descent(399/999): loss=[[9.11513621]], w0=-2.9582718978124785, w1=1.0988506981086905\n",
      "Gradient Descent(400/999): loss=[[9.11397604]], w0=-2.9616517973538468, w1=1.0991902456787779\n",
      "Gradient Descent(401/999): loss=[[9.11282422]], w0=-2.965019511709636, w1=1.0995285691148031\n",
      "Gradient Descent(402/999): loss=[[9.11168069]], w0=-2.9683750848097747, w1=1.0998656728300045\n",
      "Gradient Descent(403/999): loss=[[9.11054539]], w0=-2.971718560425817, w1=1.100201561221709\n",
      "Gradient Descent(404/999): loss=[[9.10941826]], w0=-2.9750499821715106, w1=1.1005362386713908\n",
      "Gradient Descent(405/999): loss=[[9.10829924]], w0=-2.978369393503369, w1=1.1008697095447284\n",
      "Gradient Descent(406/999): loss=[[9.10718828]], w0=-2.9816768377212353, w1=1.10120197819166\n",
      "Gradient Descent(407/999): loss=[[9.10608531]], w0=-2.9849723579688487, w1=1.1015330489464428\n",
      "Gradient Descent(408/999): loss=[[9.10499028]], w0=-2.9882559972344076, w1=1.101862926127707\n",
      "Gradient Descent(409/999): loss=[[9.10390313]], w0=-2.991527798351129, w1=1.1021916140385148\n",
      "Gradient Descent(410/999): loss=[[9.10282381]], w0=-2.994787803997808, w1=1.102519116966413\n",
      "Gradient Descent(411/999): loss=[[9.10175226]], w0=-2.998036056699375, w1=1.102845439183493\n",
      "Gradient Descent(412/999): loss=[[9.10068841]], w0=-3.0012725988274487, w1=1.103170584946443\n",
      "Gradient Descent(413/999): loss=[[9.09963223]], w0=-3.0044974726008915, w1=1.1034945584966056\n",
      "Gradient Descent(414/999): loss=[[9.09858365]], w0=-3.007710720086358, w1=1.1038173640600333\n",
      "Gradient Descent(415/999): loss=[[9.09754261]], w0=-3.010912383198844, w1=1.1041390058475409\n",
      "Gradient Descent(416/999): loss=[[9.09650907]], w0=-3.0141025037022344, w1=1.1044594880547647\n",
      "Gradient Descent(417/999): loss=[[9.09548296]], w0=-3.0172811232098473, w1=1.104778814862213\n",
      "Gradient Descent(418/999): loss=[[9.09446424]], w0=-3.0204482831849764, w1=1.1050969904353243\n",
      "Gradient Descent(419/999): loss=[[9.09345285]], w0=-3.023604024941432, w1=1.105414018924518\n",
      "Gradient Descent(420/999): loss=[[9.09244875]], w0=-3.0267483896440814, w1=1.1057299044652527\n",
      "Gradient Descent(421/999): loss=[[9.09145187]], w0=-3.029881418309383, w1=1.1060446511780753\n",
      "Gradient Descent(422/999): loss=[[9.09046216]], w0=-3.033003151805925, w1=1.1063582631686797\n",
      "Gradient Descent(423/999): loss=[[9.08947958]], w0=-3.036113630854955, w1=1.1066707445279562\n",
      "Gradient Descent(424/999): loss=[[9.08850407]], w0=-3.039212896030912, w1=1.106982099332048\n",
      "Gradient Descent(425/999): loss=[[9.08753558]], w0=-3.042300987761959, w1=1.1072923316424022\n",
      "Gradient Descent(426/999): loss=[[9.08657406]], w0=-3.0453779463305053, w1=1.1076014455058236\n",
      "Gradient Descent(427/999): loss=[[9.08561946]], w0=-3.0484438118737356, w1=1.1079094449545281\n",
      "Gradient Descent(428/999): loss=[[9.08467173]], w0=-3.0514986243841324, w1=1.1082163340061943\n",
      "Gradient Descent(429/999): loss=[[9.08373082]], w0=-3.054542423709997, w1=1.1085221166640162\n",
      "Gradient Descent(430/999): loss=[[9.08279669]], w0=-3.05757524955597, w1=1.1088267969167558\n",
      "Gradient Descent(431/999): loss=[[9.08186928]], w0=-3.0605971414835498, w1=1.1091303787387943\n",
      "Gradient Descent(432/999): loss=[[9.08094854]], w0=-3.063608138911607, w1=1.1094328660901853\n",
      "Gradient Descent(433/999): loss=[[9.08003443]], w0=-3.066608281116901, w1=1.1097342629167048\n",
      "Gradient Descent(434/999): loss=[[9.0791269]], w0=-3.0695976072345896, w1=1.1100345731499046\n",
      "Gradient Descent(435/999): loss=[[9.0782259]], w0=-3.0725761562587417, w1=1.1103338007071613\n",
      "Gradient Descent(436/999): loss=[[9.07733139]], w0=-3.075543967042845, w1=1.1106319494917294\n",
      "Gradient Descent(437/999): loss=[[9.07644331]], w0=-3.0785010783003126, w1=1.1109290233927909\n",
      "Gradient Descent(438/999): loss=[[9.07556163]], w0=-3.0814475286049885, w1=1.1112250262855075\n",
      "Gradient Descent(439/999): loss=[[9.07468629]], w0=-3.0843833563916507, w1=1.1115199620310685\n",
      "Gradient Descent(440/999): loss=[[9.07381725]], w0=-3.087308599956512, w1=1.1118138344767452\n",
      "Gradient Descent(441/999): loss=[[9.07295447]], w0=-3.090223297457721, w1=1.112106647455936\n",
      "Gradient Descent(442/999): loss=[[9.0720979]], w0=-3.093127486915858, w1=1.1123984047882218\n",
      "Gradient Descent(443/999): loss=[[9.07124749]], w0=-3.0960212062144317, w1=1.1126891102794116\n",
      "Gradient Descent(444/999): loss=[[9.0704032]], w0=-3.098904493100374, w1=1.1129787677215939\n",
      "Gradient Descent(445/999): loss=[[9.06956499]], w0=-3.101777385184532, w1=1.1132673808931868\n",
      "Gradient Descent(446/999): loss=[[9.06873281]], w0=-3.104639919942158, w1=1.113554953558985\n",
      "Gradient Descent(447/999): loss=[[9.06790662]], w0=-3.1074921347133992, w1=1.1138414894702122\n",
      "Gradient Descent(448/999): loss=[[9.06708638]], w0=-3.110334066703784, w1=1.114126992364567\n",
      "Gradient Descent(449/999): loss=[[9.06627204]], w0=-3.1131657529847088, w1=1.1144114659662727\n",
      "Gradient Descent(450/999): loss=[[9.06546357]], w0=-3.1159872304939187, w1=1.1146949139861273\n",
      "Gradient Descent(451/999): loss=[[9.06466091]], w0=-3.1187985360359924, w1=1.1149773401215497\n",
      "Gradient Descent(452/999): loss=[[9.06386403]], w0=-3.1215997062828214, w1=1.1152587480566287\n",
      "Gradient Descent(453/999): loss=[[9.06307288]], w0=-3.1243907777740865, w1=1.1155391414621723\n",
      "Gradient Descent(454/999): loss=[[9.06228743]], w0=-3.127171786917738, w1=1.1158185239957532\n",
      "Gradient Descent(455/999): loss=[[9.06150763]], w0=-3.1299427699904663, w1=1.1160968993017595\n",
      "Gradient Descent(456/999): loss=[[9.06073344]], w0=-3.132703763138179, w1=1.1163742710114384\n",
      "Gradient Descent(457/999): loss=[[9.05996483]], w0=-3.1354548023764703, w1=1.1166506427429483\n",
      "Gradient Descent(458/999): loss=[[9.05920175]], w0=-3.1381959235910912, w1=1.1169260181014011\n",
      "Gradient Descent(459/999): loss=[[9.05844416]], w0=-3.140927162538418, w1=1.1172004006789131\n",
      "Gradient Descent(460/999): loss=[[9.05769202]], w0=-3.1436485548459174, w1=1.11747379405465\n",
      "Gradient Descent(461/999): loss=[[9.0569453]], w0=-3.146360136012614, w1=1.1177462017948734\n",
      "Gradient Descent(462/999): loss=[[9.05620395]], w0=-3.14906194140955, w1=1.1180176274529883\n",
      "Gradient Descent(463/999): loss=[[9.05546794]], w0=-3.151754006280249, w1=1.118288074569589\n",
      "Gradient Descent(464/999): loss=[[9.05473722]], w0=-3.154436365741175, w1=1.1185575466725053\n",
      "Gradient Descent(465/999): loss=[[9.05401176]], w0=-3.15710905478219, w1=1.118826047276848\n",
      "Gradient Descent(466/999): loss=[[9.05329153]], w0=-3.1597721082670107, w1=1.119093579885056\n",
      "Gradient Descent(467/999): loss=[[9.05257648]], w0=-3.1624255609336642, w1=1.1193601479869408\n",
      "Gradient Descent(468/999): loss=[[9.05186657]], w0=-3.16506944739494, w1=1.1196257550597326\n",
      "Gradient Descent(469/999): loss=[[9.05116178]], w0=-3.1677038021388415, w1=1.119890404568125\n",
      "Gradient Descent(470/999): loss=[[9.05046205]], w0=-3.1703286595290368, w1=1.120154099964322\n",
      "Gradient Descent(471/999): loss=[[9.04976737]], w0=-3.1729440538053058, w1=1.1204168446880802\n",
      "Gradient Descent(472/999): loss=[[9.04907768]], w0=-3.1755500190839876, w1=1.120678642166756\n",
      "Gradient Descent(473/999): loss=[[9.04839296]], w0=-3.178146589358426, w1=1.12093949581535\n",
      "Gradient Descent(474/999): loss=[[9.04771316]], w0=-3.1807337984994115, w1=1.1211994090365498\n",
      "Gradient Descent(475/999): loss=[[9.04703826]], w0=-3.183311680255624, w1=1.121458385220777\n",
      "Gradient Descent(476/999): loss=[[9.04636822]], w0=-3.1858802682540737, w1=1.1217164277462288\n",
      "Gradient Descent(477/999): loss=[[9.04570299]], w0=-3.188439596000538, w1=1.1219735399789248\n",
      "Gradient Descent(478/999): loss=[[9.04504256]], w0=-3.19098969688, w1=1.1222297252727482\n",
      "Gradient Descent(479/999): loss=[[9.04438688]], w0=-3.1935306041570835, w1=1.122484986969491\n",
      "Gradient Descent(480/999): loss=[[9.04373592]], w0=-3.196062350976487, w1=1.1227393283988985\n",
      "Gradient Descent(481/999): loss=[[9.04308964]], w0=-3.198584970363416, w1=1.12299275287871\n",
      "Gradient Descent(482/999): loss=[[9.04244802]], w0=-3.201098495224014, w1=1.123245263714705\n",
      "Gradient Descent(483/999): loss=[[9.04181101]], w0=-3.203602958345791, w1=1.1234968642007443\n",
      "Gradient Descent(484/999): loss=[[9.04117859]], w0=-3.2060983923980517, w1=1.1237475576188143\n",
      "Gradient Descent(485/999): loss=[[9.04055072]], w0=-3.208584829932323, w1=1.1239973472390683\n",
      "Gradient Descent(486/999): loss=[[9.03992737]], w0=-3.211062303382776, w1=1.1242462363198713\n",
      "Gradient Descent(487/999): loss=[[9.0393085]], w0=-3.2135308450666504, w1=1.1244942281078398\n",
      "Gradient Descent(488/999): loss=[[9.03869409]], w0=-3.2159904871846767, w1=1.1247413258378873\n",
      "Gradient Descent(489/999): loss=[[9.03808411]], w0=-3.218441261821495, w1=1.1249875327332635\n",
      "Gradient Descent(490/999): loss=[[9.03747851]], w0=-3.220883200946075, w1=1.1252328520055979\n",
      "Gradient Descent(491/999): loss=[[9.03687727]], w0=-3.223316336412131, w1=1.125477286854942\n",
      "Gradient Descent(492/999): loss=[[9.03628036]], w0=-3.2257406999585396, w1=1.1257208404698098\n",
      "Gradient Descent(493/999): loss=[[9.03568774]], w0=-3.228156323209752, w1=1.1259635160272214\n",
      "Gradient Descent(494/999): loss=[[9.03509939]], w0=-3.2305632376762077, w1=1.126205316692741\n",
      "Gradient Descent(495/999): loss=[[9.03451528]], w0=-3.2329614747547444, w1=1.1264462456205224\n",
      "Gradient Descent(496/999): loss=[[9.03393536]], w0=-3.2353510657290085, w1=1.1266863059533467\n",
      "Gradient Descent(497/999): loss=[[9.03335963]], w0=-3.237732041769863, w1=1.1269255008226655\n",
      "Gradient Descent(498/999): loss=[[9.03278803]], w0=-3.2401044339357936, w1=1.1271638333486405\n",
      "Gradient Descent(499/999): loss=[[9.03222055]], w0=-3.2424682731733148, w1=1.1274013066401845\n",
      "Gradient Descent(500/999): loss=[[9.03165716]], w0=-3.244823590317372, w1=1.1276379237950027\n",
      "Gradient Descent(501/999): loss=[[9.03109782]], w0=-3.247170416091746, w1=1.1278736878996316\n",
      "Gradient Descent(502/999): loss=[[9.0305425]], w0=-3.2495087811094514, w1=1.1281086020294813\n",
      "Gradient Descent(503/999): loss=[[9.02999118]], w0=-3.251838715873138, w1=1.1283426692488727\n",
      "Gradient Descent(504/999): loss=[[9.02944383]], w0=-3.2541602507754863, w1=1.1285758926110816\n",
      "Gradient Descent(505/999): loss=[[9.02890042]], w0=-3.256473416099607, w1=1.1288082751583737\n",
      "Gradient Descent(506/999): loss=[[9.02836092]], w0=-3.258778242019433, w1=1.1290398199220488\n",
      "Gradient Descent(507/999): loss=[[9.02782531]], w0=-3.261074758600115, w1=1.1292705299224768\n",
      "Gradient Descent(508/999): loss=[[9.02729355]], w0=-3.2633629957984134, w1=1.1295004081691393\n",
      "Gradient Descent(509/999): loss=[[9.02676561]], w0=-3.265642983463088, w1=1.1297294576606682\n",
      "Gradient Descent(510/999): loss=[[9.02624148]], w0=-3.267914751335289, w1=1.1299576813848846\n",
      "Gradient Descent(511/999): loss=[[9.02572112]], w0=-3.270178329048943, w1=1.1301850823188377\n",
      "Gradient Descent(512/999): loss=[[9.0252045]], w0=-3.2724337461311412, w1=1.1304116634288444\n",
      "Gradient Descent(513/999): loss=[[9.0246916]], w0=-3.2746810320025244, w1=1.1306374276705267\n",
      "Gradient Descent(514/999): loss=[[9.0241824]], w0=-3.276920215977665, w1=1.1308623779888518\n",
      "Gradient Descent(515/999): loss=[[9.02367685]], w0=-3.2791513272654527, w1=1.131086517318169\n",
      "Gradient Descent(516/999): loss=[[9.02317495]], w0=-3.281374394969472, w1=1.1313098485822497\n",
      "Gradient Descent(517/999): loss=[[9.02267666]], w0=-3.2835894480883834, w1=1.1315323746943236\n",
      "Gradient Descent(518/999): loss=[[9.02218196]], w0=-3.285796515516303, w1=1.1317540985571177\n",
      "Gradient Descent(519/999): loss=[[9.02169081]], w0=-3.2879956260431764, w1=1.1319750230628949\n",
      "Gradient Descent(520/999): loss=[[9.0212032]], w0=-3.2901868083551573, w1=1.1321951510934902\n",
      "Gradient Descent(521/999): loss=[[9.0207191]], w0=-3.2923700910349796, w1=1.1324144855203493\n",
      "Gradient Descent(522/999): loss=[[9.02023849]], w0=-3.294545502562331, w1=1.1326330292045657\n",
      "Gradient Descent(523/999): loss=[[9.01976133]], w0=-3.296713071314225, w1=1.1328507849969183\n",
      "Gradient Descent(524/999): loss=[[9.01928761]], w0=-3.29887282556537, w1=1.133067755737908\n",
      "Gradient Descent(525/999): loss=[[9.0188173]], w0=-3.3010247934885384, w1=1.1332839442577958\n",
      "Gradient Descent(526/999): loss=[[9.01835037]], w0=-3.303169003154935, w1=1.1334993533766384\n",
      "Gradient Descent(527/999): loss=[[9.0178868]], w0=-3.3053054825345622, w1=1.133713985904326\n",
      "Gradient Descent(528/999): loss=[[9.01742657]], w0=-3.3074342594965853, w1=1.1339278446406187\n",
      "Gradient Descent(529/999): loss=[[9.01696966]], w0=-3.309555361809696, w1=1.134140932375182\n",
      "Gradient Descent(530/999): loss=[[9.01651603]], w0=-3.3116688171424746, w1=1.1343532518876265\n",
      "Gradient Descent(531/999): loss=[[9.01606566]], w0=-3.31377465306375, w1=1.1345648059475384\n",
      "Gradient Descent(532/999): loss=[[9.01561854]], w0=-3.315872897042962, w1=1.1347755973145217\n",
      "Gradient Descent(533/999): loss=[[9.01517464]], w0=-3.3179635764505155, w1=1.1349856287382307\n",
      "Gradient Descent(534/999): loss=[[9.01473393]], w0=-3.3200467185581415, w1=1.1351949029584059\n",
      "Gradient Descent(535/999): loss=[[9.01429639]], w0=-3.3221223505392508, w1=1.135403422704912\n",
      "Gradient Descent(536/999): loss=[[9.013862]], w0=-3.324190499469289, w1=1.13561119069777\n",
      "Gradient Descent(537/999): loss=[[9.01343074]], w0=-3.3262511923260885, w1=1.1358182096471972\n",
      "Gradient Descent(538/999): loss=[[9.01300258]], w0=-3.3283044559902226, w1=1.1360244822536374\n",
      "Gradient Descent(539/999): loss=[[9.0125775]], w0=-3.330350317245355, w1=1.1362300112078014\n",
      "Gradient Descent(540/999): loss=[[9.01215549]], w0=-3.3323888027785884, w1=1.136434799190697\n",
      "Gradient Descent(541/999): loss=[[9.01173651]], w0=-3.3344199391808136, w1=1.1366388488736687\n",
      "Gradient Descent(542/999): loss=[[9.01132054]], w0=-3.3364437529470568, w1=1.1368421629184282\n",
      "Gradient Descent(543/999): loss=[[9.01090757]], w0=-3.3384602704768236, w1=1.137044743977093\n",
      "Gradient Descent(544/999): loss=[[9.01049758]], w0=-3.340469518074445, w1=1.137246594692218\n",
      "Gradient Descent(545/999): loss=[[9.01009053]], w0=-3.3424715219494194, w1=1.1374477176968318\n",
      "Gradient Descent(546/999): loss=[[9.00968641]], w0=-3.3444663082167554, w1=1.1376481156144702\n",
      "Gradient Descent(547/999): loss=[[9.0092852]], w0=-3.3464539028973115, w1=1.1378477910592109\n",
      "Gradient Descent(548/999): loss=[[9.00888688]], w0=-3.348434331918136, w1=1.1380467466357067\n",
      "Gradient Descent(549/999): loss=[[9.00849143]], w0=-3.3504076211128067, w1=1.138244984939221\n",
      "Gradient Descent(550/999): loss=[[9.00809882]], w0=-3.3523737962217637, w1=1.1384425085556604\n",
      "Gradient Descent(551/999): loss=[[9.00770904]], w0=-3.35433288289265, w1=1.1386393200616085\n",
      "Gradient Descent(552/999): loss=[[9.00732206]], w0=-3.3562849066806435, w1=1.13883542202436\n",
      "Gradient Descent(553/999): loss=[[9.00693787]], w0=-3.35822989304879, w1=1.1390308170019547\n",
      "Gradient Descent(554/999): loss=[[9.00655644]], w0=-3.3601678673683377, w1=1.1392255075432085\n",
      "Gradient Descent(555/999): loss=[[9.00617776]], w0=-3.3620988549190645, w1=1.1394194961877508\n",
      "Gradient Descent(556/999): loss=[[9.00580181]], w0=-3.3640228808896113, w1=1.1396127854660527\n",
      "Gradient Descent(557/999): loss=[[9.00542856]], w0=-3.3659399703778092, w1=1.1398053778994637\n",
      "Gradient Descent(558/999): loss=[[9.00505799]], w0=-3.367850148391006, w1=1.1399972760002437\n",
      "Gradient Descent(559/999): loss=[[9.0046901]], w0=-3.3697534398463937, w1=1.1401884822715942\n",
      "Gradient Descent(560/999): loss=[[9.00432485]], w0=-3.3716498695713333, w1=1.1403789992076936\n",
      "Gradient Descent(561/999): loss=[[9.00396223]], w0=-3.3735394623036776, w1=1.1405688292937266\n",
      "Gradient Descent(562/999): loss=[[9.00360222]], w0=-3.3754222426920952, w1=1.1407579750059207\n",
      "Gradient Descent(563/999): loss=[[9.0032448]], w0=-3.377298235296392, w1=1.140946438811573\n",
      "Gradient Descent(564/999): loss=[[9.00288996]], w0=-3.37916746458783, w1=1.1411342231690893\n",
      "Gradient Descent(565/999): loss=[[9.00253767]], w0=-3.3810299549494482, w1=1.1413213305280085\n",
      "Gradient Descent(566/999): loss=[[9.00218791]], w0=-3.38288573067638, w1=1.1415077633290414\n",
      "Gradient Descent(567/999): loss=[[9.00184067]], w0=-3.384734815976171, w1=1.141693524004098\n",
      "Gradient Descent(568/999): loss=[[9.00149593]], w0=-3.3865772349690926, w1=1.1418786149763214\n",
      "Gradient Descent(569/999): loss=[[9.00115367]], w0=-3.3884130116884585, w1=1.1420630386601185\n",
      "Gradient Descent(570/999): loss=[[9.00081388]], w0=-3.3902421700809384, w1=1.142246797461192\n",
      "Gradient Descent(571/999): loss=[[9.00047653]], w0=-3.3920647340068686, w1=1.142429893776572\n",
      "Gradient Descent(572/999): loss=[[9.00014161]], w0=-3.393880727240565, w1=1.1426123299946456\n",
      "Gradient Descent(573/999): loss=[[8.99980909]], w0=-3.3956901734706317, w1=1.1427941084951907\n",
      "Gradient Descent(574/999): loss=[[8.99947898]], w0=-3.3974930963002725, w1=1.1429752316494048\n",
      "Gradient Descent(575/999): loss=[[8.99915123]], w0=-3.3992895192475956, w1=1.143155701819937\n",
      "Gradient Descent(576/999): loss=[[8.99882585]], w0=-3.4010794657459225, w1=1.1433355213609189\n",
      "Gradient Descent(577/999): loss=[[8.99850281]], w0=-3.402862959144093, w1=1.143514692617994\n",
      "Gradient Descent(578/999): loss=[[8.99818209]], w0=-3.4046400227067695, w1=1.143693217928351\n",
      "Gradient Descent(579/999): loss=[[8.99786369]], w0=-3.4064106796147415, w1=1.1438710996207506\n",
      "Gradient Descent(580/999): loss=[[8.99754757]], w0=-3.408174952965227, w1=1.1440483400155599\n",
      "Gradient Descent(581/999): loss=[[8.99723373]], w0=-3.409932865772174, w1=1.1442249414247787\n",
      "Gradient Descent(582/999): loss=[[8.99692215]], w0=-3.4116844409665608, w1=1.1444009061520732\n",
      "Gradient Descent(583/999): loss=[[8.99661281]], w0=-3.4134297013966957, w1=1.1445762364928032\n",
      "Gradient Descent(584/999): loss=[[8.9963057]], w0=-3.4151686698285135, w1=1.1447509347340543\n",
      "Gradient Descent(585/999): loss=[[8.99600079]], w0=-3.416901368945874, w1=1.144925003154666\n",
      "Gradient Descent(586/999): loss=[[8.99569809]], w0=-3.4186278213508574, w1=1.1450984440252625\n",
      "Gradient Descent(587/999): loss=[[8.99539756]], w0=-3.4203480495640592, w1=1.1452712596082817\n",
      "Gradient Descent(588/999): loss=[[8.99509919]], w0=-3.422062076024883, w1=1.1454434521580055\n",
      "Gradient Descent(589/999): loss=[[8.99480297]], w0=-3.4237699230918355, w1=1.1456150239205878\n",
      "Gradient Descent(590/999): loss=[[8.99450889]], w0=-3.425471613042815, w1=1.145785977134086\n",
      "Gradient Descent(591/999): loss=[[8.99421692]], w0=-3.4271671680754054, w1=1.1459563140284872\n",
      "Gradient Descent(592/999): loss=[[8.99392705]], w0=-3.4288566103071627, w1=1.1461260368257404\n",
      "Gradient Descent(593/999): loss=[[8.99363926]], w0=-3.430539961775905, w1=1.1462951477397836\n",
      "Gradient Descent(594/999): loss=[[8.99335355]], w0=-3.4322172444400008, w1=1.1464636489765725\n",
      "Gradient Descent(595/999): loss=[[8.9930699]], w0=-3.433888480178654, w1=1.1466315427341107\n",
      "Gradient Descent(596/999): loss=[[8.99278828]], w0=-3.435553690792189, w1=1.1467988312024777\n",
      "Gradient Descent(597/999): loss=[[8.9925087]], w0=-3.4372128980023366, w1=1.1469655165638566\n",
      "Gradient Descent(598/999): loss=[[8.99223112]], w0=-3.438866123452517, w1=1.1471316009925634\n",
      "Gradient Descent(599/999): loss=[[8.99195555]], w0=-3.4405133887081214, w1=1.1472970866550762\n",
      "Gradient Descent(600/999): loss=[[8.99168196]], w0=-3.442154715256793, w1=1.1474619757100615\n",
      "Gradient Descent(601/999): loss=[[8.99141033]], w0=-3.443790124508708, w1=1.1476262703084035\n",
      "Gradient Descent(602/999): loss=[[8.99114066]], w0=-3.4454196377968564, w1=1.147789972593233\n",
      "Gradient Descent(603/999): loss=[[8.99087294]], w0=-3.447043276377317, w1=1.147953084699953\n",
      "Gradient Descent(604/999): loss=[[8.99060713]], w0=-3.448661061429536, w1=1.1481156087562696\n",
      "Gradient Descent(605/999): loss=[[8.99034325]], w0=-3.450273014056606, w1=1.148277546882216\n",
      "Gradient Descent(606/999): loss=[[8.99008126]], w0=-3.451879155285536, w1=1.1484389011901845\n",
      "Gradient Descent(607/999): loss=[[8.98982116]], w0=-3.4534795060675307, w1=1.1485996737849502\n",
      "Gradient Descent(608/999): loss=[[8.98956293]], w0=-3.455074087278261, w1=1.1487598667637013\n",
      "Gradient Descent(609/999): loss=[[8.98930655]], w0=-3.456662919718137, w1=1.1489194822160638\n",
      "Gradient Descent(610/999): loss=[[8.98905203]], w0=-3.458246024112579, w1=1.1490785222241315\n",
      "Gradient Descent(611/999): loss=[[8.98879933]], w0=-3.459823421112289, w1=1.1492369888624914\n",
      "Gradient Descent(612/999): loss=[[8.98854846]], w0=-3.4613951312935187, w1=1.1493948841982509\n",
      "Gradient Descent(613/999): loss=[[8.98829938]], w0=-3.4629611751583385, w1=1.1495522102910656\n",
      "Gradient Descent(614/999): loss=[[8.98805211]], w0=-3.4645215731349044, w1=1.1497089691931652\n",
      "Gradient Descent(615/999): loss=[[8.98780661]], w0=-3.466076345577726, w1=1.1498651629493812\n",
      "Gradient Descent(616/999): loss=[[8.98756288]], w0=-3.467625512767931, w1=1.1500207935971725\n",
      "Gradient Descent(617/999): loss=[[8.9873209]], w0=-3.469169094913529, w1=1.1501758631666532\n",
      "Gradient Descent(618/999): loss=[[8.98708066]], w0=-3.4707071121496758, w1=1.1503303736806179\n",
      "Gradient Descent(619/999): loss=[[8.98684216]], w0=-3.4722395845389364, w1=1.150484327154569\n",
      "Gradient Descent(620/999): loss=[[8.98660537]], w0=-3.4737665320715467, w1=1.1506377255967424\n",
      "Gradient Descent(621/999): loss=[[8.98637028]], w0=-3.4752879746656737, w1=1.1507905710081343\n",
      "Gradient Descent(622/999): loss=[[8.98613689]], w0=-3.476803932167676, w1=1.150942865382526\n",
      "Gradient Descent(623/999): loss=[[8.98590518]], w0=-3.4783144243523614, w1=1.1510946107065123\n",
      "Gradient Descent(624/999): loss=[[8.98567513]], w0=-3.479819470923246, w1=1.1512458089595248\n",
      "Gradient Descent(625/999): loss=[[8.98544674]], w0=-3.481319091512812, w1=1.1513964621138588\n",
      "Gradient Descent(626/999): loss=[[8.98521999]], w0=-3.4828133056827615, w1=1.1515465721347002\n",
      "Gradient Descent(627/999): loss=[[8.98499488]], w0=-3.484302132924273, w1=1.151696140980149\n",
      "Gradient Descent(628/999): loss=[[8.98477139]], w0=-3.485785592658256, w1=1.151845170601246\n",
      "Gradient Descent(629/999): loss=[[8.9845495]], w0=-3.487263704235604, w1=1.1519936629419985\n",
      "Gradient Descent(630/999): loss=[[8.98432921]], w0=-3.4887364869374466, w1=1.1521416199394052\n",
      "Gradient Descent(631/999): loss=[[8.98411051]], w0=-3.490203959975401, w1=1.1522890435234814\n",
      "Gradient Descent(632/999): loss=[[8.98389338]], w0=-3.491666142491823, w1=1.1524359356172844\n",
      "Gradient Descent(633/999): loss=[[8.98367781]], w0=-3.493123053560057, w1=1.1525822981369385\n",
      "Gradient Descent(634/999): loss=[[8.9834638]], w0=-3.4945747121846837, w1=1.1527281329916597\n",
      "Gradient Descent(635/999): loss=[[8.98325132]], w0=-3.4960211373017693, w1=1.1528734420837816\n",
      "Gradient Descent(636/999): loss=[[8.98304038]], w0=-3.497462347779111, w1=1.1530182273087792\n",
      "Gradient Descent(637/999): loss=[[8.98283095]], w0=-3.4988983624164844, w1=1.1531624905552933\n",
      "Gradient Descent(638/999): loss=[[8.98262303]], w0=-3.5003291999458885, w1=1.1533062337051572\n",
      "Gradient Descent(639/999): loss=[[8.98241661]], w0=-3.5017548790317896, w1=1.1534494586334179\n",
      "Gradient Descent(640/999): loss=[[8.98221167]], w0=-3.5031754182713652, w1=1.1535921672083649\n",
      "Gradient Descent(641/999): loss=[[8.98200821]], w0=-3.5045908361947467, w1=1.1537343612915496\n",
      "Gradient Descent(642/999): loss=[[8.98180621]], w0=-3.50600115126526, w1=1.1538760427378143\n",
      "Gradient Descent(643/999): loss=[[8.98160567]], w0=-3.507406381879667, w1=1.1540172133953128\n",
      "Gradient Descent(644/999): loss=[[8.98140657]], w0=-3.5088065463684073, w1=1.154157875105537\n",
      "Gradient Descent(645/999): loss=[[8.9812089]], w0=-3.5102016629958346, w1=1.1542980297033387\n",
      "Gradient Descent(646/999): loss=[[8.98101265]], w0=-3.511591749960456, w1=1.1544376790169562\n",
      "Gradient Descent(647/999): loss=[[8.98081782]], w0=-3.5129768253951705, w1=1.1545768248680348\n",
      "Gradient Descent(648/999): loss=[[8.98062439]], w0=-3.514356907367503, w1=1.1547154690716546\n",
      "Gradient Descent(649/999): loss=[[8.98043235]], w0=-3.515732013879843, w1=1.1548536134363494\n",
      "Gradient Descent(650/999): loss=[[8.9802417]], w0=-3.517102162869677, w1=1.154991259764136\n",
      "Gradient Descent(651/999): loss=[[8.98005241]], w0=-3.518467372209823, w1=1.1551284098505312\n",
      "Gradient Descent(652/999): loss=[[8.97986449]], w0=-3.519827659708666, w1=1.155265065484582\n",
      "Gradient Descent(653/999): loss=[[8.97967792]], w0=-3.521183043110387, w1=1.1554012284488824\n",
      "Gradient Descent(654/999): loss=[[8.9794927]], w0=-3.5225335400951954, w1=1.1555369005196021\n",
      "Gradient Descent(655/999): loss=[[8.9793088]], w0=-3.5238791682795605, w1=1.1556720834665062\n",
      "Gradient Descent(656/999): loss=[[8.97912624]], w0=-3.5252199452164414, w1=1.15580677905298\n",
      "Gradient Descent(657/999): loss=[[8.97894498]], w0=-3.526555888395515, w1=1.155940989036051\n",
      "Gradient Descent(658/999): loss=[[8.97876503]], w0=-3.527887015243404, w1=1.156074715166413\n",
      "Gradient Descent(659/999): loss=[[8.97858637]], w0=-3.529213343123906, w1=1.156207959188447\n",
      "Gradient Descent(660/999): loss=[[8.978409]], w0=-3.530534889338218, w1=1.1563407228402467\n",
      "Gradient Descent(661/999): loss=[[8.97823291]], w0=-3.5318516711251626, w1=1.1564730078536383\n",
      "Gradient Descent(662/999): loss=[[8.97805809]], w0=-3.533163705661414, w1=1.156604815954205\n",
      "Gradient Descent(663/999): loss=[[8.97788452]], w0=-3.5344710100617203, w1=1.1567361488613095\n",
      "Gradient Descent(664/999): loss=[[8.9777122]], w0=-3.535773601379128, w1=1.1568670082881145\n",
      "Gradient Descent(665/999): loss=[[8.97754112]], w0=-3.537071496605205, w1=1.156997395941608\n",
      "Gradient Descent(666/999): loss=[[8.97737128]], w0=-3.5383647126702598, w1=1.1571273135226228\n",
      "Gradient Descent(667/999): loss=[[8.97720265]], w0=-3.5396532664435645, w1=1.1572567627258599\n",
      "Gradient Descent(668/999): loss=[[8.97703524]], w0=-3.5409371747335747, w1=1.157385745239912\n",
      "Gradient Descent(669/999): loss=[[8.97686904]], w0=-3.5422164542881482, w1=1.1575142627472816\n",
      "Gradient Descent(670/999): loss=[[8.97670403]], w0=-3.5434911217947627, w1=1.1576423169244083\n",
      "Gradient Descent(671/999): loss=[[8.9765402]], w0=-3.5447611938807353, w1=1.1577699094416853\n",
      "Gradient Descent(672/999): loss=[[8.97637756]], w0=-3.546026687113438, w1=1.157897041963486\n",
      "Gradient Descent(673/999): loss=[[8.97621609]], w0=-3.5472876180005146, w1=1.1580237161481812\n",
      "Gradient Descent(674/999): loss=[[8.97605578]], w0=-3.548544002990095, w1=1.1581499336481647\n",
      "Gradient Descent(675/999): loss=[[8.97589662]], w0=-3.549795858471011, w1=1.1582756961098717\n",
      "Gradient Descent(676/999): loss=[[8.97573861]], w0=-3.5510432007730097, w1=1.1584010051738032\n",
      "Gradient Descent(677/999): loss=[[8.97558174]], w0=-3.552286046166966, w1=1.1585258624745443\n",
      "Gradient Descent(678/999): loss=[[8.975426]], w0=-3.5535244108650943, w1=1.1586502696407885\n",
      "Gradient Descent(679/999): loss=[[8.97527137]], w0=-3.5547583110211627, w1=1.1587742282953564\n",
      "Gradient Descent(680/999): loss=[[8.97511786]], w0=-3.5559877627307004, w1=1.1588977400552192\n",
      "Gradient Descent(681/999): loss=[[8.97496546]], w0=-3.5572127820312103, w1=1.1590208065315173\n",
      "Gradient Descent(682/999): loss=[[8.97481415]], w0=-3.5584333849023757, w1=1.1591434293295837\n",
      "Gradient Descent(683/999): loss=[[8.97466393]], w0=-3.559649587266271, w1=1.1592656100489633\n",
      "Gradient Descent(684/999): loss=[[8.97451479]], w0=-3.5608614049875684, w1=1.1593873502834344\n",
      "Gradient Descent(685/999): loss=[[8.97436673]], w0=-3.5620688538737446, w1=1.1595086516210298\n",
      "Gradient Descent(686/999): loss=[[8.97421973]], w0=-3.5632719496752876, w1=1.1596295156440564\n",
      "Gradient Descent(687/999): loss=[[8.97407379]], w0=-3.5644707080859015, w1=1.1597499439291177\n",
      "Gradient Descent(688/999): loss=[[8.97392889]], w0=-3.565665144742712, w1=1.1598699380471318\n",
      "Gradient Descent(689/999): loss=[[8.97378505]], w0=-3.56685527522647, w1=1.1599894995633546\n",
      "Gradient Descent(690/999): loss=[[8.97364223]], w0=-3.568041115061754, w1=1.1601086300373984\n",
      "Gradient Descent(691/999): loss=[[8.97350045]], w0=-3.5692226797171744, w1=1.1602273310232527\n",
      "Gradient Descent(692/999): loss=[[8.97335969]], w0=-3.570399984605574, w1=1.1603456040693048\n",
      "Gradient Descent(693/999): loss=[[8.97321993]], w0=-3.571573045084229, w1=1.1604634507183598\n",
      "Gradient Descent(694/999): loss=[[8.97308119]], w0=-3.57274187645505, w1=1.1605808725076605\n",
      "Gradient Descent(695/999): loss=[[8.97294344]], w0=-3.5739064939647815, w1=1.1606978709689078\n",
      "Gradient Descent(696/999): loss=[[8.97280669]], w0=-3.5750669128052, w1=1.1608144476282802\n",
      "Gradient Descent(697/999): loss=[[8.97267092]], w0=-3.576223148113313, w1=1.1609306040064549\n",
      "Gradient Descent(698/999): loss=[[8.97253612]], w0=-3.577375214971556, w1=1.1610463416186254\n",
      "Gradient Descent(699/999): loss=[[8.9724023]], w0=-3.57852312840799, w1=1.161161661974524\n",
      "Gradient Descent(700/999): loss=[[8.97226944]], w0=-3.5796669033964967, w1=1.161276566578439\n",
      "Gradient Descent(701/999): loss=[[8.97213753]], w0=-3.580806554856974, w1=1.1613910569292365\n",
      "Gradient Descent(702/999): loss=[[8.97200658]], w0=-3.5819420976555305, w1=1.1615051345203782\n",
      "Gradient Descent(703/999): loss=[[8.97187657]], w0=-3.58307354660468, w1=1.1616188008399415\n",
      "Gradient Descent(704/999): loss=[[8.97174749]], w0=-3.5842009164635336, w1=1.1617320573706398\n",
      "Gradient Descent(705/999): loss=[[8.97161934]], w0=-3.585324221937994, w1=1.1618449055898397\n",
      "Gradient Descent(706/999): loss=[[8.97149212]], w0=-3.586443477680946, w1=1.1619573469695834\n",
      "Gradient Descent(707/999): loss=[[8.97136581]], w0=-3.5875586982924474, w1=1.162069382976604\n",
      "Gradient Descent(708/999): loss=[[8.97124041]], w0=-3.5886698983199206, w1=1.1621810150723488\n",
      "Gradient Descent(709/999): loss=[[8.97111591]], w0=-3.589777092258341, w1=1.1622922447129949\n",
      "Gradient Descent(710/999): loss=[[8.97099231]], w0=-3.5908802945504283, w1=1.16240307334947\n",
      "Gradient Descent(711/999): loss=[[8.97086959]], w0=-3.591979519586832, w1=1.162513502427471\n",
      "Gradient Descent(712/999): loss=[[8.97074777]], w0=-3.593074781706321, w1=1.1626235333874835\n",
      "Gradient Descent(713/999): loss=[[8.97062681]], w0=-3.5941660951959706, w1=1.1627331676647976\n",
      "Gradient Descent(714/999): loss=[[8.97050673]], w0=-3.5952534742913476, w1=1.1628424066895322\n",
      "Gradient Descent(715/999): loss=[[8.97038751]], w0=-3.5963369331766977, w1=1.1629512518866474\n",
      "Gradient Descent(716/999): loss=[[8.97026916]], w0=-3.597416485985129, w1=1.1630597046759679\n",
      "Gradient Descent(717/999): loss=[[8.97015165]], w0=-3.598492146798798, w1=1.1631677664721982\n",
      "Gradient Descent(718/999): loss=[[8.97003499]], w0=-3.599563929649091, w1=1.1632754386849444\n",
      "Gradient Descent(719/999): loss=[[8.96991916]], w0=-3.6006318485168096, w1=1.1633827227187286\n",
      "Gradient Descent(720/999): loss=[[8.96980418]], w0=-3.6016959173323513, w1=1.1634896199730114\n",
      "Gradient Descent(721/999): loss=[[8.96969002]], w0=-3.602756149975892, w1=1.163596131842206\n",
      "Gradient Descent(722/999): loss=[[8.96957668]], w0=-3.603812560277567, w1=1.1637022597157\n",
      "Gradient Descent(723/999): loss=[[8.96946415]], w0=-3.604865162017651, w1=1.1638080049778714\n",
      "Gradient Descent(724/999): loss=[[8.96935244]], w0=-3.605913968926739, w1=1.1639133690081076\n",
      "Gradient Descent(725/999): loss=[[8.96924153]], w0=-3.6069589946859235, w1=1.1640183531808224\n",
      "Gradient Descent(726/999): loss=[[8.96913142]], w0=-3.6080002529269746, w1=1.1641229588654751\n",
      "Gradient Descent(727/999): loss=[[8.9690221]], w0=-3.6090377572325174, w1=1.1642271874265875\n",
      "Gradient Descent(728/999): loss=[[8.96891357]], w0=-3.6100715211362084, w1=1.164331040223763\n",
      "Gradient Descent(729/999): loss=[[8.96880582]], w0=-3.611101558122914, w1=1.1644345186117016\n",
      "Gradient Descent(730/999): loss=[[8.96869884]], w0=-3.612127881628883, w1=1.1645376239402212\n",
      "Gradient Descent(731/999): loss=[[8.96859264]], w0=-3.6131505050419257, w1=1.1646403575542716\n",
      "Gradient Descent(732/999): loss=[[8.9684872]], w0=-3.6141694417015864, w1=1.1647427207939558\n",
      "Gradient Descent(733/999): loss=[[8.96838251]], w0=-3.6151847048993173, w1=1.1648447149945433\n",
      "Gradient Descent(734/999): loss=[[8.96827859]], w0=-3.6161963078786528, w1=1.1649463414864913\n",
      "Gradient Descent(735/999): loss=[[8.9681754]], w0=-3.617204263835381, w1=1.1650476015954603\n",
      "Gradient Descent(736/999): loss=[[8.96807297]], w0=-3.6182085859177184, w1=1.1651484966423302\n",
      "Gradient Descent(737/999): loss=[[8.96797127]], w0=-3.619209287226478, w1=1.1652490279432206\n",
      "Gradient Descent(738/999): loss=[[8.9678703]], w0=-3.6202063808152425, w1=1.165349196809505\n",
      "Gradient Descent(739/999): loss=[[8.96777006]], w0=-3.6211998796905336, w1=1.1654490045478303\n",
      "Gradient Descent(740/999): loss=[[8.96767054]], w0=-3.6221897968119827, w1=1.165548452460131\n",
      "Gradient Descent(741/999): loss=[[8.96757173]], w0=-3.623176145092499, w1=1.1656475418436496\n",
      "Gradient Descent(742/999): loss=[[8.96747364]], w0=-3.624158937398437, w1=1.165746273990951\n",
      "Gradient Descent(743/999): loss=[[8.96737625]], w0=-3.625138186549768, w1=1.1658446501899398\n",
      "Gradient Descent(744/999): loss=[[8.96727957]], w0=-3.626113905320242, w1=1.1659426717238783\n",
      "Gradient Descent(745/999): loss=[[8.96718357]], w0=-3.6270861064375595, w1=1.166040339871402\n",
      "Gradient Descent(746/999): loss=[[8.96708828]], w0=-3.628054802583534, w1=1.1661376559065364\n",
      "Gradient Descent(747/999): loss=[[8.96699366]], w0=-3.6290200063942586, w1=1.1662346210987142\n",
      "Gradient Descent(748/999): loss=[[8.96689973]], w0=-3.6299817304602713, w1=1.1663312367127916\n",
      "Gradient Descent(749/999): loss=[[8.96680647]], w0=-3.6309399873267187, w1=1.1664275040090644\n",
      "Gradient Descent(750/999): loss=[[8.96671389]], w0=-3.63189478949352, w1=1.1665234242432856\n",
      "Gradient Descent(751/999): loss=[[8.96662197]], w0=-3.6328461494155286, w1=1.1666189986666795\n",
      "Gradient Descent(752/999): loss=[[8.96653071]], w0=-3.6337940795026977, w1=1.1667142285259615\n",
      "Gradient Descent(753/999): loss=[[8.96644011]], w0=-3.6347385921202386, w1=1.1668091150633502\n",
      "Gradient Descent(754/999): loss=[[8.96635016]], w0=-3.6356796995887843, w1=1.1669036595165878\n",
      "Gradient Descent(755/999): loss=[[8.96626086]], w0=-3.63661741418455, w1=1.1669978631189522\n",
      "Gradient Descent(756/999): loss=[[8.9661722]], w0=-3.6375517481394914, w1=1.1670917270992769\n",
      "Gradient Descent(757/999): loss=[[8.96608418]], w0=-3.6384827136414675, w1=1.1671852526819642\n",
      "Gradient Descent(758/999): loss=[[8.9659968]], w0=-3.639410322834396, w1=1.1672784410870025\n",
      "Gradient Descent(759/999): loss=[[8.96591004]], w0=-3.6403345878184146, w1=1.1673712935299816\n",
      "Gradient Descent(760/999): loss=[[8.96582391]], w0=-3.6412555206500374, w1=1.1674638112221094\n",
      "Gradient Descent(761/999): loss=[[8.96573839]], w0=-3.6421731333423124, w1=1.1675559953702268\n",
      "Gradient Descent(762/999): loss=[[8.9656535]], w0=-3.643087437864978, w1=1.1676478471768237\n",
      "Gradient Descent(763/999): loss=[[8.96556921]], w0=-3.6439984461446198, w1=1.1677393678400552\n",
      "Gradient Descent(764/999): loss=[[8.96548553]], w0=-3.644906170064825, w1=1.1678305585537565\n",
      "Gradient Descent(765/999): loss=[[8.96540245]], w0=-3.6458106214663397, w1=1.1679214205074586\n",
      "Gradient Descent(766/999): loss=[[8.96531997]], w0=-3.6467118121472204, w1=1.168011954886405\n",
      "Gradient Descent(767/999): loss=[[8.96523808]], w0=-3.64760975386299, w1=1.1681021628715647\n",
      "Gradient Descent(768/999): loss=[[8.96515679]], w0=-3.6485044583267903, w1=1.1681920456396506\n",
      "Gradient Descent(769/999): loss=[[8.96507608]], w0=-3.649395937209535, w1=1.1682816043631317\n",
      "Gradient Descent(770/999): loss=[[8.96499595]], w0=-3.650284202140062, w1=1.1683708402102517\n",
      "Gradient Descent(771/999): loss=[[8.96491639]], w0=-3.651169264705285, w1=1.1684597543450415\n",
      "Gradient Descent(772/999): loss=[[8.96483741]], w0=-3.652051136450345, w1=1.1685483479273353\n",
      "Gradient Descent(773/999): loss=[[8.964759]], w0=-3.6529298288787597, w1=1.1686366221127868\n",
      "Gradient Descent(774/999): loss=[[8.96468115]], w0=-3.653805353452575, w1=1.1687245780528828\n",
      "Gradient Descent(775/999): loss=[[8.96460386]], w0=-3.6546777215925137, w1=1.1688122168949586\n",
      "Gradient Descent(776/999): loss=[[8.96452713]], w0=-3.6555469446781252, w1=1.168899539782213\n",
      "Gradient Descent(777/999): loss=[[8.96445095]], w0=-3.656413034047933, w1=1.1689865478537247\n",
      "Gradient Descent(778/999): loss=[[8.96437532]], w0=-3.657276000999583, w1=1.1690732422444636\n",
      "Gradient Descent(779/999): loss=[[8.96430023]], w0=-3.658135856789991, w1=1.16915962408531\n",
      "Gradient Descent(780/999): loss=[[8.96422568]], w0=-3.6589926126354895, w1=1.1692456945030654\n",
      "Gradient Descent(781/999): loss=[[8.96415167]], w0=-3.659846279711974, w1=1.16933145462047\n",
      "Gradient Descent(782/999): loss=[[8.96407819]], w0=-3.660696869155049, w1=1.1694169055562162\n",
      "Gradient Descent(783/999): loss=[[8.96400525]], w0=-3.6615443920601725, w1=1.169502048424963\n",
      "Gradient Descent(784/999): loss=[[8.96393282]], w0=-3.6623888594828014, w1=1.169586884337351\n",
      "Gradient Descent(785/999): loss=[[8.96386092]], w0=-3.6632302824385357, w1=1.1696714144000164\n",
      "Gradient Descent(786/999): loss=[[8.96378953]], w0=-3.6640686719032622, w1=1.1697556397156066\n",
      "Gradient Descent(787/999): loss=[[8.96371866]], w0=-3.6649040388132974, w1=1.1698395613827928\n",
      "Gradient Descent(788/999): loss=[[8.9636483]], w0=-3.66573639406553, w1=1.169923180496286\n",
      "Gradient Descent(789/999): loss=[[8.96357845]], w0=-3.6665657485175633, w1=1.1700064981468499\n",
      "Gradient Descent(790/999): loss=[[8.9635091]], w0=-3.6673921129878577, w1=1.1700895154213165\n",
      "Gradient Descent(791/999): loss=[[8.96344024]], w0=-3.66821549825587, w1=1.1701722334025988\n",
      "Gradient Descent(792/999): loss=[[8.96337189]], w0=-3.6690359150621954, w1=1.1702546531697062\n",
      "Gradient Descent(793/999): loss=[[8.96330402]], w0=-3.669853374108707, w1=1.1703367757977585\n",
      "Gradient Descent(794/999): loss=[[8.96323664]], w0=-3.670667886058696, w1=1.1704186023579979\n",
      "Gradient Descent(795/999): loss=[[8.96316975]], w0=-3.67147946153701, w1=1.1705001339178067\n",
      "Gradient Descent(796/999): loss=[[8.96310334]], w0=-3.6722881111301926, w1=1.170581371540717\n",
      "Gradient Descent(797/999): loss=[[8.96303741]], w0=-3.6730938453866195, w1=1.1706623162864287\n",
      "Gradient Descent(798/999): loss=[[8.96297195]], w0=-3.6738966748166395, w1=1.1707429692108189\n",
      "Gradient Descent(799/999): loss=[[8.96290697]], w0=-3.674696609892708, w1=1.1708233313659606\n",
      "Gradient Descent(800/999): loss=[[8.96284245]], w0=-3.6754936610495252, w1=1.1709034038001322\n",
      "Gradient Descent(801/999): loss=[[8.96277839]], w0=-3.676287838684173, w1=1.1709831875578331\n",
      "Gradient Descent(802/999): loss=[[8.9627148]], w0=-3.67707915315625, w1=1.1710626836797977\n",
      "Gradient Descent(803/999): loss=[[8.96265166]], w0=-3.6778676147880054, w1=1.1711418932030078\n",
      "Gradient Descent(804/999): loss=[[8.96258898]], w0=-3.6786532338644755, w1=1.1712208171607068\n",
      "Gradient Descent(805/999): loss=[[8.96252675]], w0=-3.679436020633617, w1=1.171299456582413\n",
      "Gradient Descent(806/999): loss=[[8.96246497]], w0=-3.6802159853064405, w1=1.1713778124939334\n",
      "Gradient Descent(807/999): loss=[[8.96240363]], w0=-3.680993138057144, w1=1.1714558859173763\n",
      "Gradient Descent(808/999): loss=[[8.96234274]], w0=-3.6817674890232452, w1=1.1715336778711656\n",
      "Gradient Descent(809/999): loss=[[8.96228228]], w0=-3.682539048305715, w1=1.1716111893700532\n",
      "Gradient Descent(810/999): loss=[[8.96222225]], w0=-3.6833078259691083, w1=1.1716884214251329\n",
      "Gradient Descent(811/999): loss=[[8.96216266]], w0=-3.6840738320416944, w1=1.1717653750438528\n",
      "Gradient Descent(812/999): loss=[[8.9621035]], w0=-3.6848370765155893, w1=1.17184205123003\n",
      "Gradient Descent(813/999): loss=[[8.96204477]], w0=-3.6855975693468856, w1=1.1719184509838612\n",
      "Gradient Descent(814/999): loss=[[8.96198645]], w0=-3.686355320455782, w1=1.171994575301939\n",
      "Gradient Descent(815/999): loss=[[8.96192856]], w0=-3.687110339726714, w1=1.1720704251772607\n",
      "Gradient Descent(816/999): loss=[[8.96187108]], w0=-3.68786263700848, w1=1.1721460015992464\n",
      "Gradient Descent(817/999): loss=[[8.96181402]], w0=-3.688612222114373, w1=1.172221305553747\n",
      "Gradient Descent(818/999): loss=[[8.96175737]], w0=-3.689359104822307, w1=1.1722963380230598\n",
      "Gradient Descent(819/999): loss=[[8.96170112]], w0=-3.690103294874944, w1=1.1723710999859418\n",
      "Gradient Descent(820/999): loss=[[8.96164528]], w0=-3.6908448019798232, w1=1.1724455924176198\n",
      "Gradient Descent(821/999): loss=[[8.96158984]], w0=-3.6915836358094847, w1=1.1725198162898058\n",
      "Gradient Descent(822/999): loss=[[8.9615348]], w0=-3.6923198060015983, w1=1.172593772570708\n",
      "Gradient Descent(823/999): loss=[[8.96148016]], w0=-3.6930533221590878, w1=1.1726674622250446\n",
      "Gradient Descent(824/999): loss=[[8.96142591]], w0=-3.6937841938502567, w1=1.172740886214055\n",
      "Gradient Descent(825/999): loss=[[8.96137205]], w0=-3.6945124306089125, w1=1.1728140454955138\n",
      "Gradient Descent(826/999): loss=[[8.96131858]], w0=-3.695238041934492, w1=1.1728869410237428\n",
      "Gradient Descent(827/999): loss=[[8.96126549]], w0=-3.6959610372921854, w1=1.1729595737496223\n",
      "Gradient Descent(828/999): loss=[[8.96121279]], w0=-3.696681426113057, w1=1.173031944620606\n",
      "Gradient Descent(829/999): loss=[[8.96116046]], w0=-3.697399217794173, w1=1.1731040545807299\n",
      "Gradient Descent(830/999): loss=[[8.96110852]], w0=-3.6981144216987185, w1=1.1731759045706285\n",
      "Gradient Descent(831/999): loss=[[8.96105694]], w0=-3.6988270471561244, w1=1.1732474955275438\n",
      "Gradient Descent(832/999): loss=[[8.96100574]], w0=-3.699537103462187, w1=1.1733188283853397\n",
      "Gradient Descent(833/999): loss=[[8.9609549]], w0=-3.7002445998791895, w1=1.1733899040745128\n",
      "Gradient Descent(834/999): loss=[[8.96090443]], w0=-3.700949545636022, w1=1.1734607235222052\n",
      "Gradient Descent(835/999): loss=[[8.96085433]], w0=-3.7016519499283036, w1=1.1735312876522168\n",
      "Gradient Descent(836/999): loss=[[8.96080458]], w0=-3.702351821918501, w1=1.1736015973850167\n",
      "Gradient Descent(837/999): loss=[[8.96075519]], w0=-3.7030491707360484, w1=1.1736716536377554\n",
      "Gradient Descent(838/999): loss=[[8.96070616]], w0=-3.703744005477467, w1=1.173741457324277\n",
      "Gradient Descent(839/999): loss=[[8.96065748]], w0=-3.7044363352064824, w1=1.1738110093551317\n",
      "Gradient Descent(840/999): loss=[[8.96060916]], w0=-3.705126168954145, w1=1.1738803106375857\n",
      "Gradient Descent(841/999): loss=[[8.96056118]], w0=-3.7058135157189454, w1=1.1739493620756356\n",
      "Gradient Descent(842/999): loss=[[8.96051354]], w0=-3.706498384466934, w1=1.174018164570018\n",
      "Gradient Descent(843/999): loss=[[8.96046625]], w0=-3.7071807841318365, w1=1.1740867190182227\n",
      "Gradient Descent(844/999): loss=[[8.9604193]], w0=-3.7078607236151697, w1=1.1741550263145037\n",
      "Gradient Descent(845/999): loss=[[8.96037268]], w0=-3.7085382117863603, w1=1.1742230873498911\n",
      "Gradient Descent(846/999): loss=[[8.9603264]], w0=-3.7092132574828582, w1=1.1742909030122024\n",
      "Gradient Descent(847/999): loss=[[8.96028046]], w0=-3.7098858695102526, w1=1.1743584741860547\n",
      "Gradient Descent(848/999): loss=[[8.96023484]], w0=-3.710556056642387, w1=1.174425801752876\n",
      "Gradient Descent(849/999): loss=[[8.96018956]], w0=-3.711223827621474, w1=1.1744928865909157\n",
      "Gradient Descent(850/999): loss=[[8.96014459]], w0=-3.7118891911582077, w1=1.1745597295752581\n",
      "Gradient Descent(851/999): loss=[[8.96009996]], w0=-3.7125521559318795, w1=1.174626331577832\n",
      "Gradient Descent(852/999): loss=[[8.96005564]], w0=-3.71321273059049, w1=1.1746926934674229\n",
      "Gradient Descent(853/999): loss=[[8.96001164]], w0=-3.7138709237508616, w1=1.174758816109684\n",
      "Gradient Descent(854/999): loss=[[8.95996796]], w0=-3.7145267439987526, w1=1.1748247003671477\n",
      "Gradient Descent(855/999): loss=[[8.9599246]], w0=-3.715180199888967, w1=1.1748903470992373\n",
      "Gradient Descent(856/999): loss=[[8.95988155]], w0=-3.715831299945467, w1=1.1749557571622766\n",
      "Gradient Descent(857/999): loss=[[8.9598388]], w0=-3.7164800526614843, w1=1.175020931409504\n",
      "Gradient Descent(858/999): loss=[[8.95979637]], w0=-3.717126466499632, w1=1.1750858706910792\n",
      "Gradient Descent(859/999): loss=[[8.95975423]], w0=-3.717770549892013, w1=1.1751505758540997\n",
      "Gradient Descent(860/999): loss=[[8.95971241]], w0=-3.7184123112403307, w1=1.1752150477426069\n",
      "Gradient Descent(861/999): loss=[[8.95967088]], w0=-3.719051758915999, w1=1.1752792871976006\n",
      "Gradient Descent(862/999): loss=[[8.95962965]], w0=-3.7196889012602505, w1=1.1753432950570482\n",
      "Gradient Descent(863/999): loss=[[8.95958872]], w0=-3.720323746584248, w1=1.1754070721558956\n",
      "Gradient Descent(864/999): loss=[[8.95954809]], w0=-3.7209563031691886, w1=1.1754706193260793\n",
      "Gradient Descent(865/999): loss=[[8.95950774]], w0=-3.721586579266416, w1=1.1755339373965363\n",
      "Gradient Descent(866/999): loss=[[8.95946769]], w0=-3.722214583097525, w1=1.1755970271932146\n",
      "Gradient Descent(867/999): loss=[[8.95942792]], w0=-3.7228403228544704, w1=1.1756598895390855\n",
      "Gradient Descent(868/999): loss=[[8.95938844]], w0=-3.723463806699674, w1=1.175722525254152\n",
      "Gradient Descent(869/999): loss=[[8.95934925]], w0=-3.7240850427661294, w1=1.1757849351554626\n",
      "Gradient Descent(870/999): loss=[[8.95931034]], w0=-3.7247040391575097, w1=1.1758471200571183\n",
      "Gradient Descent(871/999): loss=[[8.9592717]], w0=-3.7253208039482733, w1=1.175909080770286\n",
      "Gradient Descent(872/999): loss=[[8.95923335]], w0=-3.7259353451837676, w1=1.175970818103209\n",
      "Gradient Descent(873/999): loss=[[8.95919527]], w0=-3.726547670880336, w1=1.1760323328612146\n",
      "Gradient Descent(874/999): loss=[[8.95915747]], w0=-3.72715778902542, w1=1.1760936258467294\n",
      "Gradient Descent(875/999): loss=[[8.95911994]], w0=-3.727765707577667, w1=1.1761546978592845\n",
      "Gradient Descent(876/999): loss=[[8.95908267]], w0=-3.7283714344670296, w1=1.1762155496955309\n",
      "Gradient Descent(877/999): loss=[[8.95904568]], w0=-3.728974977594873, w1=1.1762761821492451\n",
      "Gradient Descent(878/999): loss=[[8.95900895]], w0=-3.729576344834076, w1=1.1763365960113448\n",
      "Gradient Descent(879/999): loss=[[8.95897249]], w0=-3.730175544029134, w1=1.176396792069893\n",
      "Gradient Descent(880/999): loss=[[8.95893629]], w0=-3.730772582996262, w1=1.1764567711101144\n",
      "Gradient Descent(881/999): loss=[[8.95890035]], w0=-3.731367469523495, w1=1.176516533914401\n",
      "Gradient Descent(882/999): loss=[[8.95886466]], w0=-3.731960211370792, w1=1.1765760812623252\n",
      "Gradient Descent(883/999): loss=[[8.95882924]], w0=-3.732550816270135, w1=1.1766354139306485\n",
      "Gradient Descent(884/999): loss=[[8.95879407]], w0=-3.7331392919256303, w1=1.1766945326933316\n",
      "Gradient Descent(885/999): loss=[[8.95875915]], w0=-3.733725646013611, w1=1.1767534383215457\n",
      "Gradient Descent(886/999): loss=[[8.95872449]], w0=-3.7343098861827335, w1=1.176812131583681\n",
      "Gradient Descent(887/999): loss=[[8.95869007]], w0=-3.7348920200540814, w1=1.1768706132453584\n",
      "Gradient Descent(888/999): loss=[[8.9586559]], w0=-3.7354720552212615, w1=1.1769288840694379\n",
      "Gradient Descent(889/999): loss=[[8.95862198]], w0=-3.7360499992505045, w1=1.1769869448160293\n",
      "Gradient Descent(890/999): loss=[[8.9585883]], w0=-3.736625859680763, w1=1.1770447962425021\n",
      "Gradient Descent(891/999): loss=[[8.95855487]], w0=-3.7371996440238116, w1=1.177102439103496\n",
      "Gradient Descent(892/999): loss=[[8.95852167]], w0=-3.737771359764342, w1=1.1771598741509293\n",
      "Gradient Descent(893/999): loss=[[8.95848872]], w0=-3.738341014360062, w1=1.1772171021340092\n",
      "Gradient Descent(894/999): loss=[[8.958456]], w0=-3.7389086152417947, w1=1.1772741237992432\n",
      "Gradient Descent(895/999): loss=[[8.95842351]], w0=-3.7394741698135725, w1=1.177330939890446\n",
      "Gradient Descent(896/999): loss=[[8.95839126]], w0=-3.7400376854527346, w1=1.1773875511487517\n",
      "Gradient Descent(897/999): loss=[[8.95835925]], w0=-3.740599169510024, w1=1.177443958312622\n",
      "Gradient Descent(898/999): loss=[[8.95832746]], w0=-3.741158629309682, w1=1.1775001621178565\n",
      "Gradient Descent(899/999): loss=[[8.9582959]], w0=-3.741716072149546, w1=1.1775561632976024\n",
      "Gradient Descent(900/999): loss=[[8.95826457]], w0=-3.742271505301143, w1=1.1776119625823624\n",
      "Gradient Descent(901/999): loss=[[8.95823346]], w0=-3.742824936009783, w1=1.1776675607000082\n",
      "Gradient Descent(902/999): loss=[[8.95820258]], w0=-3.743376371494658, w1=1.1777229583757842\n",
      "Gradient Descent(903/999): loss=[[8.95817192]], w0=-3.7439258189489317, w1=1.177778156332323\n",
      "Gradient Descent(904/999): loss=[[8.95814148]], w0=-3.744473285539835, w1=1.1778331552896497\n",
      "Gradient Descent(905/999): loss=[[8.95811126]], w0=-3.74501877840876, w1=1.1778879559651958\n",
      "Gradient Descent(906/999): loss=[[8.95808126]], w0=-3.745562304671353, w1=1.1779425590738035\n",
      "Gradient Descent(907/999): loss=[[8.95805147]], w0=-3.7461038714176067, w1=1.1779969653277411\n",
      "Gradient Descent(908/999): loss=[[8.9580219]], w0=-3.7466434857119526, w1=1.1780511754367065\n",
      "Gradient Descent(909/999): loss=[[8.95799254]], w0=-3.7471811545933544, w1=1.17810519010784\n",
      "Gradient Descent(910/999): loss=[[8.95796339]], w0=-3.747716885075399, w1=1.1781590100457326\n",
      "Gradient Descent(911/999): loss=[[8.95793446]], w0=-3.7482506841463863, w1=1.178212635952435\n",
      "Gradient Descent(912/999): loss=[[8.95790573]], w0=-3.7487825587694243, w1=1.1782660685274666\n",
      "Gradient Descent(913/999): loss=[[8.9578772]], w0=-3.7493125158825165, w1=1.1783193084678254\n",
      "Gradient Descent(914/999): loss=[[8.95784889]], w0=-3.7498405623986537, w1=1.178372356467996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(915/999): loss=[[8.95782077]], w0=-3.750366705205904, w1=1.17842521321996\n",
      "Gradient Descent(916/999): loss=[[8.95779286]], w0=-3.7508909511675026, w1=1.1784778794132031\n",
      "Gradient Descent(917/999): loss=[[8.95776515]], w0=-3.751413307121942, w1=1.1785303557347269\n",
      "Gradient Descent(918/999): loss=[[8.95773764]], w0=-3.7519337798830596, w1=1.178582642869055\n",
      "Gradient Descent(919/999): loss=[[8.95771032]], w0=-3.7524523762401287, w1=1.178634741498243\n",
      "Gradient Descent(920/999): loss=[[8.95768321]], w0=-3.7529691029579455, w1=1.1786866523018886\n",
      "Gradient Descent(921/999): loss=[[8.95765629]], w0=-3.7534839667769178, w1=1.1787383759571386\n",
      "Gradient Descent(922/999): loss=[[8.95762956]], w0=-3.7539969744131527, w1=1.1787899131386992\n",
      "Gradient Descent(923/999): loss=[[8.95760302]], w0=-3.754508132558545, w1=1.1788412645188435\n",
      "Gradient Descent(924/999): loss=[[8.95757668]], w0=-3.7550174478808636, w1=1.1788924307674211\n",
      "Gradient Descent(925/999): loss=[[8.95755052]], w0=-3.7555249270238384, w1=1.1789434125518672\n",
      "Gradient Descent(926/999): loss=[[8.95752456]], w0=-3.756030576607248, w1=1.17899421053721\n",
      "Gradient Descent(927/999): loss=[[8.95749878]], w0=-3.756534403227006, w1=1.1790448253860806\n",
      "Gradient Descent(928/999): loss=[[8.95747318]], w0=-3.7570364134552445, w1=1.1790952577587208\n",
      "Gradient Descent(929/999): loss=[[8.95744777]], w0=-3.757536613840404, w1=1.179145508312993\n",
      "Gradient Descent(930/999): loss=[[8.95742255]], w0=-3.758035010907315, w1=1.1791955777043868\n",
      "Gradient Descent(931/999): loss=[[8.9573975]], w0=-3.758531611157286, w1=1.179245466586029\n",
      "Gradient Descent(932/999): loss=[[8.95737264]], w0=-3.759026421068186, w1=1.1792951756086922\n",
      "Gradient Descent(933/999): loss=[[8.95734795]], w0=-3.759519447094531, w1=1.1793447054208024\n",
      "Gradient Descent(934/999): loss=[[8.95732344]], w0=-3.7600106956675656, w1=1.1793940566684478\n",
      "Gradient Descent(935/999): loss=[[8.95729911]], w0=-3.7605001731953505, w1=1.1794432299953879\n",
      "Gradient Descent(936/999): loss=[[8.95727495]], w0=-3.7609878860628427, w1=1.1794922260430603\n",
      "Gradient Descent(937/999): loss=[[8.95725097]], w0=-3.7614738406319814, w1=1.1795410454505912\n",
      "Gradient Descent(938/999): loss=[[8.95722716]], w0=-3.7619580432417687, w1=1.1795896888548023\n",
      "Gradient Descent(939/999): loss=[[8.95720352]], w0=-3.7624405002083536, w1=1.1796381568902188\n",
      "Gradient Descent(940/999): loss=[[8.95718005]], w0=-3.762921217825115, w1=1.1796864501890791\n",
      "Gradient Descent(941/999): loss=[[8.95715675]], w0=-3.7634002023627415, w1=1.1797345693813421\n",
      "Gradient Descent(942/999): loss=[[8.95713362]], w0=-3.7638774600693163, w1=1.1797825150946952\n",
      "Gradient Descent(943/999): loss=[[8.95711065]], w0=-3.764352997170396, w1=1.1798302879545635\n",
      "Gradient Descent(944/999): loss=[[8.95708785]], w0=-3.7648268198690933, w1=1.1798778885841164\n",
      "Gradient Descent(945/999): loss=[[8.95706522]], w0=-3.765298934346157, w1=1.1799253176042774\n",
      "Gradient Descent(946/999): loss=[[8.95704274]], w0=-3.7657693467600537, w1=1.1799725756337314\n",
      "Gradient Descent(947/999): loss=[[8.95702043]], w0=-3.766238063247047, w1=1.1800196632889326\n",
      "Gradient Descent(948/999): loss=[[8.95699828]], w0=-3.766705089921279, w1=1.1800665811841127\n",
      "Gradient Descent(949/999): loss=[[8.95697629]], w0=-3.767170432874848, w1=1.1801133299312891\n",
      "Gradient Descent(950/999): loss=[[8.95695445]], w0=-3.76763409817789, w1=1.1801599101402733\n",
      "Gradient Descent(951/999): loss=[[8.95693278]], w0=-3.7680960918786566, w1=1.180206322418677\n",
      "Gradient Descent(952/999): loss=[[8.95691126]], w0=-3.768556420003594, w1=1.1802525673719226\n",
      "Gradient Descent(953/999): loss=[[8.95688989]], w0=-3.7690150885574223, w1=1.1802986456032492\n",
      "Gradient Descent(954/999): loss=[[8.95686868]], w0=-3.769472103523214, w1=1.1803445577137215\n",
      "Gradient Descent(955/999): loss=[[8.95684762]], w0=-3.7699274708624704, w1=1.1803903043022363\n",
      "Gradient Descent(956/999): loss=[[8.95682671]], w0=-3.770381196515201, w1=1.1804358859655333\n",
      "Gradient Descent(957/999): loss=[[8.95680596]], w0=-3.7708332864000003, w1=1.180481303298198\n",
      "Gradient Descent(958/999): loss=[[8.95678535]], w0=-3.771283746414125, w1=1.180526556892675\n",
      "Gradient Descent(959/999): loss=[[8.95676489]], w0=-3.7717325824335717, w1=1.1805716473392711\n",
      "Gradient Descent(960/999): loss=[[8.95674458]], w0=-3.772179800313152, w1=1.1806165752261664\n",
      "Gradient Descent(961/999): loss=[[8.95672441]], w0=-3.7726254058865707, w1=1.180661341139419\n",
      "Gradient Descent(962/999): loss=[[8.95670439]], w0=-3.7730694049665, w1=1.1807059456629752\n",
      "Gradient Descent(963/999): loss=[[8.95668451]], w0=-3.773511803344657, w1=1.180750389378676\n",
      "Gradient Descent(964/999): loss=[[8.95666478]], w0=-3.7739526067918785, w1=1.1807946728662644\n",
      "Gradient Descent(965/999): loss=[[8.95664519]], w0=-3.774391821058196, w1=1.1808387967033933\n",
      "Gradient Descent(966/999): loss=[[8.95662574]], w0=-3.7748294518729115, w1=1.1808827614656336\n",
      "Gradient Descent(967/999): loss=[[8.95660643]], w0=-3.775265504944671, w1=1.1809265677264802\n",
      "Gradient Descent(968/999): loss=[[8.95658726]], w0=-3.7756999859615403, w1=1.1809702160573616\n",
      "Gradient Descent(969/999): loss=[[8.95656822]], w0=-3.776132900591079, w1=1.1810137070276452\n",
      "Gradient Descent(970/999): loss=[[8.95654933]], w0=-3.776564254480413, w1=1.1810570412046462\n",
      "Gradient Descent(971/999): loss=[[8.95653057]], w0=-3.7769940532563107, w1=1.1811002191536342\n",
      "Gradient Descent(972/999): loss=[[8.95651194]], w0=-3.777422302525253, w1=1.1811432414378418\n",
      "Gradient Descent(973/999): loss=[[8.95649345]], w0=-3.77784900787351, w1=1.1811861086184694\n",
      "Gradient Descent(974/999): loss=[[8.95647509]], w0=-3.778274174867212, w1=1.1812288212546962\n",
      "Gradient Descent(975/999): loss=[[8.95645686]], w0=-3.7786978090524213, w1=1.1812713799036836\n",
      "Gradient Descent(976/999): loss=[[8.95643877]], w0=-3.7791199159552065, w1=1.181313785120585\n",
      "Gradient Descent(977/999): loss=[[8.9564208]], w0=-3.7795405010817134, w1=1.1813560374585537\n",
      "Gradient Descent(978/999): loss=[[8.95640297]], w0=-3.7799595699182373, w1=1.1813981374687457\n",
      "Gradient Descent(979/999): loss=[[8.95638526]], w0=-3.780377127931294, w1=1.1814400857003333\n",
      "Gradient Descent(980/999): loss=[[8.95636768]], w0=-3.780793180567692, w1=1.181481882700507\n",
      "Gradient Descent(981/999): loss=[[8.95635023]], w0=-3.781207733254602, w1=1.1815235290144848\n",
      "Gradient Descent(982/999): loss=[[8.9563329]], w0=-3.78162079139963, w1=1.1815650251855196\n",
      "Gradient Descent(983/999): loss=[[8.9563157]], w0=-3.7820323603908856, w1=1.1816063717549052\n",
      "Gradient Descent(984/999): loss=[[8.95629862]], w0=-3.7824424455970536, w1=1.181647569261985\n",
      "Gradient Descent(985/999): loss=[[8.95628166]], w0=-3.7828510523674637, w1=1.1816886182441557\n",
      "Gradient Descent(986/999): loss=[[8.95626483]], w0=-3.78325818603216, w1=1.1817295192368797\n",
      "Gradient Descent(987/999): loss=[[8.95624812]], w0=-3.7836638519019705, w1=1.181770272773686\n",
      "Gradient Descent(988/999): loss=[[8.95623152]], w0=-3.7840680552685777, w1=1.1818108793861823\n",
      "Gradient Descent(989/999): loss=[[8.95621505]], w0=-3.7844708014045856, w1=1.1818513396040586\n",
      "Gradient Descent(990/999): loss=[[8.9561987]], w0=-3.78487209556359, w1=1.1818916539550954\n",
      "Gradient Descent(991/999): loss=[[8.95618246]], w0=-3.785271942980246, w1=1.181931822965171\n",
      "Gradient Descent(992/999): loss=[[8.95616634]], w0=-3.7856703488703376, w1=1.181971847158267\n",
      "Gradient Descent(993/999): loss=[[8.95615034]], w0=-3.7860673184308435, w1=1.1820117270564765\n",
      "Gradient Descent(994/999): loss=[[8.95613445]], w0=-3.7864628568400076, w1=1.1820514631800105\n",
      "Gradient Descent(995/999): loss=[[8.95611867]], w0=-3.786856969257405, w1=1.1820910560472038\n",
      "Gradient Descent(996/999): loss=[[8.95610301]], w0=-3.7872496608240085, w1=1.1821305061745233\n",
      "Gradient Descent(997/999): loss=[[8.95608746]], w0=-3.787640936662258, w1=1.1821698140765735\n",
      "Gradient Descent(998/999): loss=[[8.95607203]], w0=-3.7880308018761255, w1=1.1822089802661038\n",
      "Gradient Descent(999/999): loss=[[8.9560567]], w0=-3.7884192615511822, w1=1.1822480052540147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.78841926,  1.18224801])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.95604149]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHwCAYAAABdQ1JvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABY4ElEQVR4nO3dd3xV9f3H8dfXECVO3JU4wIVasaBoVVrnr8VRK+Le1bpaR60VBVfdoLhH3btuxahVS92rLjQqLsSBI6AiGlxRQ/j+/jg3NMRMknvPHa/n45FHknPPPfdzT8Llfb/5nO83xBiRJEmS1Lp50i5AkiRJyneGZkmSJKkdhmZJkiSpHYZmSZIkqR2GZkmSJKkdhmZJkiSpHYZmSXkrhHBtCOHUzNe/DiFMzNHjxhDCyjl4nH4hhOoQwtchhMNCCJeGEI7P9uPmmxDCJiGEj7tw/1TOWwjhmxDCirl+XEnpMDRL6pIQwuQQQl0mQHwaQrgmhLBgdz9OjPHJGGO/DtTzhxDCU939+E2O/1gI4fvM8/08hDA2hLDMXB7uKOCxGONCMcYLYowHxRhPyTxOl4Lk3AohnBhCqM88v9oQwn9DCBvkuo7WtPTzbXreuvmxeoUQrg4hfJJ5Y/N2COHoJo+7YIzxve5+XEn5ydAsqTtsE2NcEFgbWBc4rvkOIYQeOa8qew7JPN9VgV7Auc136ODzXQF4vXtL6xa3Zp7fksBTwNgQQki5pjScCywIrA4sAvweeDfViiSlxtAsqdvEGGuAB4A1YXabw8EhhEnApMy234UQXm4yirlW4/1DCANDCC9lRvVuBXo2uW2OkdcQwnKZUd5pIYTpIYSLQgirA5cCGzSOlGb2nS+EcFYI4cPMaPilIYSKJscaHkKYGkKYEkLYtxPP9wvgzibPd3II4egQwqvAtyGEHiGE34cQXs8838cyNRJCeATYFLgoU+uqje0oIYQFMuexd+a2b0IIvZs+dghh/cwIaFmTbdtlHpsQwnohhPEhhK8yz/mcjj6vJs+vHrgO+BmweAihdwjhnhDCFyGEd0II+zd57BNDCHeEEG7N/PxeCiH8osntc7S8NG29aS6EMCKE8G7mOG+EELbLbG/t5zvHsUII+2fq+yJTb+8mt8UQwkEhhEkhhC9DCBe38YZgXeCmGOOXMcZZMca3Yox3NH9OmfPyTZOP70IIscl++4YQ3sw83rgQwgod+wlIyieGZkndJoSwHLAVUN1k81Dgl8AaIYS1gauBA4HFgcuAezKhdl6gCrgBWAy4Hdi+lccpA/4FfAD0ASqBW2KMbwIHAc9k/nTeK3OXM0hGhQcAK2f2PyFzrC2AI4HfAKsA/9eJ57tEpsamz3dXYGuSEegVgZuBw0lGbe8H7g0hzBtj3Ax4ksyodYzx7cYDxBi/BbYEpmRuWzDGOKXpY8cYnwW+BTZrsnk34KbM1+cD58cYFwZWAm7r6PNq8vzmA/4AfBxj/DzzXD4GegM7AKeHEDZvcpdtSX5ui2XqqAohlHf2cUlGc39NMrp7EvDPEMIybfx8m9a8GTAK2AlYhuR35JZmu/2OJBD/IrPfkFbqeBY4LYSwTwhhldaKjTE2/TktCNzV+JghhKHAMcAwkt+BJ0nOo6QCY2iW1B2qMqN+TwGPA6c3uW1UjPGLGGMdsD9wWYzxuRhjQ4zxOuAHYP3MRzlwXoyxPjOi90Irj7ceSXAbHmP8Nsb4fYyxxT7mzCji/sBfM3V8nalvl8wuOwHXxBhfy4TVEzvwfC/IPN9XgKnAEU1vizF+lHm+OwP3xRgfzIzangVUABt24DE64maSkE4IYSGSNyyNgaweWDmEsESM8ZtMyO6onTLP7yNgHWBo5g3Rr4CjM+f7ZeBKYM8m93sxxnhH5rmeQ/KXgvU7+6RijLdnguisGOOtJH+lWK+Dd98duDrG+FKM8QdgJMnIdJ8m+4yOMdbGGD8EHiV5M9WSQ4EbgUOANzKj11u29eAh6XleDWj8i8WBJP8G3owxziT53RvgaLNUeAzNkrrD0BhjrxjjCjHGP2cCY6OPmny9AvC3TKtCbSaYLUcSgHsDNTHG2GT/D1p5vOWADzIhpD1LAvMDLzZ5zH9ntpN53KY1tvaYTR2Web6VMcbdY4zTmtzW9Fi9mx4vxjgrc3tlBx6jI24ChmVGhIcBL8UYGx/vjySj62+FEF4IIfyuE8e9LfP8looxbhZjfJHkuTS+6Wj0AXM+l9nPPfNcG0elOyWEsFf4XwtPLUn7yxIdvHvzc/4NML1ZnZ80+fo7kr7ln4gx1sUYT48xrkPyl5HbgNtDCIu1UveWwF9I/j00/htYATi/yXP5Agh03++ApBwxNEvKtqYh+CPgtEwga/yYP8Z4M8mIbWWz/tLlWznmR8DyoeWL7WKz7z8H6oCfN3nMRTJ/RifzuMt14DE7qunjTyEJTcDsUe/lgJpOHqflHWJ8gyQgbsmcrRnEGCfFGHcFliJpT7kj0ys9t6YAi2VGtBstz5zPZfZ5DCHMAyybuR8k4XT+Jvv+rKUHyYzAXkEyurt4pgXjNZKgCe2fl+bnfAGSwNuRc96qGONXJKPECwB9W6i7H0n/904xxqZvnD4CDmz2O18RY/xvV+qRlHuGZkm5dAVwUAjhlyGxQAhh60wQewaYCRwWkgvohtH6n+SfJwm7ozPH6BlCGJy57VNg2UyPdOOI5xXAuSGEpQBCCJUhhMY+1tuAP4QQ1gghzA/8vRuf723A1iGEzTO9vX8jaUfpSGD6lOTiu0Xa2e8m4DBgI5J+YgBCCHuEEJbMPP/azOaGTtY/WyYI/hcYlTnfa5GMZt/YZLd1QgjDMm9mDid5ro1tIS8Du4UQyjJ95Bu38lALkATjaZnnsQ+ZCy0z5vj5tuAmYJ8QwoDMCPzpwHMxxsmdeb6Zxz4+hLBuCGHeEEJPklHkWmBis/0WBu4GjmuhTehSYGQI4eeZfRcJIezY2Vokpc/QLClnYozjSfqLLwK+BN4hudCMGOOPJC0Gf8jctjMwtpXjNADbkFzU9yFJG8DOmZsfIZnG7ZMQwueZbUdnHuvZEMJXwENAv8yxHgDOy9zvncznbhFjnAjsAVxIMuK9Dcn0fD924L5vkfQnv5f5035rbQ43A5sAj2Qu1mu0BfB6COEbkosCd4kxfg+zF+X49Vw8pV1JLrycQnKx299jjA82uf1ukp/DlyS9zsMy/c2QBM5tSELn7iQXff5EZvT8bJI3UZ8C/YGnm+zS0s+36f0fBo4nmdVkKslFkLs036+DInANyc9uCsnFoltnWj6aWpvk9+mcprNoZOq5i2Sk/5bM795rJH8ZkFRgwpztg5IkdV4I4URg5RjjHmnXIknZ4EizJEmS1A5DsyRJktQO2zMkSZKkdjjSLEmSJLXD0CxJkiS1o6WFAfLOEkssEfv06ZN2GZIkSSpyL7744ucxxiWbby+I0NynTx/Gjx+fdhmSJEkqciGED1rabnuGJEmS1I6sheYQwnIhhEdDCG+GEF4PIfwls/3EEEJNCOHlzMdW2apBkiRJ6g7ZbM+YCfwtxvhSCGEh4MUQQuNyq+fGGM/K4mNLkiRJ3SZroTnGOBWYmvn66xDCm0Bldx2/vr6ejz/+mO+//767Dqm51LNnT5ZddlnKy8vTLkWSJCkrcnIhYAihDzAQeA4YDBwSQtgLGE8yGv1lZ4/58ccfs9BCC9GnTx9CCN1arzouxsj06dP5+OOP6du3b9rlSJIkZUXWLwQMISwI3AkcHmP8CrgEWAkYQDISfXYr9zsghDA+hDB+2rRpP7n9+++/Z/HFFzcwpyyEwOKLL+6IvyRJKmpZDc0hhHKSwHxjjHEsQIzx0xhjQ4xxFnAFsF5L940xXh5jHBRjHLTkkj+ZKq/x+FmqXJ3hz0GSJBW7bM6eEYCrgDdjjOc02b5Mk922A17LVg3ZVlZWxoABA1hzzTXZcccd+e677+b6WH/4wx+44447ANhvv/144403Wt33scce47///e/s7y+99FKuv/76uX5sSZIktS2bPc2DgT2BCSGElzPbjgF2DSEMACIwGTgwizVkVUVFBS+//DIAu+++O5deeilHHHHE7NsbGhooKyvr9HGvvPLKNm9/7LHHWHDBBdlwww0BOOiggzr9GJIkSeq4rI00xxifijGGGONaMcYBmY/7Y4x7xhj7Z7b/PjPLRsH79a9/zTvvvMNjjz3Gpptuym677Ub//v1paGhg+PDhrLvuuqy11lpcdtllQHIB3SGHHMIaa6zB1ltvzWeffTb7WJtsssnsFRD//e9/s/baa/OLX/yCzTffnMmTJ3PppZdy7rnnMmDAAJ588klOPPFEzjormcHv5ZdfZv3112ettdZiu+2248svv5x9zKOPPpr11luPVVddlSeffDLHZ0iSJKlwFcQy2u06/HDIjPh2mwED4LzzOrTrzJkzeeCBB9hiiy0AeP7553nttdfo27cvl19+OYsssggvvPACP/zwA4MHD+a3v/0t1dXVTJw4kQkTJvDpp5+yxhprsO+++85x3GnTprH//vvzxBNP0LdvX7744gsWW2wxDjroIBZccEGOPPJIAB5++OHZ99lrr7248MIL2XjjjTnhhBM46aSTOC/zPGbOnMnzzz/P/fffz0knncRDDz3U5dMkSZJUCoojNKekrq6OAQMGAMlI8x//+Ef++9//st56682efu0///kPr7766ux+5RkzZjBp0iSeeOIJdt11V8rKyujduzebbbbZT47/7LPPstFGG80+1mKLLdZmPTNmzKC2tpaNN94YgL333psdd9xx9u3Dhg0DYJ111mHy5Mldeu6SJEmlpDhCcwdHhLtb057mphZYYIHZX8cYufDCCxkyZMgc+9x///3tzjoRY+zWmSnmm28+ILmAcebMmd12XEmSpGKX9XmaS92QIUO45JJLqK+vB+Dtt9/m22+/ZaONNuKWW26hoaGBqVOn8uijj/7kvhtssAGPP/4477//PgBffPEFAAsttBBff/31T/ZfZJFFWHTRRWf3K99www2zR50lSZI094pjpDmP7bfffkyePJm1116bGCNLLrkkVVVVbLfddjzyyCP079+fVVddtcVwu+SSS3L55ZczbNgwZs2axVJLLcWDDz7INttsww477MDdd9/NhRdeOMd9rrvuOg466CC+++47VlxxRa655ppcPVVJkqSiFWKMadfQrkGDBsXG2SQavfnmm6y++uopVaTm/HlIkqSuqqquYcy4iUypraN3rwqGD+nH0IGVOa0hhPBijHFQ8+2ONEuSJCl1VdU1jBw7gbr6BgBqausYOXYCQM6Dc0vsaZYkSVLqxoybODswN6qrb2DMuIkpVTQnQ7MkSZJSN6W2rlPbc83QLEmSpNT17lXRqe25ZmiWJElS6oYP6UdFedkc2yrKyxg+pF9KFc3JCwElSZKUusaL/dKePaM1hua5NH36dDbffHMAPvnkE8rKylhyySUBeP7555l33nnn6rhbbbUVN910E7169epSfZMnT2b11VdntdVW4/vvv2ehhRbi4IMPZu+9927zfi+//DJTpkxhq6226tLjS5IkddbQgZV5E5KbMzTPpcUXX3z2EtonnngiCy64IEceeeTs22fOnEmPHp0/vffff393lchKK61EdXU1AO+9997sRVL22WefVu/z8ssvM378eEOzJElSEyXT01xVXcPg0Y/Qd8R9DB79CFXVNd3+GH/4wx844ogj2HTTTTn66KN5/vnn2XDDDRk4cCAbbrghEycmU6Zce+21DBs2jC222IJVVlmFo446avYx+vTpw+effz57pHj//ffn5z//Ob/97W+pq0uuHn3hhRdYa6212GCDDRg+fDhrrrlmu7WtuOKKnHPOOVxwwQUALdb2448/csIJJ3DrrbcyYMAAbr311lafgyRJUikpiZHmXE6W/fbbb/PQQw9RVlbGV199xRNPPEGPHj146KGHOOaYY7jzzjuBZES3urqa+eabj379+nHooYey3HLLzXGsSZMmcfPNN3PFFVew0047ceedd7LHHnuwzz77cPnll7PhhhsyYsSIDte29tpr89ZbbwGw2mqrtVjbySefzPjx47nooosA2nwOkiRJpaIkQnNbk2V3d2jecccdKStLrvycMWMGe++9N5MmTSKEQH19/ez9Nt98cxZZZBEA1lhjDT744IOfhOa+ffsyYMAAANZZZx0mT55MbW0tX3/9NRtuuCEAu+22G//61786VFvTJdPbqq2pju4nSZJUzEqiPSOXk2UvsMACs78+/vjj2XTTTXnttde49957+f7772ffNt98883+uqysjJkzZ/7kWC3t0zT4dlZ1dTWrr756u7U11dH9JEmSillJhOa0JsueMWMGlZXJSPa1117bLcdcdNFFWWihhXj22WcBuOWWWzp0v8mTJ3PkkUdy6KGHtlnbQgstxNdffz37+2w8B0mSpEJTEqE5rcmyjzrqKEaOHMngwYNpaGho/w4ddNVVV3HAAQewwQYbEGOc3ebR3LvvvsvAgQNZffXV2WmnnTj00ENnz5zRWm2bbropb7zxxuwLAbP1HCRJkgpJ6Mqf+3Nl0KBBcfz48XNse/PNN2e3GnREVXVN3k6W3VnffPMNCy64IACjR49m6tSpnH/++anW1NmfhyRJUj4KIbwYYxzUfHtJXAgI+T1Zdmfdd999jBo1ipkzZ7LCCivYNiFJkpRlJROai8nOO+/MzjvvnHYZkiRJJaMkepolSZKkrijo0FwI/dilwJ+DJEkqdgUbmnv27Mn06dMNbCmLMTJ9+nR69uyZdimSJElZU7A9zcsuuywff/wx06ZNS7uUktezZ0+WXXbZtMuQJEnKmoINzeXl5fTt2zftMiRJUsqKaVpZ5a+CDc2SJElV1TWMHDuBuvpkAa6a2jpGjp0AYHBWtyrYnmZJkqQx4ybODsyN6uobGDNuYkoVqVgZmiVJUsGaUlvXqe3S3DI0S5KkgtW7V0WntqsAfPklPPNM2lX8hKFZkiQVrOFD+lFRXjbHtoryMoYP6ZdSRZpr06fDccdBnz6w/fZQX592RXMwNEuSpII1dGAlo4b1p7JXBQGo7FXBqGH9vQiwkEybBiNGJGH5tNPgt7+FBx6A8vK0K5uDs2dIkqSCNnRgpSG5EH3yCZx1FlxyCdTVwc47w7HHwpprpl1ZiwzNkiRJyp2aGjjzTLj8cvjxR9h9dzjmGFhttbQra5OhWZIkSdn34Ydwxhlw5ZXQ0AB77ZWE5ZVXTruyDjE0S5IkKXsmT4ZRo+Caa5Lv99kn6WEusJWdDc2SJEnqfu+8k4Tl66+HeeaB/feHo4+G5ZdPu7K5YmiWJElS95k4MZkF48YbYd554c9/hqOOgsrCvljT0CxJkpSSquoaxoybyJTaOnr3qmD4kH6FOxPIG2/AqafCLbdAz57w17/CkUfCz36WdmXdwtAsSZKUgqrqGkaOnUBdfQMANbV1jBw7AaCwgvOrryZh+Y47YP75k1HlI46ApZZKu7Ju5eImkiRJKRgzbuLswNyorr6BMeMmplRRJ1VXw7Bh8ItfwL//DSNHJhf9jR5ddIEZHGmWJElKxZTauk5tzxsvvACnnAL33guLLAJ//zv85S+w6KJpV5ZVjjRLkiSloHevik5tT90zz8CWW8J668HTTyctGR98ACeeWPSBGQzNkiRJqRg+pB8V5WVzbKsoL2P4kH4pVdSKJ56A3/wGNtwQxo9P2i8mT06WvF5kkbSryxnbMyRJklLQeLFfXs6eESM8+iicfDI8/jgsvTScdRYcdBAssEDa1aXC0CxJkpSSoQMr8yMkN4oRHnwwCctPPw29e8P55ycLk1TkadtIjtieIUmSVOpihPvvhw02gCFDkl7liy+Gd9+Fww4r+cAMhmZJkqTSFSPccw+suy5svTV88glcdlmyBPaf/5wsUiLA0CxJklR6Zs2CO++EgQNh222hthauvhomTYIDDoD55ku7wrxjT7MkSVLKcracdkNDsnLfKafA66/DqqvCddfBbrtBD2NhWxxpliRJSlHjcto1tXVE/recdlV1Tfc9yMyZcOONsOaasMsuSVvGTTfBG2/AXnsZmDvA0CxJkpSirC6nXV8P114La6wBe+wB5eVw++0wYQLsuiuUlbV7CCV8WyFJkpSirCyn/eOPcP31cPrp8P77Se/y2LFJ//I8jpnODc+aJElSirp1Oe0ffoBLLoFVVknmVl5iCbj3XnjxRdhuOwNzF3jmJEmSUtQty2nX1cGFF8JKKyVTxVVWwr//Dc89B7/7HYTQzVWXHtszJEmSUtSl5bS/+y6ZV/nMM5M5ljfaKJkNY7PNDMrdzNAsSZKUsk4vp/3NN0kbxllnwWefJSH55pthk02yVmOpMzRLkiQViq++Spa3PvtsmD49WfL6+ONh8OC0Kyt6hmZJkqR8V1sLF1wA550HX34JW20FJ5wAv/xl2pWVDEOzJElSvvriiyQon39+Msq87bbJyPI666RdWckxNEuSJOWbadPgnHPgoouS/uXtt4fjjoMBA9KurGQZmiVJkvLFp58mF/f94x/JNHI77piMLK+5ZtqVlTxDsyRJUtqmTEmmjbvssmQ1v912g2OOgdVXT7syZWRtcZMQwnIhhEdDCG+GEF4PIfwls32xEMKDIYRJmc+LZqsGSZKkvPbRR3DIIbDiikkrxi67wFtvwQ03GJjzTDZXBJwJ/C3GuDqwPnBwCGENYATwcIxxFeDhzPeSJEmlY/JkOOigZAW/yy6DPfeEt9+Ga65JlsBW3slae0aMcSowNfP11yGEN4FKYFtgk8xu1wGPAUdnqw5JkqS88e67cPrpcP31MM88sN9+cPTRsMIKaVemduSkpzmE0AcYCDwHLJ0J1MQYp4YQlspFDZIkSal5+2047TS48Ubo0QP+9Cc46ihYdtm0K1MHZT00hxAWBO4EDo8xfhU6uA56COEA4ACA5ZdfPnsFSpIkZcsbbyRh+ZZbYL754LDDYPhwWGaZtCtTJ2Wzp5kQQjlJYL4xxjg2s/nTEMIymduXAT5r6b4xxstjjINijIOWXHLJbJYpSZLUvSZMgJ13TqaKu/tu+Nvf4P33k7mXDcwFKZuzZwTgKuDNGOM5TW66B9g78/XewN3ZqkGSJCmnqqth2DBYay144AEYOTK56O/MM2HppdOuTl2QzfaMwcCewIQQwsuZbccAo4HbQgh/BD4EdsxiDZIkSdn3wgtwyilw772wyCJwwgnwl7/AYoulXZm6STZnz3gKaK2BefNsPa4kSVLOPPssnHxyMqq86KJJcD7kEOjVK+3K1M1cEVCSJKmznnwyCcgPPghLLAGjRsGf/wwLL5x2ZcoSQ7MkSVJHxAiPPZaMLD/2GCy1FIwZkyxSsuCCaVenLDM0S5IktSVGeOihJCw/9VQy+8W558IBB8D886ddnXIkq1POSZIkFawYk17lDTeE3/42mQXjoovgvffg8MMNzCXG0CxJktRUjHDPPbDeerDVVjBlClx6KbzzDhx8MPTsmXaFSoGhWZIkCWDWLBg7FtZeG7bdFr74Aq68EiZNggMPTFb0U8kyNEuSpNLW0AC33Qa/+AVsvz18+y1cdx1MnAh//CPMO2/aFSoPGJolSVJpmjkTbrwR+vdPlrxuaEi+f/NN2Gsv6OF8CfofQ7MkSSotM2cmI8lrrAF77AFlZXDLLTBhAuy2W/K91IxvoSRJUmn48Ue44QY4/fRkBowBA5Ie5m23hXkcR1Tb/A2RJEnF7YcfktkvVl0V9tsPFlssmR3jpZdgu+0MzOoQf0skSVJx+v77ZF7llVaCP/0pWZTkgQfg+edhm20ghLQrVAGxPUOSJBWX776Dyy5LlrieOhV+9Su49lrYfHODsuaaoVmSJBWHb76BSy6Bs86Czz6DTTaBm26CjTc2LKvLDM2SJKmwffUVXHwxnH02TJ+eLHl9/PHJCLPUTQzNkiSpMNXWwoUXwrnnwpdfJkteH388rL9+2pWpCBmaJUlSYfniCzj//ORjxgz4/e+TsDxoUNqVqYgZmiVJUmH4/PNkVPnCC+Hrr2HYMDjuOBg4MO3KVAIMzZIkKb99+mnSr/yPfyQzY+y4YxKW+/dPuzKVEEOzJEnKT1OnJtPGXXppskDJLrskYXn11dOuTCXI0CxJkvLLxx/DGWfAFVfAzJmwxx5wzDHJin5SSgzNkiQpP3zwQRKWr7oKZs2CvfeGkSOTFf2klBmaJUlSut57D0aNSlbtCwH22ScJy336pF1ZSamqrmHMuIlMqa2jd68Khg/px9CBlWmXlTcMzZIkKR2TJsHpp8MNN0CPHnDQQXDUUbDccmlXVnKqqmsYOXYCdfUNANTU1jFy7AQAg3PGPGkXIEmSSsxbb8Gee8Jqq8Gtt8KhhyajzRdeaGBOyZhxE2cH5kZ19Q2MGTcxpYryjyPNkiQpN157DU49FW67DSoq4Igj4MgjYeml066s5E2prevU9lLkSLMkScqul1+GHXZI5lW+7z4YMQImT06mkzMw54XevSo6tb0UGZoLTFV1DYNHP0LfEfcxePQjVFXXpF2SJEkte/FFGDo0WbHvoYfghBOSGTJOPx2WXDLt6tTE8CH9qCgvm2NbRXkZw4f0S6mi/GN7RgGxSV+SVBCefRZOOQXuvx8WXRROOgkOOwx69Uq7MrWiMUc4e0brDM0FpK0mfX+pJUmpe+opOPlkePBBWHzxZET54INh4YXTrkwdMHRgpXmiDYbmAmKTviQp78QIjz+ehOVHH03aLs48E/70J1hwwbSrk7qNPc0FxCZ9SVLeiDHpU954Y9h0U3jzTTjnnOQCv+HDDcwqOobmAmKTviQpdTHCAw/AhhvCb37zv/mV33sP/vpXmH/+tCuUssL2jAJik74kKTUxwr/+lVzg98ILsPzycMklyZLX882XdnVS1hmaC4xN+pKknJo1C+6+OwnL1dXQty9ccQXstRfMO2/a1Uk5Y2iWJEk/1dAAd96ZrOA3YQKsvDJccw3svjuUl6dd3Ryqqmv8K6yyztAsSZL+p6EBbr01CctvvgmrrQb//CfsvDP0yL/Y4BoGyhUvBJQkSTBzJlx/PayxRjKaPM88cMst8Npryfd5GJih7TUMpO5kaJYkqZTV18PVV0O/frD33lBRAXfcAa++mowul5W1f4wUuYaBcsXQLElSKfrhB7j8clh1VfjjH5Plru++O7nYb/vtk5HmAuAaBsqVwvgXIUmSusf338PFFycX9h14ICy9NNx3XzKN3O9/DyGkXWGnuIaBciU/G5QkSVL3qqtLRpbPOAOmToXBg5O2jP/7v4ILyk25hoFyxdAsSVIx+/ZbuPRSGDMGPv0UNtkEbrwx+VzAYbkp1zBQLhiaJUkqRl9/nbRhnH02fP55MqJ8++3w61+nXZlUkAzNkiQVkxkz4MIL4dxz4YsvYMst4fjjYYMN0q5MKmiGZkmSisEXX8D55ycfM2bANtskYXndddOuTCoKhmZJkgrZ558no8oXXpi0ZAwdCiecAAMHpl2ZVFQMzZIkFaLPPkv6lS++GL77DnbcEY47Dvr3T7syqSgZmiVJKiRTp8JZZ8EllyQLlOyyCxx7bLL8taSsMTRLklQIPv4YzjwTrrgCfvwR9tgDjjkmWf5aUtYZmiVJymcffgijR8NVV8GsWbDXXjByZLKin6ScMTRLkpSP3n8fRo2Ca69Nvt93XxgxAvr0SbMqqWQZmiVJyifvvAOnnw7XXw9lZXDAAXD00bDccmlXJpU0Q7MkSflg4kQ47bRkiet554VDDoHhw6HS5aGlfGBoliQpTa+/DqeeCrfeChUVcMQR8Le/wc9+lnZlkpowNEuSlIZXXknC8h13wIILJi0YRxwBSy6ZdmWSWmBoliQpl156CU45BaqqYOGFk6Wu//IXWHzxtCuT1AZDsyRJufD880lY/te/oFcvOOkkOOyw5GtJec/QLElqVVV1DWPGTWRKbR29e1UwfEg/hg70wrRO+e9/4eSTYdw4WGyx5GK/Qw5JRpklFQxDsySpRVXVNYwcO4G6+gYAamrrGDl2AoDBuSMefzwJy488kvQpn3EG/OlPsNBCaVcmaS7Mk3YBkqT8NGbcxNmBuVFdfQNjxk1MqaICECM8/DBsvDFssgm88Qacc06yUMlRRxmYpQLmSLMkqUVTaus6tb2kxZi0X5x8MjzzDPTuDRdcAPvtl0wjJ6ngOdIsSWpR714th73WtpekGJML+375S9hyS/j4Y/jHP+Ddd+HQQw3MUhExNEuSWjR8SD8qysvm2FZRXsbwIf1SqiiPzJqVTBm3zjqwzTYwbRpcfnmyBPaf/gQ9e6ZdoaRuZnuGJKlFjRf7OXtGE7NmwZ13JouSvPoqrLQSXH017LEHlJenXZ2kLMpaaA4hXA38DvgsxrhmZtuJwP7AtMxux8QY789WDZKkrhk6sLK0Q3Kjhga47bYkLL/xBvTrBzfcALvsAj0cf5JKQTbbM64Ftmhh+7kxxgGZDwOzJCl/zZyZhOM11oDddku23XwzvP56MrpsYJZKRtZCc4zxCeCLbB1fkqSsqa+Ha66B1VaDvfZKepRvvx0mTEhGl8vK2j+GpKKSxoWAh4QQXg0hXB1CWLS1nUIIB4QQxocQxk+bNq213SRJ6j4//ghXXAGrrgr77guLLAJ33QXV1bDDDjCP189LpSrX//ovAVYCBgBTgbNb2zHGeHmMcVCMcdCSSy6Zo/IkSSXp+++TqeJWXhkOOACWWiqZSm78eBg61LAsKbezZ8QYP238OoRwBfCvXD6+JBWCquoaZ6zIlbq6ZGT5jDNgyhTYcEO48kr4zW8ghLSrk5RHchqaQwjLxBinZr7dDngtl48vSfmuqrqGkWMnzF6+uqa2jpFjJwAYnLvTt9/CZZfBmWfCp58my15ffz1stplhWVKLsjnl3M3AJsASIYSPgb8Dm4QQBgARmAwcmK3Hl6RCNGbcxNmBuVFdfQNjxk00NHeHr7+GSy6Bs85KFiTZfHO49dYkNEtSG7IWmmOMu7aw+apsPZ4kFYMptXWd2q4OmjEDLroIzjkHvvgCttgCjj8+aceQpA5wgklJyiO9e1VQ00JA7t2rIoVqisCXX8L55ycftbXwu98lYXm99dKuTFKB8XJgScojw4f0o6J8zjmAK8rLGD6kX0oVFajp0+G442CFFeCkk2CTTeDFF+Heew3MkuaKI82SlEca+5adPWMuffZZ0oJx0UXw3Xew/fZJeP7FL9KuTFKBMzRLUp4ZOrDSkNxZn3wCY8bApZcm08jtsgsceyz8/OdpVyapSBiaJUmFq6YmmTbu8suT1fx23x2OOSZZ/lqSupGhWZJUeD78MFmQ5MoroaEB9torCcsrr5x2ZZKKlKFZklQ43n8fRo+Ga65Jvt9nHxgxAvr2TbcuSUXP0CxJyn/vvAOnn56s2ldWBvvvD0cfDcsvn3ZlkkqEoVmSlL8mToTTToMbb4R554WDD4ajjoJKL5SUlFuGZklS/nn99SQs33ILVFTA4YfDkUfCMsukXZmkEmVoliTlj1dfhVNPhTvugPnnT0aVjzgClloq7coklThDs0peVXWNC0lIaXvpJTjlFKiqgoUXTuZYPvxwWHzxtCuTJMDQrBJXVV3DyLETqKtvAKCmto6RYycAGJylXHj++SQs/+tf0KsXnHgiHHYYLLpo2pVJ0hzmSbsAKU1jxk2cHZgb1dU3MGbcxJQqkkrEM8/AllvCL38J//1v0pIxeTL8/e8GZkl5yZFmlbQptXWd2i7NDVuAmnjiiWRk+aGHYMklkzmX//xnWGihtCuTpDYZmlXSeveqoKaFgNy7V0UK1agY2QIExAiPPgonnwyPPw5LLw1nnw0HHggLLJB2dZLUIbZnqKQNH9KPivKyObZVlJcxfEi/lCpSsSnpFqAYYdw4+PWvYfPNYdIkOP/8ZFW/I44wMEsqKI40q6Q1jvT5p3NlS0m2AMUI99+fjCw//zwstxxcfDHsuy/07Jl2dZI0VwzNKnlDB1YakpU1JdUCFCPcc08Sll96CVZYAS69FP7wB5hvvrSrk6QusT1DkrKoJFqAZs2CO++EgQNh6FCYMQOuvjppxzjwQAOzpKLgSLMkZVFRtwA1NMDttyfTxb3+Oqy6Klx/Pey6K/TwvxdJxcVXNUnKsqJrAZo5E265JQnLEyfC6qvDjTfCzjtDWVn795ekAmR7hiSpY+rr4dprk5C8554w77xw223w2muw224GZklFzZFmSVLbfvwxabs4/fRkuriBA+Guu+D3v4d5HHuRVBp8tZMkteyHH+CSS2CVVWD//WGJJZLZMV58Mbngz8AsqYQ40ixJmlNdHVx5JZxxBtTUwAYbwGWXwZAhEELa1UlSKgzNkqTEd98l4fjMM+GTT2CjjeC662CzzQzLkkqeoVmSSt0338A//gFnnw2ffZYseX3LLbDxxmlXJkl5w9AsSaXqq6/goovgnHNg+vSk/eL442Hw4LQrk6S8Y2iWpFJTWwsXXADnnpt8vfXWSVj+5S/TrkyS8pahWZJKxfTpcN55SWD+6ivYdtskLK+zTtqVSVLeMzRLUrGbNi1pwbjooqR/eYcd4Ljj4Be/SLsySSoYhmZJKlaffAJnnZXMtVxXlyxzfeyxsOaaaVcmSQXH0CxJxWbKlGTauMsuS1bz2203OOaYZPlrSdJcMTRLOVJVXcOYcROZUltH714VDB/Sj6EDK9MuS8Xko4+SBUmuvBJmzoQ990zC8iqrpF2ZJBU8Q7OUA1XVNYwcO4G6+gYAamrrGDl2AoDBWV03eTKMGgXXXAMxwj77wIgRsOKKaVfWKb6xlJTPDM1SRjb/wx4zbuLswNyorr6BMeMmGgo09959F04/Ha6/HuaZB/bbD44+GlZYIe3KOs03lpLynaFZJaW1YJzt/7Cn1NZ1arvUprffhtNOgxtvhB494E9/gqOOgmWXTbuyueYbS0n5ztCsDiv0P522FYyz/R92714V1LQQkHv3qujysVVC3ngjCcu33ALzzQeHHQbDh8Myy6RdWZf5xlJSvpsn7QJUGBoDZ01tHZH/Bc6q6pq0S+uwtoJxtv/DHj6kHxXlZXNsqygvY/iQft1yfBW5V1+FnXZKpoq7+27429/g/feTuZeLIDBD628gfWMpKV8YmtUhbQXOQtFWMM72f9hDB1Yyalh/KntVEIDKXhWMGta/oEbqlYLqahg2LFmE5N//hpEjk4v+zjwTll467eq6lW8sJeU72zPUIcXwp9O2WiSGD+k3R+sGdP9/2EMHVhqS1TEvvACnnAL33guLLAJ//3vSirHYYmlXljWN/zYKuQVMUnEzNKtDiqEnt61g7H/YygvPPJOE5QceSALyKafAoYcmwbkE+MZSUj4zNKtDcjESm23tBWP/w1ZqnnwyCcgPPghLLAGjR8Of/wwLLZR2ZZKkDEOzOqRYRmINxsobMcJjj8HJJyefl14azjoLDjoIFlgg7eokSc0YmtVhBk6pG8QIDz2UhOWnnkpmvzj3XDjgAJh//rSrkyS1wtAsSbkQY9KrfPLJ8NxzyUIkF18M++4LPXumXZ0kqR0dmnIuhPBwR7ZJkpqJEe65B9ZdF7beGj75BC67DN55J+lbNjBLUkFoc6Q5hNATmB9YIoSwKBAyNy0M9M5ybZJUuGbNgrvuSi7we+UVWHFFuPJK2GsvKC9PuzpJUie1155xIHA4SUB+qcn2r4CLs1STJBWuhga4444kLL/+OqyyClx3Hey2G/SwI06SClWbr+AxxvOB80MIh8YYL8xRTZJUeGbOhFtvhVNPhbfegtVXhxtvhJ13hrKy9u8vScpr7bVnbBZjfASoCSEMa357jHFs1iqTpEJQXw833QSnnQaTJsGaaybheYcdYJ4OXTYiSSoA7f2tcCPgEWCbFm6LgKFZUmn68Ue44QY4/XR47z0YMADGjoVttzUsS1IRai80f5n5fFWM8alsFyNJee+HH+Caa2DUKPjwQxg0CM47D373Owih3btLkgpTe8Mh+2Q+X5DtQiQpr33/PVx0Eay0EvzpT9C7N9x3Hzz/PGyzjYFZkopceyPNb4YQJgNLhhBebbI9ADHGuFbWKpOkfPDdd3D55XDmmTB1Kvz613DttbD55gZlSSoh7c2esWsI4WfAOOD3uSlJkvLAN9/AJZfAWWfBZ5/BZpvBzTfDxhunXZkkKQXtThoaY/wE+EUIYV5g1czmiTHG+qxWJklp+OqrZHnrs8+G6dPhN7+BE06AX/0q7cokSSnq0Ez7IYSNgeuBySStGcuFEPaOMT6RxdokKXdqa+HCC+Hcc+HLL2GrreD442H99dOuTJKUBzq6PNU5wG9jjBMBQgirAjcD62SrMEnKiS++gPPPTz5mzEimjDvuuGRWDEmSMjoamssbAzNAjPHtEEJ5lmqSpOz7/HM455xkRoyvv4Zhw5KR5QED0q5MkpSHOhqaXwwhXAXckPl+d+DF7JQkSVn06afJxX2XXJLMjLHTTsnI8pprpl2ZJCmPdTQ0HwQcDBxG0tP8BPCPbBUlSd1uyhQYMwYuuyxZoGTXXeHYY2H11dOuTJJUANoNzSGEeYAXY4xrkvQ2S1Lh+OgjOOMMuPJKmDkTdt89Ccurrtr+fbtJVXUNY8ZNZEptHb17VTB8SD+GDqzM2eNLkrquvRUBiTHOAl4JISzfmQOHEK4OIXwWQnitybbFQggPhhAmZT4vOhc1S1L7Jk+Ggw5KVvC77DLYc094+2247rqcB+aRYydQU1tHBGpq6xg5dgJV1TU5q0GS1HXthuaMZYDXQwgPhxDuafxo5z7XAls02zYCeDjGuArwcOZ7Seo+774L++0Hq6wCV18Nf/wjvPMOXHEFrLhizssZM24idfUNc2yrq29gzLiJrdxDkpSPOtrTfFJnDxxjfCKE0KfZ5m2BTTJfXwc8Bhzd2WNL0k+8/Tacfjr885/QowcceCAcfTQst1yqZU2prevUdklSfmozNIcQepJcBLgyMAG4KsY4swuPt3SMcSpAjHFqCGGpLhxLkuDNN+G005IlruebDw49FIYPh969064MgN69KqhpISD37lWRQjWSpLnVXnvGdcAgksC8JXB21ivKCCEcEEIYH0IYP23atFw9rKRCMWEC7Lwz/PzncNdd8Le/wfvvJyv65UlgBhg+pB8V5WVzbKsoL2P4kH4pVSRJmhvttWesEWPsD5CZp/n5Lj7epyGEZTKjzMsAn7W2Y4zxcuBygEGDBsUuPq6kYvHyy3DKKTB2LCy4YNKCccQRsOSSaVfWosZZMpw9Q5IKW3uhub7xixjjzBBCVx/vHmBvYHTm891dPaCkEjF+fBKW77kHFlkkWb3v8MNhscXSrqxdQwdWGpIlqcC1F5p/EUL4KvN1ACoy3wcgxhgXbu2OIYSbSS76WyKE8DHwd5KwfFsI4Y/Ah8COXaxfUrF79tkkLN9/Pyy6KJx8ctK33KtX2pVJkkpIm6E5xljW1u3t3HfXVm7afG6PKamEPPVUEpAffBAWXzyZGePgg2HhVt+rS5KUNR2dck6Ssi9GePzxJCw/+igstVSy9PVBByX9y5IkpcTQLCl9McLDDydh+ckn4Wc/S2bBOOAAmH/+tKuTJKnDKwJKUveLER54ADbcEH7zG3jvPbjgguTz4YcbmCVJecPQLCn3YkxmwVhvPdhqK5gyBS69NFkC+9BDocKFPyRJ+cXQLCl3Zs1K5ldee23YdluYPh2uvBImTUqWvZ5vvrQrlCSpRfY0Z1FVdY0LGkgADQ1w553J1HGvvQYrrwzXXgu77Qbl5WlXJ0lSuwzNWVJVXcPIsROoq28AoKa2jpFjJwAYnFU6Zs6EW2+F006DN9+E1VaDf/4zWf66hy8/c8s35JKUe7ZnZMmYcRNnB+ZGdfUNjBk3MaWKpByaOROuvx7WWAP22APKyuCWW5JR5t13NzB3QeMb8praOiL/e0NeVV2TdmmSVNQMzVkypbauU9ulovDjj3DVVdCvH+y9dzL7xZ13wiuvJKPLZXO9XpIyfEMuSekwNGdJ714tX/3f2napoP3wA1x2Gay6Kuy3X7Lc9d13Q3U1DBsG8/hS0118Qy5J6fB/siwZPqQfFeVzjqpVlJcxfEi/lCqSsuD77+Hii5ML+w46KFmU5P774YUX4Pe/hxDSrrDo+IZcktJhaM6SoQMrGTWsP5W9KghAZa8KRg3r78U6Kg7ffQfnnQcrrgiHHAIrrAD/+Q888wxsuaVhOYt8Qy5J6fBqnCwaOrDSkKzi8u23cMklMGYMfPYZbLIJ3Hhj8tmgnBONrynOniFJuWVoltS+r79O2jDOPhs+/xz+7//g+ONho43Srqwk+YZcknLP0CypdTNmwIUXwrnnwhdfwBZbJGF5ww3TrkySpJwyNEv6qS+/hPPPT/qWZ8yAbbZJwvK666ZdmSRJqTA0S/qfzz9PRpUvvDBpyRg2DI47DgYOTLsySZJSZWiWlFzUd/bZSd/yd9/BDjskI8v9+6ddmSRJecHQLJWyqVPhrLOSGTF++AF23RWOPRZWXz3tyiRJyiuGZqkU1dTAmWfC5ZdDfT3ssQccc0yyop8kSfoJQ7NUSj78EEaPhquuglmzYK+9krC80kppVyZJUl4zNEul4P33YdQouPba5Pt994URI6BPnzSrkiSpYBiapWI2aRKcfjrccAP06AEHHABHHw3LLZd2Zd2qqrrGFfIkSVllaJaK0VtvwWmnwU03wbzzwiGHwFFHQe/eaVfW7aqqaxg5dgJ19Q0A1NTWMXLsBACDsySp28yTdgGSutHrryczYKyxBowdC0cckbRmnHdeUQZmgDHjJs4OzI3q6hsYM25iShVJkoqRI81SMXjlFTjlFLjzTlhwwaQF44gjYMklW9y9mNoZptTWdWq7JElzw9AsFbIXX0zC8t13w8ILJ6v3HX44LL54q3cptnaG3r0qqGkhIPfuVZFCNZKkYmV7hlSInnsOfvc7GDQIHn8cTjoJPvggCdBtBGYovnaG4UP6UVFeNse2ivIyhg/pl1JFkqRi5EizVEiefppPjzyWpZ99nC97LsRtv92XyuOG87tfr9bhQxRbO0Pj6HixtJtIkvKToVkqBI8/DiefDI88Qo/5F2HUJn/gnwO24tv55qfiP5OZueBCHQ6JxdjOMHRgpSFZkpRVtmdI+SpGePhh2Hhj2GQTeOMNLtjqIH514FVc9ssd+Ha++YHOt1bYziBJUucZmqV8EyP8+9/wq1/B//0fvPsuXHABvPce5/b/HXXz9vzJXTrTWjF0YCWjhvWnslcFAajsVcGoYf0dqZUkqQ22Z0j5Ika4776kDeOFF2D55eGSS2CffWC++YDua62wnUGSpM4xNOeRYpo7N5/l3XmeNQvuuScJy9XV0LcvXHEF7LVXsppfE8OH9JtjujiwtUKSpFwwNOeJYps7N1/l1XmeNStZjOTUU+HVV2HlleGaa2D33aG8vMW7OFOEJEnpMDTnibbmzjUQdZ+8OM8NDXDbbUlYfuMN6NcPbrgBdtkFerT/T9LWCkmScs/QnCeKbe7cfJXqeZ45E26+OQnLb78NP/853HIL7LADlJW1f/8ClHetMJIkzSVnz8gTrV3IVchz5+ajVM5zfT1cfTWstlrSp9yzJ9xxR9KSsfPORR2YR46dQE1tHZH/tcJUVdekXZokSZ1maM4Tzp2bGzk9zz/8AJdfDquuCn/8IyyyCFRVJRf7bb89zFPc//yKbbluSVJpK+7/tQvI0IGVbL9OJWUhAFAWAtuvY+9qd8vJHMXffw//+AessgoceCBfLNiLI/c+jb7/dxKD31yIqlemdt9j5TFbjiRJxcSe5jxRVV3DnS/W0BAjAA0xcueLNQxaYbGiDM6d6XXt7r7YrF1IV1eXjCyfeSZMmQIbbsjTI0az35RFqZs5C8ivWVGy3W9cjMt1S5JKlyPNeaKU/pTdmV7XguiL/fZbOPvsZH7lww9Ppo57+GF46imO+upnswNzozR+rlXVNQwe/Qh9R9zH4NGPcFzVhKyfV1uOJEnFxNCcJ7L5p+zmgSntwNmZNwh5/Wbi66/hjDOgTx848khYc0147DF4/HHYbDMIIS9aFFp643Hjsx9m/by6XLckqZjYnpEnsvWn7LxazCOjM0FybkNnVlsPZsyAiy6Cc86BL76ALbaA44+HDTf8ya750KLQ0huP2Mq+3R3mnVNaklQsHGnOE9n6U3Y+jtR2Ztq3uZkiLmstHV9+CSeemIwsH3dcEpKfew4eeKDFwAz50aLQmSBsv7EkSS0zNOeJbP0pOx/aA5rrTJCcm9DZ7W8Upk9PQnKfPnDSSbDJJvDii3DvvbDeem3eNR9aFFoLwqHZ9/YbS5LUOtsz8kg2/pSdD+0BzTU+x460T3Rm30bd9kbhs8+SFoyLL04u9tthhyQ8r7VWpw6TdovC8CH95mjRgSQgb79OJY++Nc3V+iRJ6gBDc5FrLTClPaLYmSDZ2dDZ5TcKn3wCZ50Fl1ySzLm8885w7LHJstcFaG7eeEiSpDkZmotcKQamuX6jUFOTzLF8+eXJ0te77w7HHAP9Cr9lIe3RbkmSCp2huQSUWmDq9BuFDz9Mpo678kqYNQv22gtGjkzmW5YkScLQrCLVoTcK778Po0fDNdck3++zD4wYkSxSIkmS1IShuQRle/nkvPfOO3D66XD99VBWBgccAEcfDcstl3ZlkiQpTxmaS0w+LnaSMxMnwmmnwY03wrzzwiGHwPDhUFnkz1uSJHWZ8zSXmHxc7CTrXn8ddt0VVl8d7rwT/vrXpDXjvPMMzJIkqUMcaS4x+bjYSda88gqceirccQcssAAcdRQccQQstVTalUmSpALjSHOJmZtlqQvOSy/BdtvBgAHwn/8wcb+/sOVfb6Avv2bw1a91fTltSZJUcgzNraiqrmHw6EfoO+I+Bo9+pGiC1twsS10wnn8ettkG1lkHHnsMTjyR++75L0N/tgVv1s9L5H893MXy85QkSblhaG5B48VyNbV1RRe0hg6sZNSw/lT2qiAAlb0qGDWsf2FfBPjf/8IWW8Avf5l8feqpMHky/P3vnP7Mp6XXwy1JkrqdPc0taOtiuYIOlxlFs9jJE0/AySfDww/DEkskcy7/+c+w0EKzdympHm5JkpQ1huYWFHLQKvo5mGOERx9NwvLjj8PSS8NZZ8FBByUX+zXTu1cFNS383Iqqh1uSJGWdobkFhRq0cjkHc87DeYzw4INJWH76aejdG84/H/bbD+afv9W7DR/Sb45zAvnZw130b3ayzPMnSco2Q3MLCiVoNddaW8lJ974++/buCBU5XSAlRrj/fjjlFHjuuWTVvosvhn33hZ492717Yz35HKhKesGZbuD5kyTlQogxpl1DuwYNGhTHjx+f08csxJGrviPuo7WfZnlZoL7hf7dWlJfN9QWAg0c/0uJIfGWvCp4esVmnj9eiGOGee5KR5Zdegj594JhjYO+9k9X8ikhOzmcR8/xJkrpTCOHFGOOg5tsdaW5FIV4s12v+cr78rr7F25oGZujahY1Z7fmeNQvGjk1mwHjlFVhpJbj6athjDygv7/rxuygbb6YKuYc+H3j+JEm5YGhuQyGNNldV1/DN9zM7dZ+5DRVZ6fluaIDbb0/C8uuvQ79+cMMNsMsu0CM/fk2z1QZQqD30+cLzJ0nKhVTmaQ4hTA4hTAghvBxCyG3fRQcV2lzNY8ZNpH5W51pt5jZUdOsCKTNnwj//CT//Oey6a9KWcfPNSXDeY4+8CczQ9lSEXVHUC87kgOdPkpQLaS5usmmMcUBLPSP5IFsBKVvaGjUunydQXhbm2NaVUNEtC6TU18M118Bqq8GeezJpRj0HbzuCX+1+PlX9fg1lZe0fI8ey1QZQlAvO5JDnT5KUC/kzjJdnCq1PsrU/UZeFwJgdfwF07wwSc93z/eOPcN11cPrpMHkytav157gdj+e+vusSwzzw1Q95O/NBNtsACrGHPp94/iRJ2ZZWaI7Af0IIEbgsxnh5SnW0qtD6JFubJq/piFu2Q0WbPeDff59c0Dd6NHz0Eay7Llx0EVu/2pOaGd/PcZx8XX2xUKcilCRJXZdWe8bgGOPawJbAwSGEjZrvEEI4IIQwPoQwftq0aTkvsND6JNP+E3VrPeD3PPMOXHBBMgvGwQfDssvCAw8kcy5vvTVTmgXmRvk4op/2OZYkSelJZaQ5xjgl8/mzEMJdwHrAE832uRy4HJJ5mnNdYyEsitFcmn+ibt4DXvHj9+z2/AMMPm8sfPMlbLQRXH89bLYZhP/1VxfaiL5tAJIklaach+YQwgLAPDHGrzNf/xY4Odd1dIQBqeMaR4YX+OE79qy+n/1euIslvpvB0yusxeB/3QUbb9zi/Wx5kCRJhSCNkealgbtCMtrYA7gpxvjvFOrIuUKa97mzVunZwG8euYP9Xqhi0e+/5vG+a3PBhrvwyZrr8HQrgRk6P6JfzOdQkiTlr5yH5hjje8Avcv24acvWwhipq62F88/nX+ecy7xfz+Dhldblwg134eXeSU/4qA6MGHd0RL9oz6EkScp7ac7TXFIKbd7ndk2fDscfDyusACeeyLybbcKj/7yfE/YbzSu9+2XlIrmiO4eSJKlgOE9zjhTavM+tmjYNzjkHLroIvvkGtt8ejjsOBgxgU+Dpdu7elfaKojmHkiSp4Biac6TQZon4iU8/hbPOgn/8A+rqYOed4dhjYc01O3T3quoaTrr3db78rn72tqbtFdB+X3PBn0NJklSwDM1Z0nxEddPVluTOF2sKb5aIKVPgzDPhssuS1fx22y0Jy6ut1uFDNO9FbqquvoET73mdH2bOardXOVszbXhxoSRJao89zVnQ0kIfd75Yw/brVBbOwhgffQSHHAIrrpi0Yuy6K7z1FtxwQ6cCM7Tci9xUbV19h3qVs7G4SGuLslRV18z1MSVJUvFxpDkLWrtg7dG3pvH0iM1SqqqDJk9Olrq++urk+332gREjoG/fuT7k3PYct3S/1mbamNvR4rYuLszbNzSSJCnnDM1ZUJAXrL37Lpx+erJq3zzzwP77w9FHw/LLz/UhG4NsW8s5VpSX0bN8njl6nRt1tFe5K1PRFeTPSpIk5ZztGVnQWtjLywvWJk6EvfeGfv3gppvgz3+G996Diy/ucmBubHtoTa+KckYN68/ft/k5FeVlc9zWmV7lrkxFl8bPqqq6hsGjH6HviPsYPPoRW0EkSSoAhuYsGD6kX5dCYE688UZyUd8aa8Dtt8Nf/pKE5fPPh8pkdLYr4a6tPubKXhXssf7yLDBfD/5668uMGTexS/3eXRktzvXPyh5qSZIKk+0ZrejKjAqdXRo6p159FU49Fe64A+afH4YPhyOOgKWWmmO3rq6+11pgDfx0FozGCyXn9qK+rkxFl+uflT3UkiQVJkNzC7pjueaOLg2dMy+9BKecAlVVsNBCcMwxcPjhsMQSLe7e1XDXVpDt7uDY1anocvmzsodakqTCZHtGC4pqueYXXoBttoF11oFHH4W//x0++CAZbW4lMEPXw11bbQ/dHRyzMRVdthRUv7skSZrNkeYWFMVo4DPP8OmRx7D0fx+jtueC3Pabfeh93HB+t9HqHbp7V1ffa6vtYcy4id2+sl/ejey3IlsLtEiSpOwyNLegoJdrfvJJOPlkeOghyudfmDM23pvrB27Nt/PNT8WDHzBzoYU7FC67I9y1FmRLOTjmdb+7JElqlaG5BXMT6lJdijlGeOyxJCw/9hgsvTQXbnUg/+j3G+rm7Tl7t870DWcz3JV6cCyUUXFJkvQ/Ica2lp7ID4MGDYrjx4/P6WN2JgQ3v3AQkpCd9b7aGOGhh5Kw/NRTsMwyyYIk++9P35MfbXFRkQC8P3rr7NUkSZJUwEIIL8YYBzXf7khzKzozGtiR2SC6dSQ6RnjggSQsP/ccLLssXHQR/PGP0DMZWU6jxSTV0XZJkqQscvaMbtDehYPdtqBFjHDPPbDuurD11vDJJ3DZZfDOO3DwwbMDM7hohyRJUncyNHeD9qYRa20k+m+3vdKx1fZmzYKxY2HttWHbbeHLL+Gqq2DSJDjgAJhvvp/cJdfTsBXVNH2SJEnN2J7RDdq7cLC1keiGTD95TW0dw29/BWi2eEpDQ7Jy36mnwmuvwSqrwHXXJctf9+jRbjuEi3ZIkiR1D0eau0Fbo7pV1TXME0K7x6ifFTnxnteTb2bOhBtvhDXXhF12ScLzjTfCm2/CXnvNDsz51A7hoh2SJKmYOdLcTVoa1W0Mtg0dnKHkm2/qkpHk005LWi/694fbboPtt4d55nx/091LUXdVKc+9LEmSip+hOYtaCrYtKW+oZ9hrj3DwM7fBjE9h4MCkh3nbbX8SlhvlWztEqc+9LEmSipuhOYvaC7DzzqxnxwkP8qdnb2fZr6bxWuWq8M8rk5kx2mnpyMdVC120Q5IkFStDcxu6Ou9wa8H2Z+Wz2Oq5+9j/mTtY5pvpvNS7HydseQi/P2pf1lx72Q4d23YISZKk3DE0t6L5Kn+NF9oBHQ7OzYNtz/rv+cOE/3B4dRU9P/+Ml/v056itDue9ARswfIvVOhXIbYeQJEnKHZfRbsXg0Y+0OEpc2auCp0ds1uHjVFXXcNE9L7P5Y3dy4Pi7WOybWthsMzjhBNh4426sWJIkSV3lMtqd1C0X2n31FUP/fT1DLzwbpk+H3/4Wjj8efvWrbqpSkiRJuWBobkWXLrSrrYULLoDzzktW79tqqyQsr79+t9cpSZKk7HNxk1YMH9KPivKyOba1e6HdF18kbRcrrAB//ztstBG88ALcd1/WA3NVdQ2DRz/SsWW5JUmS1CmONLeiUxfaff45nHMOXHghfPNNshjJccfBgAHtPk5XZ+hoPEZXL1qUJElS6wzNbWh33uFPP4WzzoJLLoHvvoOddoJjj01W8uuA7gq7+bY6oCRJUrGxPWNuTJkCf/0r9O2bjDAPHQqvvw633NLhwAxth91OlZNnqwNKkiQVG0eaO+Ojj+DMM+GKK2DmTNhzTzjmGFhllbk6XHeF3XxcHVCSJKmYONLcEZMnw0EHwUorwaWXwh57wNtvwzXXzHVghtZDbWfD7lxdtChJkqQOMzS35d13mbzdrsxcaWV+uPIqxq69JeOqnoIrr4QVV+zy4bsr7A4dWMmoYf2p7FVBIFmAZdSw/vYzS5IkdRPbM1pz/fXM2ndflmEebhi4FZettz2fLLwEFc9+yajeNQwdWNnlmS+6cynsdi9alCRJ0lwzNLdm4425bf2hnD1gW6YtuNjszU0v1OuOmS8Mu5IkSfnP0NyaFVZg5K/2IbZw05Taui5P89Z0lLrX/OXECDPq6uf4uisjz5IkSeo+huY2tDUrRUvbgVa3N9V8fuYvv6uffVvTr12kRJIkKT94IWAb2rpQryyEFu/T2vamWhqlbs3czNssSZKk7mVobkNbs1I0xJYaN2h1e1OdnYfZRUokSZLSZXtGO1q7UK+ylRaNyg7MsdxWe0dr+0uSJCk9jjTPpa7MsdzSfVvjIiWSJEnpc6R5LnVljuXm93X2DEmSpPwWYgd6cNM2aNCgOH78+LTLKAhdXXBFkiSplIUQXowxDmq+3ZHmItJ8KjunrJMkSeoehuY815mR464uuCJJkqSWGZrzWGdHjlubms4p6yRJkrrG2TPyWFsjxy1pbWo6p6yTJEnqGkNzHuvsyHFXpsGbG1XVNQwe/Qh9R9zH4NGPUFVdk5XHkSRJSpvtGVnS2ItcU1tHWQg0xEhlJ2ezaG0RlNZGjrsyDV5nedGhJEkqJYbmLGgeKBuX1u5ssBw+pB/Db3+F+ln/mxawfJ7Q5shxaysYdjcvOpQkSaXE9owsaClQNmqrJ7lFoZ3vU+JFh5IkqZQYmrOgveDY0WA5ZtxE6hvmXHymviF2LnRniRcdSpKkUmJozoL2gmOEDl04l8+jubm+6FCSJClN9jR3s6rqGr77cWa7+3Wkv7mzFwI2Pn4uLgTM5UWHkiRJaTM0d0BHg+hxVRO48dkPic22B/jJNmj/wrnhQ/rNcUEhtD2am+sZLXJ10aEkSVLabM9oR2MQramtI/K/INq8taKquqbFwAzJyHBr1+/V1Na12qoxdGAlo4b1pzJz/8peFYwa1n+ultGWJEnS3HOkuR0dnVptzLiJLQZmYPYIdUutFtD2iHBnRnPzuQdakiSpkDnS3I6OBtG2gmljS0fzC+ea6o4RYWe0kCRJyg5Dczs6GkRb2y/A7B7oxlaL1nR1RLi9GS1c9lqSJGnuGJrb0dGp1VraLwC7r7/87PaKoQMreXrEZq0G566OCLfVA93R3mxJkiT9lD3N7Wg6tVpNbR1lIczRStE0EDfu19jDvOlqS/LoW9PoO+K+OWbd6OysGJ2tt6Ue6FJe9jpX0/BJkqTiZWjugMaA1d50bk0Da0emf8tlkCvViwRzPQ2fJEkqTqmE5hDCFsD5QBlwZYxxdBp1dEZnR2rb2z/XcxzPzUIpxaCUR9glSVL3yXlPcwihDLgY2BJYA9g1hLBGruvorM6O1ObbyG6pLnudbz8HSZJUmNK4EHA94J0Y43sxxh+BW4BtU6ijUzo7nVu+Tf/W2YVSikW+/RwkSVJhSiM0VwIfNfn+48y2vNbZkdp8HNltnL3j/dFb8/SIzYo+MEN+/hwkSVLhSaOnuaUVpX+ymF4I4QDgAIDll18+2zW1q7MX76VxsZ9+yp+DJEnqDiHG1hZ/ztIDhrABcGKMcUjm+5EAMcZRrd1n0KBBcfz48TmqUJIkSaUqhPBijHFQ8+1ptGe8AKwSQugbQpgX2AW4J4U6JEmSpA7JeXtGjHFmCOEQYBzJlHNXxxhfz3UdkiRJUkelMk9zjPF+4P40HluSJEnqrDTaMyRJkqSCYmiWJEmS2pFKe0ahqqquceoySZKkEmRo7qCq6hpGjp1AXX0DADW1dYwcOwHA4CxJklTkbM/ooDHjJs4OzI3q6hsYM25iShVJkiQpVwzNHTSltq5T2yVJklQ8DM0d1LtXRae2S5IkqXgYmjto+JB+VJSXzbGtoryM4UP6pVSRJEmScsULATuo8WI/Z8+QJEkqPYbmThg6sNKQLEmSVIIMzXPB+ZolSZJKi6G5k5yvWZIkqfQYmjuprfma8yU0OxIuSZLUvQzNnZTv8zU7Ei5JktT9nHKuk/J9vmZXLpQkSep+huZOyvf5mvN9JFySJKkQGZo7aejASkYN609lrwoCUNmrglHD+udN60O+j4RLkiQVInua50I+z9c8fEi/OXqaIb9GwiVJkgqRobnIuHKhJElS9zM0F6F8HgmXJEkqRPY0S5IkSe0wNEuSJEntMDRLkiRJ7TA0S5IkSe0wNEuSJEntMDRLkiRJ7TA0S5IkSe0wNEuSJEntMDRLkiRJ7TA0S5IkSe0wNEuSJEnt6JF2AfmoqrqGMeMmMqW2jt69Khg+pB9DB1amXZYkSZJSYmhupqq6hpFjJ1BX3wBATW0dI8dOADA4S5IklSjbM5oZM27i7MDcqK6+gTHjJqZUkSRJktJmaG5mSm1dp7ZLkiSp+Bmam+ndq6JT2yVJklT8DM3NDB/Sj4rysjm2VZSXMXxIv5QqkiRJUtq8ELCZxov9nD1DkiRJjQzNLRg6sNKQLEmSpNlsz5AkSZLaYWiWJEmS2mFoliRJktphaJYkSZLaYWiWJEmS2mFoliRJktphaJYkSZLaYWiWJEmS2mFoliRJktphaJYkSZLaYWiWJEmS2mFoliRJktphaJYkSZLaYWiWJEmS2mFoliRJktoRYoxp19CuEMI04IMcP+wSwOc5fsxS4znOPs9xdnl+s89znF2e3+zzHGdfd5/jFWKMSzbfWBChOQ0hhPExxkFp11HMPMfZ5znOLs9v9nmOs8vzm32e4+zL1Tm2PUOSJElqh6FZkiRJaoehuXWXp11ACfAcZ5/nOLs8v9nnOc4uz2/2eY6zLyfn2J5mSZIkqR2ONEuSJEntKPnQHEKYHEKYEEJ4OYQwvoXbQwjhghDCOyGEV0MIa6dRZ6EKIfTLnNvGj69CCIc322eTEMKMJvuckFK5BSOEcHUI4bMQwmtNti0WQngwhDAp83nRVu67RQhhYuZ3ekTuqi4crZzfMSGEtzKvA3eFEHq1ct82X1OUaOUcnxhCqGnyWrBVK/f1d7gdrZzfW5uc28khhJdbua+/wx0QQlguhPBoCOHNEMLrIYS/ZLb7WtwN2ji/qb0Wl3x7RghhMjAoxtji/H6ZF+1Dga2AXwLnxxh/mbsKi0cIoQyoAX4ZY/ygyfZNgCNjjL9LqbSCE0LYCPgGuD7GuGZm25nAFzHG0ZkX4EVjjEc3u18Z8DbwG+Bj4AVg1xjjGzl9AnmulfP7W+CRGOPMEMIZAM3Pb2a/ybTxmqJEK+f4ROCbGONZbdzP3+EOaOn8Nrv9bGBGjPHkFm6bjL/D7QohLAMsE2N8KYSwEPAiMBT4A74Wd1kb53dZUnotLvmR5g7YluRFJ8YYnwV6ZX6Q6rzNgXebBmbNnRjjE8AXzTZvC1yX+fo6kheX5tYD3okxvhdj/BG4JXM/NdHS+Y0x/ifGODPz7bMkL9yaS638DneEv8Md0Nb5DSEEYCfg5pwWVWRijFNjjC9lvv4aeBOoxNfibtHa+U3ztdjQDBH4TwjhxRDCAS3cXgl81OT7jzPb1Hm70PqL9AYhhFdCCA+EEH6ey6KKyNIxxqmQvNgAS7Wwj7/P3WNf4IFWbmvvNUVtOyTzZ9erW/mztr/DXfdr4NMY46RWbvd3uJNCCH2AgcBz+Frc7Zqd36Zy+lrcozsOUuAGxxinhBCWAh4MIbyVeYfeKLRwn9LuaZkLIYR5gd8DI1u4+SWSJSu/ybTDVAGr5LC8UuLvcxeFEI4FZgI3trJLe68pat0lwCkkv5OnAGeT/KfYlL/DXbcrbY8y+zvcCSGEBYE7gcNjjF8lA/nt362Fbf4et6D5+W2yPeevxSU/0hxjnJL5/BlwF8mfTJr6GFiuyffLAlNyU11R2RJ4Kcb4afMbYoxfxRi/yXx9P1AeQlgi1wUWgU8bW4cynz9rYR9/n7sghLA38Dtg99jKBSEdeE1RK2KMn8YYG2KMs4AraPnc+TvcBSGEHsAw4NbW9vF3uONCCOUkge7GGOPYzGZfi7tJK+c3tdfikg7NIYQFMs3lhBAWAH4LvNZst3uAvUJifZILJ6bmuNRi0OrIRgjhZ5keO0II65H8Xk7PYW3F4h5g78zXewN3t7DPC8AqIYS+mdH/XTL3UztCCFsARwO/jzF+18o+HXlNUSuaXS+yHS2fO3+Hu+b/gLdijB+3dKO/wx2X+X/rKuDNGOM5TW7ytbgbtHZ+U30tjjGW7AewIvBK5uN14NjM9oOAgzJfB+Bi4F1gAsmVmKnXXkgfwPwkIXiRJtuanuNDMuf/FZKm/g3TrjnfP0jegEwF6klGLP4ILA48DEzKfF4ss29v4P4m992K5Krtdxt/5/3o0Pl9h6QH8eXMx6XNz29rryl+dPgc35B5nX2VJEAs0/wcZ773d3guzm9m+7WNr71N9vV3eO7O8a9IWipebfK6sJWvxVk/v6m9Fpf8lHOSJElSe0q6PUOSJEnqCEOzJEmS1A5DsyRJktQOQ7MkSZLUDkOzJEmS1A5DsySlIITQEEJ4OYTwWgjh9hDC/N18/MdCCIPa2efwpo8bQrg/hNCrO+uQpGJhaJakdNTFGAfEGNcEfiSZuzzXDieZRx2AGONWMcbaFOqQpLxnaJak9D0JrBxCWCyEUBVCeDWE8GwIYS2AEMKJIYQbQgiPhBAmhRD2z2zfJITwr8aDhBAuCiH8ofnBQwiXhBDGhxBeDyGclNl2GMliAI+GEB7NbJvcuIR9COGIzCj4ayGEwzPb+oQQ3gwhXJE51n9CCBVZPTOSlCcMzZKUohBCD2BLkpXwTgKqY4xrAccA1zfZdS1ga2AD4IQQQu9OPMyxMcZBmWNsHEJYK8Z4ATAF2DTGuGmzmtYB9gF+CawP7B9CGJi5eRXg4hjjz4FaYPvOPF9JKlSGZklKR0UI4WVgPPAhcBXJsrE3AMQYHwEWDyEsktn/7hhjXYzxc+BRYL1OPNZOIYSXgGrg58Aa7ez/K+CuGOO3McZvgLHArzO3vR9jfDnz9YtAn07UIUkFq0faBUhSiaqLMQ5ouiGEEFrYLzb73HT7TOYc/OjZ/M4hhL7AkcC6McYvQwjXtrRf87u1cdsPTb5uAGzPkFQSHGmWpPzxBLA7JP3KwOcxxq8yt20bQugZQlgc2AR4AfgAWCOEMF9mRHrzFo65MPAtMCOEsDRJK0ijr4GFWqljaAhh/hDCAsB2JH3XklSyHGmWpPxxInBNCOFV4Dtg7ya3PQ/cBywPnBJjnAIQQrgNeBWYRNJ+MYcY4yshhGrgdeA94OkmN18OPBBCmNq0rznG+FJmRPr5zKYrY4zVIYQ+3fEkJakQhRib/8VPkpRPQggnAt/EGM9KuxZJKlW2Z0iSJEntcKRZkiRJaocjzZIkSVI7DM2SJElSOwzNkiRJUjsMzZIkSVI7DM2SJElSOwzNkiRJUjv+H6yytbh0VuLYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
